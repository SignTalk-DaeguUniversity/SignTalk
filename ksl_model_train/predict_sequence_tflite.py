"""
ì‹œí€€ìŠ¤ ëª¨ë¸ (Bidirectional LSTM) TFLite ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
ë¼ì¦ˆë² ë¦¬íŒŒì´ 3 ë° ì„ë² ë””ë“œ í™˜ê²½ìš©
"""
import os
import sys
import numpy as np
import cv2
import mediapipe as mp
import time
from collections import deque

# TFLite ì¸í„°í”„ë¦¬í„° import
try:
    from tflite_runtime.interpreter import Interpreter
    print("âœ… tflite_runtime ì‚¬ìš©")
except Exception:
    try:
        from tensorflow.lite.python.interpreter import Interpreter
        print("âœ… tensorflow.lite ì‚¬ìš©")
    except Exception as e:
        print("âŒ TFLite Interpreterë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:", e)
        sys.exit(1)

# ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(__file__)
MODEL_DIR = os.path.join(BASE_DIR, "model")

# ëª¨ë¸ íŒŒì¼ ì„ íƒ (ìš°ì„ ìˆœìœ„: FP16 > FP32 > INT8)
TFLITE_FP16 = os.path.join(MODEL_DIR, "ksl_sequence_fp16.tflite")
TFLITE_FP32 = os.path.join(MODEL_DIR, "ksl_sequence_fp32.tflite")
TFLITE_INT8 = os.path.join(MODEL_DIR, "ksl_sequence_int8.tflite")

LABELS_PATH = os.path.join(MODEL_DIR, "ksl_seq_labels.npy")
MAX_TIMESTEPS_PATH = os.path.join(MODEL_DIR, "ksl_seq_max_timesteps.npy")
NORM_MEAN_PATH = os.path.join(MODEL_DIR, "ksl_seq_norm_mean.npy")
NORM_STD_PATH = os.path.join(MODEL_DIR, "ksl_seq_norm_std.npy")

# ë¼ë²¨ ë° ì •ê·œí™” í†µê³„ ë¡œë“œ
if not os.path.exists(LABELS_PATH):
    print(f"âŒ ë¼ë²¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LABELS_PATH}")
    sys.exit(1)

labels = np.load(LABELS_PATH, allow_pickle=True)
max_timesteps = int(np.load(MAX_TIMESTEPS_PATH))
norm_mean = np.load(NORM_MEAN_PATH)
norm_std = np.load(NORM_STD_PATH)

print(f"âœ… ë¼ë²¨ ë¡œë“œ: {labels}")
print(f"âœ… Max timesteps: {max_timesteps}")

# ëª¨ë¸ ì„ íƒ
if os.path.exists(TFLITE_FP16):
    model_path = TFLITE_FP16
    print(f"âœ… FP16 ëª¨ë¸ ì‚¬ìš© (ê¶Œì¥)")
elif os.path.exists(TFLITE_FP32):
    model_path = TFLITE_FP32
    print(f"âœ… FP32 ëª¨ë¸ ì‚¬ìš©")
elif os.path.exists(TFLITE_INT8):
    model_path = TFLITE_INT8
    print(f"âœ… INT8 ëª¨ë¸ ì‚¬ìš©")
else:
    print("âŒ TFLite ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("   export_sequence_tflite.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
    sys.exit(1)

print(f"ğŸ“‚ ëª¨ë¸ ê²½ë¡œ: {model_path}")
print(f"ğŸ“¦ ëª¨ë¸ í¬ê¸°: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB")

# ì¸í„°í”„ë¦¬í„° ì„¤ì •
print("\nğŸ”§ TFLite ì¸í„°í”„ë¦¬í„° ì´ˆê¸°í™” ì¤‘...")
interpreter = Interpreter(model_path=model_path, num_threads=2)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print(f"   ì…ë ¥ shape: {input_details[0]['shape']}")
print(f"   ì…ë ¥ dtype: {input_details[0]['dtype']}")
print(f"   ì¶œë ¥ shape: {output_details[0]['shape']}")
print(f"   ì¶œë ¥ dtype: {output_details[0]['dtype']}")

# MediaPipe ì„¤ì •
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5,
)
mp_draw = mp.solutions.drawing_utils

# ì¹´ë©”ë¼ ì—´ê¸°
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

print("âœ… ì¹´ë©”ë¼ ì—´ê¸° ì„±ê³µ")

# ì‹œí€€ìŠ¤ ë²„í¼ (0.5~0.8ì´ˆ ë¶„ëŸ‰)
sequence_buffer = deque(maxlen=max_timesteps)
feature_dim = norm_mean.shape[0]

print("\n" + "="*60)
print("ğŸ¯ ì‹œí€€ìŠ¤ ì¸ì‹ ì‹œì‘")
print("="*60)
print("ì¡°ì‘ë²•:")
print("  - ì†ì„ ì›€ì§ì—¬ ìŒììŒ/ë³µí•©ëª¨ìŒ í‘œí˜„")
print("  - ìë™ìœ¼ë¡œ ì‹œí€€ìŠ¤ ì¸ì‹ (0.5ì´ˆ ì´ìƒ ì† ê°ì§€ ì‹œ)")
print("  - SPACE: ë²„í¼ ì´ˆê¸°í™”")
print("  - ESC: ì¢…ë£Œ")
print("="*60)

latest_char = ""
last_prediction_time = 0
prediction_interval = 0.5  # 0.5ì´ˆë§ˆë‹¤ ì˜ˆì¸¡
confidence_threshold = 0.6

# ì„±ëŠ¥ ì¸¡ì •
frame_count = 0
start_time = time.time()

try:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
            break

        frame_count += 1
        image = cv2.flip(frame, 1)
        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        result = hands.process(rgb)

        current_time = time.time()

        # ì† ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
        if result.multi_hand_landmarks:
            for hand_landmarks in result.multi_hand_landmarks:
                mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                # íŠ¹ì§• ì¶”ì¶œ
                features = []
                lm_list = list(hand_landmarks.landmark)
                
                for i, lm in enumerate(lm_list):
                    features.extend([lm.x, lm.y])
                    
                    # dx, dy, spd_sum ê³„ì‚° (ê°„ë‹¨í•œ ê·¼ì‚¬)
                    if i > 0:
                        prev_lm = lm_list[i-1]
                        dx = lm.x - prev_lm.x
                        dy = lm.y - prev_lm.y
                        spd = np.sqrt(dx**2 + dy**2)
                    else:
                        dx, dy, spd = 0, 0, 0
                    
                    features.extend([dx, dy, spd])
                
                # ë²„í¼ì— ì¶”ê°€
                sequence_buffer.append(features[:feature_dim])

                # ì˜ˆì¸¡ (ì¶©ë¶„í•œ í”„ë ˆì„ì´ ìŒ“ì´ë©´)
                if (len(sequence_buffer) >= max_timesteps // 2 and 
                    current_time - last_prediction_time >= prediction_interval):
                    
                    # ì‹œí€€ìŠ¤ íŒ¨ë”©
                    padded_seq = np.zeros((1, max_timesteps, feature_dim), dtype=np.float32)
                    seq_len = min(len(sequence_buffer), max_timesteps)
                    
                    for i, frame_features in enumerate(list(sequence_buffer)[-seq_len:]):
                        padded_seq[0, i, :] = frame_features
                    
                    # ì •ê·œí™”
                    padded_seq = (padded_seq - norm_mean) / norm_std
                    
                    # ì¶”ë¡ 
                    interpreter.set_tensor(input_details[0]['index'], padded_seq)
                    interpreter.invoke()
                    output_data = interpreter.get_tensor(output_details[0]['index'])
                    
                    probs = output_data.reshape(-1)
                    idx = int(np.argmax(probs))
                    confidence = float(np.max(probs))
                    
                    if 0 <= idx < len(labels) and confidence > confidence_threshold:
                        latest_char = labels[idx]
                        print(f"ğŸ¯ ì¸ì‹: {latest_char} (ì‹ ë¢°ë„: {confidence:.3f}, ë²„í¼: {len(sequence_buffer)}í”„ë ˆì„)")
                    
                    last_prediction_time = current_time
        else:
            # ì†ì´ ì—†ìœ¼ë©´ ë²„í¼ ìœ ì§€ (ì¼ì‹œì  ê°€ë¦¼ ëŒ€ì‘)
            pass

        # í™”ë©´ í‘œì‹œ
        cv2.putText(image, f"Char: {latest_char}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
        cv2.putText(image, f"Buffer: {len(sequence_buffer)}/{max_timesteps}", (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        
        # FPS í‘œì‹œ
        if frame_count % 30 == 0:
            elapsed = time.time() - start_time
            fps = frame_count / elapsed
            cv2.putText(image, f"FPS: {fps:.1f}", (10, 90),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

        cv2.imshow("KSL Sequence TFLite (ìŒììŒ/ë³µí•©ëª¨ìŒ)", image)
        
        key = cv2.waitKey(10) & 0xFF
        if key == 27:  # ESC
            break
        elif key == 32:  # SPACE
            sequence_buffer.clear()
            latest_char = ""
            print("ğŸ”„ ë²„í¼ ì´ˆê¸°í™”")

except KeyboardInterrupt:
    print("\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")

finally:
    # ìµœì¢… ì„±ëŠ¥ í†µê³„
    elapsed = time.time() - start_time
    fps = frame_count / elapsed
    
    print("\n" + "="*60)
    print("ğŸ“Š ì„±ëŠ¥ í†µê³„")
    print("="*60)
    print(f"ì´ í”„ë ˆì„: {frame_count}")
    print(f"ì‹¤í–‰ ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"í‰ê·  FPS: {fps:.2f}")
    print(f"ëª¨ë¸: {os.path.basename(model_path)}")
    print("="*60)
    
    cap.release()
    cv2.destroyAllWindows()
    hands.close()
    print("âœ… ì¢…ë£Œ ì™„ë£Œ")
