from flask import Flask, Response, jsonify, request
import cv2
import mediapipe as mp
import numpy as np
import time
import tensorflow as tf
import os
from datetime import datetime
from flask_cors import CORS
from flask_jwt_extended import JWTManager
from config import Config
from auth.models import db
from auth.routes import auth_bp, bcrypt
from api.progress import progress_bp
from api.learning import learning_bp
from api.recognition import recognition_bp
from api.quiz import quiz_bp
from api.jamo_decompose import jamo_decompose_bp
from api.jamo_compose import jamo_compose_bp

app = Flask(__name__)
app.config.from_object(Config)

# í™•ì¥ ì´ˆê¸°í™”
db.init_app(app)
bcrypt.init_app(app)
jwt = JWTManager(app)
CORS(app)  # Flutterì™€ í†µì‹ ì„ ìœ„í•œ CORS ì„¤ì •


# JWT ë¸”ë™ë¦¬ìŠ¤íŠ¸ import
from auth.routes import blacklisted_tokens

# JWT í† í° ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì²´í¬ í•¨ìˆ˜
@jwt.token_in_blocklist_loader
def check_if_token_revoked(jwt_header, jwt_payload):
    """í† í°ì´ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸"""
    jti = jwt_payload['jti']
    return jti in blacklisted_tokens
    
# ë¸”ë£¨í”„ë¦°íŠ¸ ë“±ë¡
app.register_blueprint(auth_bp)
app.register_blueprint(progress_bp)
app.register_blueprint(learning_bp)
app.register_blueprint(recognition_bp)
app.register_blueprint(quiz_bp)
app.register_blueprint(jamo_decompose_bp)
app.register_blueprint(jamo_compose_bp)

# ==== ê²½ë¡œ ì„¤ì • ====
BASE_DIR = os.path.dirname(__file__)
MODEL_DIR = os.path.join(BASE_DIR, "model")

KSL_MODEL_PATH = os.path.join(MODEL_DIR, "ksl_model.h5")
KSL_LABELS_PATH = os.path.join(MODEL_DIR, "ksl_labels.npy")

# ==== ëª¨ë¸ ë¡œë”© (H5 ëª¨ë¸) ====
try:
    ksl_model = tf.keras.models.load_model(KSL_MODEL_PATH)
    labels_ksl = np.load(KSL_LABELS_PATH, allow_pickle=True)

    print("âœ… KSL H5 ëª¨ë¸ ë° ë¼ë²¨ ë¡œë”© ì„±ê³µ")
    print(f"   - ëª¨ë¸ ê²½ë¡œ: {KSL_MODEL_PATH}")
    print(f"   - ë¼ë²¨ ê°œìˆ˜: {len(labels_ksl)}")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    print("ğŸ“± API ì„œë²„ë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ìˆ˜ì–´ ì¸ì‹ ê¸°ëŠ¥ ë¹„í™œì„±í™”)")
    ksl_model = None

# ==== Mediapipe ì„¤ì • ====
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5)
mp_draw = mp.solutions.drawing_utils

# ==== ì¸ì‹ ê²°ê³¼ ì €ì¥ ====
recognized_string = {"ksl": ""}
latest_char = {"ksl": ""}
last_recognized_char = {"ksl": ""}  # ì´ì „ ì¸ì‹ ë¬¸ì
last_recognized_time = {"ksl": 0}  # ì´ì „ ì¸ì‹ ì‹œê°„

# ==== ìŒììŒ ë§¤í•‘ ====
DOUBLE_CONSONANT_MAP = {
    'ã„±': 'ã„²',
    'ã„·': 'ã„¸',
    'ã…‚': 'ã…ƒ',
    'ã……': 'ã…†',
    'ã…ˆ': 'ã…‰'
}



# ==== í˜„ì¬ í”„ë ˆì„ ì €ì¥ìš© ì „ì—­ ë³€ìˆ˜ ====
current_frame_cache = {}  # {lang_key: frame}

# ==== ê³µí†µ ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° (H5 ëª¨ë¸ìš©) ====
def generate_frames(model, labels, lang_key, camera_device=0):
    global current_frame_cache
    # ì¹´ë©”ë¼ ì—´ê¸° (macOS í˜¸í™˜ì„± ê°œì„ )
    print(f"ğŸ“· ì¹´ë©”ë¼ {camera_device}ë²ˆ ì—´ê¸° ì‹œë„...")
    cap = cv2.VideoCapture(camera_device)
    
    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨")
        print("   - ë‹¤ë¥¸ ì•±ì´ ì¹´ë©”ë¼ë¥¼ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
        print("   - ì‹œìŠ¤í…œ ì„¤ì • > ê°œì¸ì •ë³´ ë³´í˜¸ > ì¹´ë©”ë¼ ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”")
        return
    
    print(f"âœ… ì¹´ë©”ë¼ {camera_device}ë²ˆ ì—´ê¸° ì„±ê³µ")
    
    # ê¸°ë³¸ ì„¤ì •ë§Œ ì ìš© (macOS í˜¸í™˜ì„±)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
    
    actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    actual_fps = cap.get(cv2.CAP_PROP_FPS)
    
    print(f"ğŸ“· ì¹´ë©”ë¼ ì„¤ì • ì™„ë£Œ: {actual_width}x{actual_height} @ {actual_fps}fps")

    last_prediction_time = 0
    prediction_interval = 0.15  # 0.15ì´ˆë§ˆë‹¤ ì¸ì‹ (ë¹ ë¥¸ ì‘ë‹µ)
    prev_idx = -1
    consecutive_same = 0  # ì—°ì† ê°™ì€ ê²°ê³¼ ì¹´ìš´íŠ¸
    last_predicted_char = ""
    confidence_threshold = 0.6  # ì‹ ë¢°ë„ ì„ê³„ê°’ ìƒí–¥
    
    # MediaPipe í•­ìƒ í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)
    print("ğŸš€ MediaPipe í•­ìƒ í™œì„±í™” ëª¨ë“œ")

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            if len(frame.shape) == 2 or frame.shape[2] == 1:
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)

            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ìµœì í™”
            image = cv2.flip(frame, 1)  # ì¢Œìš° ë°˜ì „
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            current_time = time.time()
            
            # í˜„ì¬ í”„ë ˆì„ì„ ìºì‹œì— ì €ì¥ (APIì—ì„œ ì‚¬ìš©)
            current_frame_cache[lang_key] = image.copy()

            # MediaPipe í•­ìƒ í™œì„±í™”
            result = hands.process(rgb_image)

            if result.multi_hand_landmarks:
                for hand_landmarks in result.multi_hand_landmarks:
                    # ì† ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                    mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                    if current_time - last_prediction_time >= prediction_interval:
                        coords = [v for lm in hand_landmarks.landmark for v in (lm.x, lm.y)]
                        input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
                        prediction = model.predict(input_data, verbose=0)
                        idx = np.argmax(prediction)
                        confidence = float(np.max(prediction))

                        # ì‹ ë¢°ë„ ì„ê³„ê°’
                        if 0 <= idx < len(labels) and confidence > confidence_threshold:
                            predicted_char = labels[idx]
                            
                            # ì¦‰ì‹œ ì—…ë°ì´íŠ¸ (ë¹ ë¥¸ ì‘ë‹µ)
                            latest_char[lang_key] = predicted_char
                            current_time_sec = time.time()
                            time_diff = current_time_sec - last_recognized_time.get(lang_key, 0)
                            
                            # ìŒììŒ ì²˜ë¦¬ ë¡œì§
                            if (predicted_char in DOUBLE_CONSONANT_MAP and 
                                predicted_char == last_recognized_char.get(lang_key, '') and 
                                0.5 < time_diff < 3.0):
                                
                                # ìŒììŒìœ¼ë¡œ ë³€í™˜
                                double_char = DOUBLE_CONSONANT_MAP[predicted_char]
                                latest_char[lang_key] = double_char
                                print(f"ğŸ¯ğŸ¯ ìŒììŒ: {predicted_char} + {predicted_char} â†’ {double_char}")
                                
                                # ì´ˆê¸°í™”
                                last_recognized_char[lang_key] = ""
                                last_recognized_time[lang_key] = 0
                            else:
                                # ì¼ë°˜ ì¸ì‹
                                print(f"ğŸ¯ {predicted_char} ì¸ì‹ (ì‹ ë¢°ë„: {confidence:.3f})")
                                
                                # ìŒììŒ ëŒ€ê¸° ì •ë³´ ì €ì¥
                                last_recognized_char[lang_key] = predicted_char
                                last_recognized_time[lang_key] = current_time_sec
                        else:
                            latest_char[lang_key] = ""
                            consecutive_same = 0
                            last_predicted_char = ""

                        prev_idx = idx
                        last_prediction_time = current_time
            else:
                # ì†ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì´ˆê¸°í™”
                latest_char[lang_key] = ""
                consecutive_same = 0
                last_predicted_char = ""
                # ìŒììŒ íƒ€ì´ë¨¸ëŠ” ìœ ì§€ (ì†ì„ ë–¼ë„ 3ì´ˆ ì´ë‚´ë©´ ìŒììŒ ê°€ëŠ¥)

            # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
            hands_detected = "YES" if result.multi_hand_landmarks else "NO"
            current_char = latest_char[lang_key] if latest_char[lang_key] else "None"
            
            # ìƒë‹¨: í˜„ì¬ ì¸ì‹ ê²°ê³¼
            cv2.putText(image, f"Current: {current_char}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # ì¤‘ê°„: ì† ê°ì§€ ìƒíƒœ
            cv2.putText(image, f"Hands: {hands_detected}", (10, 60),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
            
            # í•˜ë‹¨: ëˆ„ì  ë¬¸ìì—´
            accumulated = recognized_string[lang_key][:10]  # ì²˜ìŒ 10ê¸€ìë§Œ
            cv2.putText(image, f"Text: {accumulated}", (10, 90),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

            # JPEG ì••ì¶• ìµœì í™” (ì „ì†¡ ì†ë„ í–¥ìƒ, ì¸ì‹ ì •í™•ë„ëŠ” ìœ ì§€)
            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 75]
            ret, buffer = cv2.imencode('.jpg', image, encode_param)
            frame = buffer.tobytes()

            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')

    except GeneratorExit:
        print("ğŸ›‘ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ë‹¨ ê°ì§€: í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œë¨")
    finally:
        cap.release()
        print("âœ… ì¹´ë©”ë¼ ìì› í•´ì œ ì™„ë£Œ")

# ==== ë¼ìš°íŒ… ====
@app.route('/')
def index():
    """ì„œë²„ ìƒíƒœ í™•ì¸ í˜ì´ì§€"""
    return jsonify({
        'server': 'SignTalk API Server',
        'status': 'running',
        'version': '1.0.0',
        'endpoints': {
            'video_stream': '/video_feed_ksl',
            'recognition': '/api/recognition/current/<lang>',
            'health': '/api/auth/health',
            'progress': '/api/progress/<lang>'
        }
    })

@app.route('/video_feed_ksl')
def video_feed_ksl():
    if ksl_model is None:
        return jsonify({'error': 'KSL ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}), 503
    
    # í´ë¼ì´ì–¸íŠ¸ ì •ë³´ í™•ì¸ (ì—ë®¬ë ˆì´í„° vs ì‹¤ì œ ê¸°ê¸°)
    client_ip = request.environ.get('HTTP_X_FORWARDED_FOR', request.environ.get('REMOTE_ADDR', ''))
    user_agent = request.headers.get('User-Agent', '')
    remote_addr = request.environ.get('REMOTE_ADDR', '')
    
    print("="*60)
    print(f"ğŸ” ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ìš”ì²­ ìƒì„¸:")
    print(f"   - HTTP_X_FORWARDED_FOR: {request.environ.get('HTTP_X_FORWARDED_FOR', 'None')}")
    print(f"   - REMOTE_ADDR: {remote_addr}")
    print(f"   - Client IP (ìµœì¢…): {client_ip}")
    print(f"   - User-Agent: {user_agent}")
    print(f"   - Request URL: {request.url}")
    print("="*60)
    
    # ì—ë®¬ë ˆì´í„° ê°ì§€ (ë” ê°•ë ¥í•œ ì¡°ê±´)
    is_emulator = (
        '10.0.2.2' in str(client_ip) or
        '10.0.2.2' in str(remote_addr) or
        '127.0.0.1' in str(client_ip) or
        'localhost' in str(client_ip) or
        '::1' in str(client_ip)  # IPv6 localhost
    )
    
    # ì¹´ë©”ë¼ ì„ íƒ
    if is_emulator:
        # ì—ë®¬ë ˆì´í„°: ë…¸íŠ¸ë¶ ë‚´ì¥ ì¹´ë©”ë¼ ì°¾ê¸°
        # macOS Continuity Camera ë¬¸ì œ íšŒí”¼: 1ë²ˆ ì¹´ë©”ë¼ ì‹œë„
        camera_device = 0  # 0ë²ˆ ì¹´ë©”ë¼ ì‚¬ìš© (ìœ ì¼í•œ ì¹´ë©”ë¼)
        print("âœ… ì—ë®¬ë ˆì´í„° ê°ì§€ â†’ ì¹´ë©”ë¼ 0ë²ˆ ì‚¬ìš©")
        print("   (iPhone Continuity Cameraë“  ë…¸íŠ¸ë¶ ì¹´ë©”ë¼ë“  0ë²ˆë§Œ ì¡´ì¬)")
    else:
        camera_device = 0  # ì‹¤ì œ ê¸°ê¸° ì „ë©´ ì¹´ë©”ë¼
        print("âœ… ì‹¤ì œ ê¸°ê¸° ê°ì§€ â†’ ê¸°ê¸° ì „ë©´ ì¹´ë©”ë¼ (0ë²ˆ) ì‚¬ìš©")
    
    print(f"ğŸ“· ìµœì¢… ì„ íƒëœ ì¹´ë©”ë¼: {camera_device}ë²ˆ")
    print("="*60)
    
    return Response(generate_frames(ksl_model, labels_ksl, "ksl", camera_device),
                    mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/api/recognition/current/<lang>')
@app.route('/get_string/<lang>')  # í•˜ìœ„ í˜¸í™˜ì„±
def get_current_recognition(lang):
    """í˜„ì¬ ì¸ì‹ ê²°ê³¼ ë°˜í™˜ (í†µí•© API)"""
    current_char = latest_char.get(lang, '')
    accumulated_string = recognized_string.get(lang, '')
    
    # ë””ë²„ê¹… ì •ë³´
    print(f"ğŸ“± ì¸ì‹ ê²°ê³¼ ìš”ì²­: {lang} - Current: '{current_char}', String: '{accumulated_string}'")
    
    return jsonify({
        # ìƒˆ API í˜•ì‹
        'current_character': current_char,
        'accumulated_string': accumulated_string,
        # ê¸°ì¡´ API í˜•ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
        'current': current_char,
        'string': accumulated_string,
        # ì¶”ê°€ ì •ë³´
        'timestamp': time.time(),
        'language': lang,
        'has_current': bool(current_char and current_char.strip())
    })

@app.route('/camera_info')
def camera_info():
    """í˜„ì¬ ì¹´ë©”ë¼ ì„¤ì • ì •ë³´ ë°˜í™˜"""
    try:
        # í´ë¼ì´ì–¸íŠ¸ ì •ë³´
        client_ip = request.environ.get('HTTP_X_FORWARDED_FOR', request.environ.get('REMOTE_ADDR', ''))
        user_agent = request.headers.get('User-Agent', '')
        
        # ì—ë®¬ë ˆì´í„° ê°ì§€
        is_emulator = (
            '10.0.2.2' in client_ip or
            '127.0.0.1' in client_ip or
            'localhost' in client_ip
        )
        
        return jsonify({
            'client_ip': client_ip,
            'user_agent': user_agent,
            'is_emulator': is_emulator,
            'camera_device': 0,  # í•­ìƒ 0ë²ˆ ì¹´ë©”ë¼ ì‚¬ìš©
            'camera_type': 'laptop_webcam' if is_emulator else 'device_front_camera',
            'platform': {
                'system': os.name,
                'platform': __import__('platform').system()
            }
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/auth/health')
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸ (Flutter ì•±ìš©)"""
    return jsonify({
        'status': 'healthy',
        'server': 'SignTalk API Server',
        'version': '1.0.0',
        'timestamp': datetime.utcnow().isoformat()
    }), 200

@app.route('/add_char/<lang>')
def add_char(lang):
    if latest_char[lang] and latest_char[lang] not in ["ERR:IDX", "ERR:DIM", ""]:
        recognized_string[lang] += latest_char[lang]
        print(f"âœ… ë¬¸ì ì¶”ê°€: {latest_char[lang]} â†’ {recognized_string[lang]}")
    return jsonify({
        'success': True, 
        'current': latest_char[lang],
        'accumulated': recognized_string[lang]
    })



@app.route('/remove_char/<lang>')
def remove_char(lang):
    if recognized_string[lang]:
        recognized_string[lang] = recognized_string[lang][:-1]
    return jsonify({'success': True})

@app.route('/clear_string/<lang>')
def clear_string(lang):
    recognized_string[lang] = ""
    return jsonify({'success': True})

@app.route('/upload_image/<lang>', methods=['POST'])
def upload_image(lang):
    """ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼ì—ì„œ ì´¬ì˜í•œ ì´ë¯¸ì§€ë¥¼ ë°›ì•„ì„œ ìˆ˜ì–´ ì¸ì‹ ì²˜ë¦¬"""
    try:
        if 'image' not in request.files:
            return jsonify({'error': 'No image file provided'}), 400
        
        file = request.files['image']
        if file.filename == '':
            return jsonify({'error': 'No image file selected'}), 400
        
        # ì´ë¯¸ì§€ íŒŒì¼ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        import numpy as np
        from PIL import Image
        import io
        
        # íŒŒì¼ì„ ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
        image_bytes = file.read()
        image = Image.open(io.BytesIO(image_bytes))
        
        # OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        image_array = np.array(image)
        if len(image_array.shape) == 3 and image_array.shape[2] == 3:
            # RGB to BGR ë³€í™˜ (OpenCVëŠ” BGR ì‚¬ìš©)
            image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
        
        # ìˆ˜ì–´ ì¸ì‹ ì²˜ë¦¬
        result = process_uploaded_image(image_array, lang)
        
        return jsonify({
            'success': True,
            'recognized_character': result.get('character', ''),
            'confidence': result.get('confidence', 0.0)
        })
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return jsonify({'error': str(e)}), 500

def process_uploaded_image(image, lang):
    """ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì—ì„œ ìˆ˜ì–´ ì¸ì‹ ì²˜ë¦¬"""
    try:
        # ì–¸ì–´ë³„ ëª¨ë¸ ì„ íƒ
        if lang == 'ksl':
            model = ksl_model
            labels = labels_ksl
        else:
            # ASL ëª¨ë¸ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬
            return {'character': '', 'confidence': 0.0}
        
        if model is None:
            return {'character': '', 'confidence': 0.0}
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        image = cv2.resize(image, (320, 240))
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # MediaPipeë¡œ ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
        result = hands.process(rgb_image)
        
        if result.multi_hand_landmarks:
            for hand_landmarks in result.multi_hand_landmarks:
                # ì¢Œí‘œ ì¶”ì¶œ
                coords = [v for lm in hand_landmarks.landmark for v in (lm.x, lm.y)]
                input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
                
                # ëª¨ë¸ ì¶”ë¡  (H5 ëª¨ë¸)
                prediction = model.predict(input_data, verbose=0)
                
                idx = np.argmax(prediction)
                confidence = float(np.max(prediction))
                
                if 0 <= idx < len(labels):
                    character = labels[idx]
                    # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
                    latest_char[lang] = character
                    return {'character': character, 'confidence': confidence}
        
        return {'character': '', 'confidence': 0.0}
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {'character': '', 'confidence': 0.0}



if __name__ == '__main__':
    # ì‹¤ì œ ê¸°ê¸°ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ 0.0.0.0ìœ¼ë¡œ ë°”ì¸ë”©
    app.run(debug=True, host='0.0.0.0', port=5002)