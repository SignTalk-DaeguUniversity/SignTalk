from flask import Flask, render_template, Response, jsonify, request
import cv2
import mediapipe as mp
import numpy as np
import time
import tensorflow as tf
import os
from deep_translator import GoogleTranslator
from gtts import gTTS
import subprocess
from jamo import combine_hangul_jamo
from flask_cors import CORS
from flask_jwt_extended import JWTManager
from config import Config
from auth.models import db
from auth.routes import auth_bp, bcrypt
from api.progress import progress_bp
from api.learning import learning_bp
from api.recognition import recognition_bp
from api.quiz import quiz_bp

app = Flask(__name__)
app.config.from_object(Config)

# í™•ì¥ ì´ˆê¸°í™”
db.init_app(app)
bcrypt.init_app(app)
jwt = JWTManager(app)
CORS(app)  # Flutterì™€ í†µì‹ ì„ ìœ„í•œ CORS ì„¤ì •


# JWT ë¸”ë™ë¦¬ìŠ¤íŠ¸ import
from auth.routes import blacklisted_tokens

# JWT í† í° ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì²´í¬ í•¨ìˆ˜
@jwt.token_in_blocklist_loader
def check_if_token_revoked(jwt_header, jwt_payload):
    """í† í°ì´ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸"""
    jti = jwt_payload['jti']
    return jti in blacklisted_tokens
    
# ë¸”ë£¨í”„ë¦°íŠ¸ ë“±ë¡
app.register_blueprint(auth_bp)
app.register_blueprint(progress_bp)
app.register_blueprint(learning_bp)
app.register_blueprint(recognition_bp)
app.register_blueprint(quiz_bp)

# ==== ê²½ë¡œ ì„¤ì • ====
BASE_DIR = os.path.dirname(__file__)
MODEL_DIR = os.path.join(BASE_DIR, "model")

KSL_MODEL_PATH = os.path.join(MODEL_DIR, "ksl_model.tflite")
KSL_LABELS_PATH = os.path.join(MODEL_DIR, "ksl_labels.npy")

# ==== ëª¨ë¸ ë¡œë”© ====
try:
    ksl_interpreter = tf.lite.Interpreter(model_path=KSL_MODEL_PATH)
    ksl_interpreter.allocate_tensors()
    ksl_input_details = ksl_interpreter.get_input_details()
    ksl_output_details = ksl_interpreter.get_output_details()
    labels_ksl = np.load(KSL_LABELS_PATH, allow_pickle=True)

    print(" KSL ëª¨ë¸ ë° ë¼ë²¨ ë¡œë”© ì„±ê³µ")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    print("ğŸ“± API ì„œë²„ë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ìˆ˜ì–´ ì¸ì‹ ê¸°ëŠ¥ ë¹„í™œì„±í™”)")
    ksl_interpreter = None

# ==== Mediapipe ì„¤ì • ====
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5)
mp_draw = mp.solutions.drawing_utils

# ==== ì¸ì‹ ê²°ê³¼ ì €ì¥ ====
recognized_string = {"asl": "", "ksl": ""}
latest_char = {"asl": "", "ksl": ""}

# ==== ê³µí†µ ì˜ìƒ ìŠ¤íŠ¸ë¦¬ë° ====
def generate_frames(interpreter, input_details, output_details, labels, lang_key):
    # ì•ˆë“œë¡œì´ë“œ ì—ë®¬ë ˆì´í„° ìµœì í™” ì„¤ì •
    cap = cv2.VideoCapture(0)
    
    # ë‚®ì€ í•´ìƒë„ë¡œ ì„±ëŠ¥ í–¥ìƒ
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)
    cap.set(cv2.CAP_PROP_FPS, 15)  # FPS ì œí•œìœ¼ë¡œ CPU ë¶€í•˜ ê°ì†Œ
    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))
    
    # ë²„í¼ í¬ê¸° ìµœì†Œí™” (ì§€ì—° ê°ì†Œ)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨")
        return

    last_prediction_time = 0
    prediction_interval = 1.5  # ë” ë¹ ë¥¸ ì¸ì‹ ê°„ê²©
    prev_idx = -1
    process_active = True
    last_switch_time = time.time()
    active_duration = 3  # ë” ê¸´ í™œì„±í™” ì‹œê°„
    inactive_duration = 1  # ë” ì§§ì€ íœ´ì‹ ì‹œê°„

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            if len(frame.shape) == 2 or frame.shape[2] == 1:
                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)

            # ì´ë¯¸ì§€ í¬ê¸° ì¶”ê°€ ì¶•ì†Œ (ì„±ëŠ¥ í–¥ìƒ)
            frame = cv2.resize(frame, (320, 240))
            image = cv2.flip(frame, 1)
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            current_time = time.time()

            if process_active and current_time - last_switch_time >= active_duration:
                process_active = False
                last_switch_time = current_time
                print("ğŸ›‘ Mediapipe ë¹„í™œì„±í™” (2ì´ˆ íœ´ì‹)")
            elif not process_active and current_time - last_switch_time >= inactive_duration:
                process_active = True
                last_switch_time = current_time
                print("âœ… Mediapipe í™œì„±í™” (2ì´ˆ ì‹¤í–‰)")

            if process_active:
                result = hands.process(rgb_image)

                if result.multi_hand_landmarks:
                    for hand_landmarks in result.multi_hand_landmarks:
                        mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                        if current_time - last_prediction_time >= prediction_interval:
                            coords = [v for lm in hand_landmarks.landmark for v in (lm.x, lm.y)]
                            input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
                            interpreter.set_tensor(input_details[0]['index'], input_data)
                            interpreter.invoke()
                            prediction = interpreter.get_tensor(output_details[0]['index'])
                            idx = np.argmax(prediction)

                            if 0 <= idx < len(labels):
                                latest_char[lang_key] = labels[idx]
                            else:
                                latest_char[lang_key] = "ERR:IDX"

                            prev_idx = idx
                            last_prediction_time = current_time

            #ë””ë²„ê¹…ìš©
            #display_text = f"Current: {latest_char[lang_key]} | Accumulated: {recognized_string[lang_key]}"
            #cv2.putText(image, display_text, (20, 40),
            #            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            ret, buffer = cv2.imencode('.jpg', image)
            frame = buffer.tobytes()

            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')

    except GeneratorExit:
        print("ğŸ›‘ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ë‹¨ ê°ì§€: í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œë¨")
    finally:
        cap.release()
        print("âœ… ì¹´ë©”ë¼ ìì› í•´ì œ ì™„ë£Œ")

# ==== ë¼ìš°íŒ… ====
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/asl')
def asl_page():
    return render_template('asl.html')

@app.route('/ksl')
def ksl_page():
    return render_template('ksl.html')

@app.route('/video_feed_asl')
def video_feed_asl():
    return Response(generate_frames(asl_interpreter, asl_input_details, asl_output_details, labels_asl, "asl"),
                    mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/video_feed_ksl')
def video_feed_ksl():
    return Response(generate_frames(ksl_interpreter, ksl_input_details, ksl_output_details, labels_ksl, "ksl"),
                    mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/get_string/<lang>')
def get_string(lang):
    return {'string': recognized_string[lang], 'current': latest_char[lang]}

@app.route('/add_char/<lang>')
def add_char(lang):
    if latest_char[lang] and latest_char[lang] not in ["ERR:IDX", "ERR:DIM"]:
        recognized_string[lang] += latest_char[lang]
    return jsonify({'success': True})

@app.route('/remove_char/<lang>')
def remove_char(lang):
    if recognized_string[lang]:
        recognized_string[lang] = recognized_string[lang][:-1]
    return jsonify({'success': True})

@app.route('/clear_string/<lang>')
def clear_string(lang):
    recognized_string[lang] = ""
    return jsonify({'success': True})

@app.route('/upload_image/<lang>', methods=['POST'])
def upload_image(lang):
    """ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼ì—ì„œ ì´¬ì˜í•œ ì´ë¯¸ì§€ë¥¼ ë°›ì•„ì„œ ìˆ˜ì–´ ì¸ì‹ ì²˜ë¦¬"""
    try:
        if 'image' not in request.files:
            return jsonify({'error': 'No image file provided'}), 400
        
        file = request.files['image']
        if file.filename == '':
            return jsonify({'error': 'No image file selected'}), 400
        
        # ì´ë¯¸ì§€ íŒŒì¼ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        import numpy as np
        from PIL import Image
        import io
        
        # íŒŒì¼ì„ ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
        image_bytes = file.read()
        image = Image.open(io.BytesIO(image_bytes))
        
        # OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        image_array = np.array(image)
        if len(image_array.shape) == 3 and image_array.shape[2] == 3:
            # RGB to BGR ë³€í™˜ (OpenCVëŠ” BGR ì‚¬ìš©)
            image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
        
        # ìˆ˜ì–´ ì¸ì‹ ì²˜ë¦¬
        result = process_uploaded_image(image_array, lang)
        
        return jsonify({
            'success': True,
            'recognized_character': result.get('character', ''),
            'confidence': result.get('confidence', 0.0)
        })
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì—…ë¡œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return jsonify({'error': str(e)}), 500

def process_uploaded_image(image, lang):
    """ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì—ì„œ ìˆ˜ì–´ ì¸ì‹ ì²˜ë¦¬"""
    try:
        # ì–¸ì–´ë³„ ëª¨ë¸ ì„ íƒ
        if lang == 'ksl':
            interpreter = ksl_interpreter
            labels = labels_ksl
            input_details = ksl_input_details
            output_details = ksl_output_details
        else:
            # ASL ëª¨ë¸ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬
            return {'character': '', 'confidence': 0.0}
        
        if interpreter is None:
            return {'character': '', 'confidence': 0.0}
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
        image = cv2.resize(image, (320, 240))
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # MediaPipeë¡œ ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
        result = hands.process(rgb_image)
        
        if result.multi_hand_landmarks:
            for hand_landmarks in result.multi_hand_landmarks:
                # ì¢Œí‘œ ì¶”ì¶œ
                coords = [v for lm in hand_landmarks.landmark for v in (lm.x, lm.y)]
                input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
                
                # ëª¨ë¸ ì¶”ë¡ 
                interpreter.set_tensor(input_details[0]['index'], input_data)
                interpreter.invoke()
                prediction = interpreter.get_tensor(output_details[0]['index'])
                
                idx = np.argmax(prediction)
                confidence = float(np.max(prediction))
                
                if 0 <= idx < len(labels):
                    character = labels[idx]
                    # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
                    latest_char[lang] = character
                    return {'character': character, 'confidence': confidence}
        
        return {'character': '', 'confidence': 0.0}
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {'character': '', 'confidence': 0.0}

@app.route('/translate/<lang>')
def translate(lang):
    original = combine_hangul_jamo(list(recognized_string[lang].strip())) or "Hello"
    try:
        en = GoogleTranslator(source='auto', target='en').translate(original)
        ko = GoogleTranslator(source='auto', target='ko').translate(original)
        zh = GoogleTranslator(source='auto', target='zh-CN').translate(original)
        ja = GoogleTranslator(source='auto', target='ja').translate(original)
    except Exception as e:
        print("âŒ ë²ˆì—­ ì‹¤íŒ¨:", e)
        en = ko = zh = ja = "(ë²ˆì—­ ì˜¤ë¥˜)"

    # ë’¤ë¡œê°€ê¸° ì£¼ì†Œ ê²°ì •
    prev_url = f"/{lang}" if lang in ["asl", "ksl"] else "/"

    return render_template('translate.html', ko=ko, en=en, zh=zh, ja=ja, prev_url=prev_url)


@app.route('/edu/<lang>')
def edu_page(lang):
    string = recognized_string.get(lang, "")
    chars = list(string)
    return render_template("edu.html", chars=chars, lang=lang)

# ==== TTS ìŒì„± ì¶œë ¥ ====
@app.route('/speak/<lang_code>')
def speak(lang_code):
    try:
        # ì¡°í•©ëœ í•œê¸€ ë¬¸ìì—´ ë§Œë“¤ê¸° (ìëª¨ â†’ ì™„ì„±í˜•)
        raw = recognized_string["asl"] or recognized_string["ksl"]
        original_text = combine_hangul_jamo(list(raw.strip())) if raw else ""

        if not original_text:
            return jsonify({'success': False, 'msg': 'ì¸ì‹ëœ ë¬¸ìì—´ì´ ì—†ìŠµë‹ˆë‹¤.'})

        # ë²ˆì—­ ê²°ê³¼ ì‚¬ìš© (ì •í™•í•œ ë°œìŒì„ ìœ„í•´)
        text_map = {
            "ko": original_text,
            "en": GoogleTranslator(source='ko', target='en').translate(original_text),
            "zh": GoogleTranslator(source='ko', target='zh-CN').translate(original_text),
            "ja": GoogleTranslator(source='ko', target='ja').translate(original_text),
        }

        text = text_map.get(lang_code, "")
        if not text:
            return jsonify({'success': False, 'msg': 'í•´ë‹¹ ì–¸ì–´ ì½”ë“œê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'})

        tts = gTTS(text=text, lang=lang_code)
        mp3_path = os.path.join(BASE_DIR, "temp.mp3")
        wav_path = os.path.join(BASE_DIR, "temp.wav")
        tts.save(mp3_path)

        subprocess.run(["ffmpeg", "-y", "-i", mp3_path, wav_path],
                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        subprocess.run(["aplay", wav_path],
                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

        return jsonify({'success': True})
    except Exception as e:
        print(f"âŒ ìŒì„± ì¶œë ¥ ì‹¤íŒ¨: {e}")
        return jsonify({'success': False, 'error': str(e)})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5002)