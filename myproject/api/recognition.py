# - ì†ëª¨ì–‘ ë¶„ì„ ë° ì„¸ì…˜ ê´€ë¦¬ API (H5 ëª¨ë¸ ë²„ì „)
from flask import Blueprint, request, jsonify
from flask_jwt_extended import jwt_required, get_jwt_identity
from auth.models import db, Recognition
from datetime import datetime
import uuid
import random
import tensorflow as tf
import mediapipe as mp
import cv2
import numpy as np
import os
import base64
from PIL import Image
import io

recognition_bp = Blueprint('recognition', __name__)

# ì „ì—­ ì„¸ì…˜ ì €ì¥ì†Œ
active_sessions = {}

# ==== AI ëª¨ë¸ ì´ˆê¸°í™” ====
BASE_DIR = os.path.dirname(os.path.dirname(__file__))  # myproject í´ë”
MODEL_DIR = os.path.join(BASE_DIR, "model")
KSL_MODEL_PATH = os.path.join(MODEL_DIR, "ksl_model.h5")
KSL_LABELS_PATH = os.path.join(MODEL_DIR, "ksl_labels.npy")

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜
ksl_model = None
labels_ksl = None
mp_hands = None
hands = None

def initialize_ai_models():
    """AI ëª¨ë¸ ì´ˆê¸°í™”"""
    global ksl_model, labels_ksl, mp_hands, hands
    
    try:
        # Keras ëª¨ë¸ ë¡œë”©
        ksl_model = tf.keras.models.load_model(KSL_MODEL_PATH)
        labels_ksl = np.load(KSL_LABELS_PATH, allow_pickle=True)
        
        # MediaPipe ì´ˆê¸°í™”
        mp_hands = mp.solutions.hands
        hands = mp_hands.Hands(
            static_image_mode=True,  # ì •ì  ì´ë¯¸ì§€ ëª¨ë“œ
            max_num_hands=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        print("âœ… AI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ (H5 ëª¨ë¸)")
        print(f"   - ëª¨ë¸ ê²½ë¡œ: {KSL_MODEL_PATH}")
        print(f"   - ë¼ë²¨ ê°œìˆ˜: {len(labels_ksl)}")
        return True
        
    except Exception as e:
        print(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# ëª¨ë¸ ì´ˆê¸°í™” ì‹¤í–‰
model_initialized = initialize_ai_models()

def decode_base64_image(image_data):
    """Base64 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ OpenCV ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    try:
        # Base64 í—¤ë” ì œê±° (data:image/jpeg;base64, ë¶€ë¶„)
        if ',' in image_data:
            image_data = image_data.split(',')[1]
        
        # Base64 ë””ì½”ë”©
        image_bytes = base64.b64decode(image_data)
        
        # PIL Imageë¡œ ë³€í™˜
        pil_image = Image.open(io.BytesIO(image_bytes))
        
        # OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        opencv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        
        return opencv_image
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        return None

def analyze_sign_accuracy(image_data, target_sign, language):
    """ì‹¤ì œ AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ìˆ˜ì–´ ì •í™•ë„ ë¶„ì„"""
    
    # ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° í´ë°±
    if not model_initialized or ksl_model is None:
        print("âš ï¸ AI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ. í´ë°± ëª¨ë“œ ì‚¬ìš©")
        return fallback_analysis(target_sign, language)
    
    try:
        # 1. ì´ë¯¸ì§€ ë””ì½”ë”©
        if not image_data:
            print("âš ï¸ ì´ë¯¸ì§€ ë°ì´í„° ì—†ìŒ")
            return fallback_analysis(target_sign, language)
        
        image = decode_base64_image(image_data)
        if image is None:
            print("âš ï¸ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨")
            return fallback_analysis(target_sign, language)
        
        # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # 3. MediaPipeë¡œ ì† ì¸ì‹
        results = hands.process(image_rgb)
        
        if not results.multi_hand_landmarks:
            return {
                'accuracy': 0.0,
                'confidence': 0.0,
                'feedback': generate_detailed_feedback(0.0, target_sign, language),
                'hand_detected': False,
                'target_sign': target_sign,
                'language': language,
                'error': 'ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'
            }
        
        # 4. ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
        hand_landmarks = results.multi_hand_landmarks[0]
        coords = []
        for landmark in hand_landmarks.landmark:
            coords.extend([landmark.x, landmark.y])
        
        # 5. AI ëª¨ë¸ ì˜ˆì¸¡ (H5 ëª¨ë¸)
        input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
        prediction = ksl_model.predict(input_data, verbose=0)
        
        # 6. ê²°ê³¼ ë¶„ì„
        predicted_idx = np.argmax(prediction)
        confidence_score = float(np.max(prediction))
        
        if 0 <= predicted_idx < len(labels_ksl):
            predicted_sign = labels_ksl[predicted_idx]
        else:
            predicted_sign = "UNKNOWN"
        
        # 7. ì •í™•ë„ ê³„ì‚°
        is_correct = predicted_sign == target_sign
        accuracy = confidence_score * 100 if is_correct else max(0, confidence_score * 50)
        
        # 8. í”¼ë“œë°± ìƒì„±
        feedback = generate_detailed_feedback(accuracy, target_sign, language)
        
        return {
            'accuracy': round(accuracy, 1),
            'confidence': round(confidence_score, 2),
            'feedback': feedback,
            'hand_detected': True,
            'target_sign': target_sign,
            'predicted_sign': predicted_sign,
            'is_correct': is_correct,
            'language': language
        }
        
    except Exception as e:
        print(f"âŒ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return fallback_analysis(target_sign, language)

def fallback_analysis(target_sign, language):
    """AI ëª¨ë¸ ì‹¤íŒ¨ ì‹œ í´ë°± ë¶„ì„"""
    # ìˆ˜ì–´ë³„ ë‚œì´ë„ ì„¤ì •
    sign_difficulty = {
        'A': 0.9, 'B': 0.8, 'C': 0.7, 'D': 0.8, 'E': 0.9,
        'F': 0.7, 'G': 0.6, 'H': 0.8, 'I': 0.9, 'J': 0.6,
        'Hello': 0.6, 'Thank you': 0.5, 'Please': 0.6,
        'ã„±': 0.8, 'ã„´': 0.7, 'ã„·': 0.8, 'ã„¹': 0.6, 'ã…': 0.7,
        'ì•ˆë…•í•˜ì„¸ìš”': 0.5, 'ê°ì‚¬í•©ë‹ˆë‹¤': 0.4
    }
    
    # ê¸°ë³¸ ì •í™•ë„ ê³„ì‚° (í´ë°± ëª¨ë“œ)
    base_accuracy = 75.0
    difficulty_factor = sign_difficulty.get(target_sign, 0.7)
    random_factor = random.uniform(0.7, 1.3)
    language_factor = 1.0 if language == 'asl' else 0.95
    
    final_accuracy = min(100.0, base_accuracy * difficulty_factor * random_factor * language_factor)
    confidence = final_accuracy / 100.0
    
    # í”¼ë“œë°± ìƒì„±
    feedback = generate_detailed_feedback(final_accuracy, target_sign, language)
    
    return {
        'accuracy': round(final_accuracy, 1),
        'confidence': round(confidence, 2),
        'feedback': feedback,
        'hand_detected': True,
        'target_sign': target_sign,
        'language': language,
        'fallback_mode': True
    }

def generate_detailed_feedback(accuracy, target_sign, language):
    """ìƒì„¸ í”¼ë“œë°± ìƒì„±"""
    
    if accuracy >= 90:
        return {
            'level': 'excellent',
            'message': f'ì™„ë²½í•œ "{target_sign}" ìˆ˜ì–´ì…ë‹ˆë‹¤! ğŸ‰',
            'suggestions': ['í›Œë¥­í•´ìš”! ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”'],
            'color': 'green',
            'score': 'A+'
        }
    elif accuracy >= 80:
        return {
            'level': 'very_good',
            'message': f'ì•„ì£¼ ì¢‹ì€ "{target_sign}" ìˆ˜ì–´ì…ë‹ˆë‹¤! ğŸ‘',
            'suggestions': ['ê±°ì˜ ì™„ë²½í•´ìš”!', 'ì¡°ê¸ˆë§Œ ë” ì—°ìŠµí•˜ë©´ ì™„ë²½í•  ê±°ì˜ˆìš”'],
            'color': 'lightgreen',
            'score': 'A'
        }
    elif accuracy >= 70:
        return {
            'level': 'good',
            'message': f'ì¢‹ì€ "{target_sign}" ìˆ˜ì–´ì…ë‹ˆë‹¤! ğŸ’ª',
            'suggestions': [
                'ì†ê°€ë½ ìœ„ì¹˜ë¥¼ ì¡°ê¸ˆ ë” ì •í™•í•˜ê²Œ í•´ë³´ì„¸ìš”',
                'ì†ëª©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ì§€í•˜ì„¸ìš”'
            ],
            'color': 'blue',
            'score': 'B+'
        }
    elif accuracy >= 60:
        return {
            'level': 'fair',
            'message': f'"{target_sign}" ìˆ˜ì–´ë¥¼ ì—°ìŠµ ì¤‘ì´ë„¤ìš” ğŸ¤”',
            'suggestions': [
                'ì† ëª¨ì–‘ì„ ë” ëª…í™•í•˜ê²Œ í•´ë³´ì„¸ìš”',
                'ì°¸ê³  ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”',
                'ì²œì²œíˆ ì •í™•í•˜ê²Œ í•´ë³´ì„¸ìš”'
            ],
            'color': 'orange',
            'score': 'B'
        }
    else:
        return {
            'level': 'needs_improvement',
            'message': 'ì† ëª¨ì–‘ì„ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”',
            'suggestions': [
                'ì¹´ë©”ë¼ì™€ ì ì ˆí•œ ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ì„¸ìš”',
                'ì¡°ëª…ì´ ì¶©ë¶„í•œ ê³³ì—ì„œ ì‹œë„í•˜ì„¸ìš”',
                'ì†ì„ ì¹´ë©”ë¼ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”'
            ],
            'color': 'red',
            'score': 'C'
        }

# ===== ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ API =====

@recognition_bp.route('/api/recognition/real-time', methods=['POST'])
@jwt_required()
def real_time_recognition():
    """ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ (ë‹¨ì¼ ì´ë¯¸ì§€)"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json()
        
        # í•„ìˆ˜ ë°ì´í„° í™•ì¸
        if not data.get('image_data'):
            return jsonify({'error': 'ì´ë¯¸ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
        
        language = data.get('language', 'ksl')
        
        # ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        if not model_initialized or ksl_model is None:
            return jsonify({
                'error': 'AI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                'model_available': False
            }), 503
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¸ì‹
        result = recognize_sign_from_image(data['image_data'], language)
        
        return jsonify({
            'recognition_result': result,
            'timestamp': datetime.utcnow().isoformat(),
            'model_available': True
        }), 200
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def recognize_sign_from_image(image_data, language):
    """ì´ë¯¸ì§€ì—ì„œ ìˆ˜ì–´ ì¸ì‹"""
    try:
        # 1. ì´ë¯¸ì§€ ë””ì½”ë”©
        image = decode_base64_image(image_data)
        if image is None:
            return {
                'recognized_sign': None,
                'confidence': 0.0,
                'hand_detected': False,
                'error': 'ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨'
            }
        
        # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # 3. MediaPipeë¡œ ì† ì¸ì‹
        results = hands.process(image_rgb)
        
        if not results.multi_hand_landmarks:
            return {
                'recognized_sign': None,
                'confidence': 0.0,
                'hand_detected': False,
                'error': 'ì†ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ'
            }
        
        # 4. ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
        hand_landmarks = results.multi_hand_landmarks[0]
        coords = []
        for landmark in hand_landmarks.landmark:
            coords.extend([landmark.x, landmark.y])
        
        # 5. AI ëª¨ë¸ ì˜ˆì¸¡ (H5 ëª¨ë¸)
        input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
        prediction = ksl_model.predict(input_data, verbose=0)
        
        # 6. ê²°ê³¼ ë¶„ì„
        predicted_idx = np.argmax(prediction)
        confidence_score = float(np.max(prediction))
        
        if 0 <= predicted_idx < len(labels_ksl):
            recognized_sign = labels_ksl[predicted_idx]
        else:
            recognized_sign = "UNKNOWN"
        
        return {
            'recognized_sign': recognized_sign,
            'confidence': round(confidence_score, 3),
            'hand_detected': True,
            'prediction_index': int(predicted_idx),
            'all_predictions': prediction.tolist()
        }
        
    except Exception as e:
        return {
            'recognized_sign': None,
            'confidence': 0.0,
            'hand_detected': False,
            'error': str(e)
        }

# ===== ì†ëª¨ì–‘ ë¶„ì„ API =====

@recognition_bp.route('/api/recognition/analyze-hand', methods=['POST'])
@jwt_required()
def analyze_hand_shape():
    """ì†ëª¨ì–‘ ë¶„ì„ ë° ì •í™•ë„ ì¸¡ì •"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json()
        
        # í•„ìˆ˜ ë°ì´í„° í™•ì¸
        required_fields = ['target_sign', 'language']
        for field in required_fields:
            if not data.get(field):
                return jsonify({'error': f'{field}ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.'}), 400
        
        # ì†ëª¨ì–‘ ë¶„ì„ ìˆ˜í–‰
        analysis_result = analyze_sign_accuracy(
            data.get('image_data', ''),
            data['target_sign'],
            data['language']
        )
        
        return jsonify({
            'analysis': analysis_result,
            'message': 'ì†ëª¨ì–‘ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'model_type': 'H5'
        }), 200
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ===== ëª¨ë¸ ìƒíƒœ í™•ì¸ API =====

@recognition_bp.route('/api/recognition/model-status', methods=['GET'])
def get_model_status():
    """AI ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    try:
        status = {
            'model_initialized': model_initialized,
            'ksl_model_available': ksl_model is not None,
            'mediapipe_available': hands is not None,
            'model_path': KSL_MODEL_PATH,
            'model_type': 'H5 (Keras)',
            'labels_count': len(labels_ksl) if labels_ksl is not None else 0
        }
        
        if labels_ksl is not None:
            status['available_signs'] = labels_ksl.tolist()
        
        return jsonify(status), 200
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500