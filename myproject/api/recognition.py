# - ì†ëª¨ì–‘ ë¶„ì„ ë° ì„¸ì…˜ ê´€ë¦¬ API (H5 ëª¨ë¸ ë²„ì „)
from flask import Blueprint, request, jsonify
from flask_jwt_extended import jwt_required, get_jwt_identity
from auth.models import db, Recognition
from datetime import datetime
import uuid
import random
import tensorflow as tf
import mediapipe as mp
import cv2
import numpy as np
import os
import base64
from PIL import Image
import io

recognition_bp = Blueprint('recognition', __name__)

# ì „ì—­ ì„¸ì…˜ ì €ì¥ì†Œ
active_sessions = {}

# ==== ìŒììŒ/ë³µí•©ëª¨ìŒ ì •ì˜ ====
# ì‹œí€€ìŠ¤ ëª¨ë¸ ì‚¬ìš© (ì—°ì† ë™ì‘ í•„ìš”)
SEQUENCE_SIGNS = ['ã„²', 'ã„¸', 'ã…ƒ', 'ã…†', 'ã…‰', 'ã…˜', 'ã…™', 'ã…', 'ã…']
# ã…š, ã…Ÿ, ã…¢ëŠ” ì •ì  ëª¨ë¸ë¡œ ì¸ì‹ (í•œ ë²ˆì— ê°€ëŠ¥)

DOUBLE_CONSONANT_MAP = {
    'ã„±': 'ã„²',
    'ã„·': 'ã„¸',
    'ã…‚': 'ã…ƒ',
    'ã……': 'ã…†',
    'ã…ˆ': 'ã…‰'
}

# ==== AI ëª¨ë¸ ì´ˆê¸°í™” ====
BASE_DIR = os.path.dirname(os.path.dirname(__file__))  # myproject í´ë”
MODEL_DIR = os.path.join(BASE_DIR, "model")

# ì •ì  ëª¨ë¸ (ê¸°ë³¸ ììŒ/ëª¨ìŒ)
KSL_MODEL_PATH = os.path.join(MODEL_DIR, "ksl_model.h5")
KSL_LABELS_PATH = os.path.join(MODEL_DIR, "ksl_labels.npy")
KSL_NORM_MEAN_PATH = os.path.join(MODEL_DIR, "ksl_norm_mean.npy")
KSL_NORM_STD_PATH = os.path.join(MODEL_DIR, "ksl_norm_std.npy")

# ì‹œí€€ìŠ¤ ëª¨ë¸ (ìŒììŒ/ë³µí•©ëª¨ìŒ)
KSL_SEQ_MODEL_PATH = os.path.join(MODEL_DIR, "ksl_model_sequence.h5")
KSL_SEQ_LABELS_PATH = os.path.join(MODEL_DIR, "ksl_labels_sequence.npy")
KSL_SEQ_CONFIG_PATH = os.path.join(MODEL_DIR, "ksl_sequence_config.npy")
KSL_SEQ_NORM_MEAN_PATH = os.path.join(MODEL_DIR, "ksl_seq_norm_mean.npy")
KSL_SEQ_NORM_STD_PATH = os.path.join(MODEL_DIR, "ksl_seq_norm_std.npy")

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜
ksl_model = None  # ì •ì  ëª¨ë¸
labels_ksl = None  # ì •ì  ë¼ë²¨
ksl_norm_mean = None  # ì •ì  ëª¨ë¸ ì •ê·œí™” í‰ê· 
ksl_norm_std = None  # ì •ì  ëª¨ë¸ ì •ê·œí™” í‘œì¤€í¸ì°¨
ksl_seq_model = None  # ì‹œí€€ìŠ¤ ëª¨ë¸
labels_ksl_seq = None  # ì‹œí€€ìŠ¤ ë¼ë²¨
seq_max_timesteps = None  # ì‹œí€€ìŠ¤ ìµœëŒ€ í”„ë ˆì„ ìˆ˜
seq_norm_mean = None  # ì‹œí€€ìŠ¤ ì •ê·œí™” í‰ê· 
seq_norm_std = None  # ì‹œí€€ìŠ¤ ì •ê·œí™” í‘œì¤€í¸ì°¨
mp_hands = None
hands = None

# ì‹œí€€ìŠ¤ ë²„í¼ (ì‚¬ìš©ìë³„)
from collections import deque
sequence_buffers = {}  # {user_id: deque}

def initialize_ai_models():
    """AI ëª¨ë¸ ì´ˆê¸°í™” (í•˜ì´ë¸Œë¦¬ë“œ: ì •ì  + ì‹œí€€ìŠ¤)"""
    global ksl_model, labels_ksl, ksl_norm_mean, ksl_norm_std, ksl_seq_model, labels_ksl_seq, seq_max_timesteps, seq_norm_mean, seq_norm_std, mp_hands, hands
    
    try:
        # 1. ì •ì  ëª¨ë¸ ë¡œë”© (ê¸°ë³¸ ììŒ/ëª¨ìŒ)
        ksl_model = tf.keras.models.load_model(KSL_MODEL_PATH)
        labels_ksl = np.load(KSL_LABELS_PATH, allow_pickle=True)
        
        # ì •ê·œí™” í†µê³„ ë¡œë“œ (ì •ì  ëª¨ë¸ìš©)
        if os.path.exists(KSL_NORM_MEAN_PATH) and os.path.exists(KSL_NORM_STD_PATH):
            ksl_norm_mean = np.load(KSL_NORM_MEAN_PATH)
            ksl_norm_std = np.load(KSL_NORM_STD_PATH)
            print(f"âœ… ì •ì  ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {len(labels_ksl)}ê°œ ë¼ë²¨ (ì •ê·œí™” ì ìš©)")
        else:
            print(f"âš ï¸ ì •ê·œí™” íŒŒì¼ ì—†ìŒ - ì •ì  ëª¨ë¸ ì •í™•ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
            print(f"âœ… ì •ì  ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {len(labels_ksl)}ê°œ ë¼ë²¨ (ì •ê·œí™” ì—†ìŒ)")
        
        # 2. ì‹œí€€ìŠ¤ ëª¨ë¸ ë¡œë”© (ìŒììŒ/ë³µí•©ëª¨ìŒ)
        if os.path.exists(KSL_SEQ_MODEL_PATH):
            ksl_seq_model = tf.keras.models.load_model(KSL_SEQ_MODEL_PATH)
            labels_ksl_seq = np.load(KSL_SEQ_LABELS_PATH, allow_pickle=True)
            seq_max_timesteps = int(np.load(KSL_SEQ_CONFIG_PATH))
            
            # ì •ê·œí™” í†µê³„ ë¡œë“œ
            if os.path.exists(KSL_SEQ_NORM_MEAN_PATH) and os.path.exists(KSL_SEQ_NORM_STD_PATH):
                seq_norm_mean = np.load(KSL_SEQ_NORM_MEAN_PATH)
                seq_norm_std = np.load(KSL_SEQ_NORM_STD_PATH)
                print(f"âœ… ì‹œí€€ìŠ¤ ì •ê·œí™” í†µê³„ ë¡œë“œ ì„±ê³µ")
            else:
                print("âš ï¸ ì‹œí€€ìŠ¤ ì •ê·œí™” í†µê³„ ì—†ìŒ - ì •ê·œí™” ì—†ì´ ì§„í–‰")
            
            print(f"âœ… ì‹œí€€ìŠ¤ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {len(labels_ksl_seq)}ê°œ ë¼ë²¨ (max_timesteps={seq_max_timesteps})")
        else:
            print("âš ï¸ ì‹œí€€ìŠ¤ ëª¨ë¸ ì—†ìŒ - ìŒììŒ/ë³µí•©ëª¨ìŒì€ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬")
        
        # 3. MediaPipe ì´ˆê¸°í™” (ì–‘ì† ì§€ì›)
        mp_hands = mp.solutions.hands
        hands = mp_hands.Hands(
            static_image_mode=False,  # ì‹œí€€ìŠ¤ ì§€ì›ì„ ìœ„í•´ False
            max_num_hands=2,  # ì–‘ì† ì§€ì›
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        print("âœ… í•˜ì´ë¸Œë¦¬ë“œ AI ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")
        print(f"   - ì •ì  ëª¨ë¸: {KSL_MODEL_PATH}")
        print(f"   - ì‹œí€€ìŠ¤ ëª¨ë¸: {KSL_SEQ_MODEL_PATH}")
        return True
        
    except Exception as e:
        print(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

# ëª¨ë¸ ì´ˆê¸°í™” ì‹¤í–‰
model_initialized = initialize_ai_models()

def decode_base64_image(image_data):
    """Base64 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ OpenCV ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    try:
        # Base64 í—¤ë” ì œê±° (data:image/jpeg;base64, ë¶€ë¶„)
        if ',' in image_data:
            image_data = image_data.split(',')[1]
        
        # Base64 ë””ì½”ë”©
        image_bytes = base64.b64decode(image_data)
        
        # PIL Imageë¡œ ë³€í™˜
        pil_image = Image.open(io.BytesIO(image_bytes))
        
        # OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        opencv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        
        return opencv_image
        
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        return None

def analyze_sign_accuracy(image_data, target_sign, language, user_id=None):
    """í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì–´ ì •í™•ë„ ë¶„ì„ (ì •ì  + ì‹œí€€ìŠ¤)"""
    
    # ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° í´ë°±
    if not model_initialized or ksl_model is None:
        print("âš ï¸ AI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ. í´ë°± ëª¨ë“œ ì‚¬ìš©")
        return fallback_analysis(target_sign, language)
    
    # ì‹œí€€ìŠ¤ ëª¨ë¸ì´ í•„ìš”í•œ ê²½ìš° (ìŒììŒ/ë³µí•©ëª¨ìŒ)
    if target_sign in SEQUENCE_SIGNS:
        print(f"ğŸ”„ ì‹œí€€ìŠ¤ ì‚¬ì¸ ê°ì§€: {target_sign}")
        
        # ì‹œí€€ìŠ¤ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°˜í™˜
        if ksl_seq_model is None:
            print("âŒ ì‹œí€€ìŠ¤ ëª¨ë¸ ì—†ìŒ - í•™ìŠµ í•„ìš”")
            return {
                'accuracy': 0.0,
                'confidence': 0.0,
                'feedback': {
                    'level': 'error',
                    'message': f'"{target_sign}" ì¸ì‹ì„ ìœ„í•œ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',
                    'suggestions': ['ì‹œí€€ìŠ¤ ëª¨ë¸ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤', 'ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”'],
                    'color': 'red',
                    'score': 'F'
                },
                'hand_detected': False,
                'target_sign': target_sign,
                'predicted_sign': None,
                'is_correct': False,
                'language': language,
                'model_type': 'sequence_not_available',
                'error': 'Sequence model not loaded'
            }
        
        result = analyze_sequence_sign(image_data, target_sign, language, user_id)
        print(f"ï¿½ ë¸ì‹œí€€ìŠ¤ ë¶„ì„ ê²°ê³¼: predicted={result.get('predicted_sign')}, accuracy={result.get('accuracy')}, collecting={result.get('collecting')}")
        return result
    
    # ì •ì  ëª¨ë¸ ì‚¬ìš© (ê¸°ë³¸ ììŒ/ëª¨ìŒ)
    print(f"ğŸ“· ì •ì  ëª¨ë¸ ì‚¬ìš©: {target_sign}")
    return analyze_static_sign(image_data, target_sign, language)

def analyze_sequence_sign(image_data, target_sign, language, user_id):
    """ì‹œí€€ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•œ ìˆ˜ì–´ ë¶„ì„ (ìŒììŒ/ë³µí•©ëª¨ìŒ)"""
    
    print(f"ğŸ¬ analyze_sequence_sign ì‹œì‘: target={target_sign}, user_id={user_id}")
    
    try:
        if user_id is None:
            user_id = "anonymous"
        
        # ì‚¬ìš©ìë³„ ì‹œí€€ìŠ¤ ë²„í¼ ì´ˆê¸°í™”
        if user_id not in sequence_buffers:
            print(f"ğŸ†• ìƒˆ ë²„í¼ ìƒì„±: user_id={user_id}")
            sequence_buffers[user_id] = {
                'buffer': deque(maxlen=seq_max_timesteps),
                'prev_xy': {},
                'target': target_sign
            }
        
        user_buffer = sequence_buffers[user_id]
        
        # ëª©í‘œê°€ ë°”ë€Œë©´ ë²„í¼ ì´ˆê¸°í™” (ì¤‘ìš”!)
        if user_buffer.get('target') != target_sign:
            print(f"ğŸ”„ ëª©í‘œ ë³€ê²½: {user_buffer.get('target')} â†’ {target_sign}, ë²„í¼ ì´ˆê¸°í™”")
            user_buffer['buffer'] = deque(maxlen=seq_max_timesteps)  # ìƒˆ deque ìƒì„±
            user_buffer['prev_xy'] = {}  # ìƒˆ dict ìƒì„±
            user_buffer['target'] = target_sign
            print(f"âœ… ë²„í¼ ì´ˆê¸°í™” ì™„ë£Œ: í¬ê¸°={len(user_buffer['buffer'])}")
        # 1. ì´ë¯¸ì§€ ë””ì½”ë”©
        print(f"ğŸ“¸ Step 1: ì´ë¯¸ì§€ ë””ì½”ë”© ì‹œì‘")
        if not image_data:
            print("âš ï¸ image_data ì—†ìŒ")
            return {
                'accuracy': 0.0,
                'confidence': 0.0,
                'feedback': {
                    'level': 'error',
                    'message': 'ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤',
                    'suggestions': ['ì¹´ë©”ë¼ë¥¼ í™•ì¸í•˜ì„¸ìš”'],
                    'color': 'red',
                    'score': 'F'
                },
                'hand_detected': False,
                'target_sign': target_sign,
                'predicted_sign': None,
                'is_correct': False,
                'language': language,
                'model_type': 'sequence_no_image',
                'error': 'No image data'
            }
        
        image = decode_base64_image(image_data)
        if image is None:
            print("âš ï¸ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨")
            return fallback_analysis(target_sign, language)
        
        print(f"âœ… ì´ë¯¸ì§€ ë””ì½”ë”© ì„±ê³µ: {image.shape}")
        
        # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        print(f"ğŸ¨ Step 2: ì´ë¯¸ì§€ ì „ì²˜ë¦¬")
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # 3. MediaPipeë¡œ ì† ì¸ì‹
        print(f"ğŸ‘‹ Step 3: MediaPipe ì† ì¸ì‹")
        results = hands.process(image_rgb)
        print(f"âœ… MediaPipe ì²˜ë¦¬ ì™„ë£Œ: ì† ê°ì§€={results.multi_hand_landmarks is not None}")
        
        if not results.multi_hand_landmarks:
            # ì†ì´ ì—†ìœ¼ë©´ ë²„í¼ ì´ˆê¸°í™”
            if len(user_buffer['buffer']) > 0:
                print(f"ğŸ‘‹ ì† ê°ì§€ ì•ˆë¨ - ë²„í¼ ì´ˆê¸°í™” (ì´ì „ í¬ê¸°: {len(user_buffer['buffer'])})")
                user_buffer['buffer'].clear()
                user_buffer['prev_xy'].clear()
            return {
                'accuracy': 0.0,
                'confidence': 0.0,
                'feedback': generate_detailed_feedback(0.0, target_sign, language),
                'hand_detected': False,
                'target_sign': target_sign,
                'language': language,
                'model_type': 'sequence',
                'buffer_size': 0,
                'error': 'ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'
            }
        
        # 4. ì† ëœë“œë§ˆí¬ ì¶”ì¶œ (wrist, index_tipë§Œ ì‚¬ìš©)
        hand_landmarks = results.multi_hand_landmarks[0]
        lms = hand_landmarks.landmark
        
        # ì‚¬ìš©í•  ëœë“œë§ˆí¬ (capture_sequence.pyì™€ ë™ì¼)
        USE_LANDMARKS = {0: "wrist", 8: "index_tip"}
        
        frame_features = []
        spd_sum_total = 0.0
        
        # ì†ë„ ê³„ì‚°
        for lm_id in USE_LANDMARKS.keys():
            lm = lms[lm_id]
            x, y = float(lm.x), float(lm.y)
            
            dx = dy = 0.0
            if lm_id in user_buffer['prev_xy']:
                dx = x - user_buffer['prev_xy'][lm_id][0]
                dy = y - user_buffer['prev_xy'][lm_id][1]
            
            spd = abs(dx) + abs(dy)
            spd_sum_total += spd
            user_buffer['prev_xy'][lm_id] = (x, y)
        
        # íŠ¹ì§• ë²¡í„° ìƒì„±
        for lm_id in USE_LANDMARKS.keys():
            lm = lms[lm_id]
            x, y = float(lm.x), float(lm.y)
            
            dx = dy = 0.0
            if lm_id in user_buffer['prev_xy']:
                prev_x, prev_y = user_buffer['prev_xy'][lm_id]
                dx = x - prev_x
                dy = y - prev_y
            
            frame_features.extend([x, y, dx, dy, spd_sum_total])
        
        # ë²„í¼ì— ì¶”ê°€
        user_buffer['buffer'].append(frame_features)
        
        # ì¶©ë¶„í•œ í”„ë ˆì„ì´ ëª¨ì´ë©´ ì˜ˆì¸¡
        buffer_size = len(user_buffer['buffer'])
        min_frames = 5  # ìµœì†Œ 5í”„ë ˆì„ (ë” ì•ˆì •ì ì¸ ì¸ì‹)
        
        print(f"ğŸ”¢ ë²„í¼ ìƒíƒœ: {buffer_size}/{seq_max_timesteps} í”„ë ˆì„ (ìµœì†Œ: {min_frames}, ëª©í‘œ: {target_sign})")
        
        if buffer_size < min_frames:
            # í”„ë ˆì„ ìˆ˜ì§‘ ì¤‘
            progress_ratio = buffer_size / min_frames
            collecting_accuracy = 50 + (progress_ratio * 30)  # 50~80%
            
            return {
                'accuracy': collecting_accuracy,
                'confidence': 0.5,
                'feedback': {
                    'level': 'collecting',
                    'message': f'"{target_sign}" ë™ì‘ì„ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤... ({buffer_size}/{min_frames})',
                    'suggestions': [
                        'ì²œì²œíˆ ë™ì‘ì„ ê³„ì†í•˜ì„¸ìš”',
                        'ì†ì„ ì¹´ë©”ë¼ì— ì˜ ë³´ì´ê²Œ ìœ ì§€í•˜ì„¸ìš”'
                    ],
                    'color': 'blue',
                    'score': '-'
                },
                'hand_detected': True,
                'target_sign': target_sign,
                'predicted_sign': None,
                'is_correct': False,
                'language': language,
                'model_type': 'sequence',
                'buffer_size': buffer_size,
                'collecting': True
            }
        
        # 5. ì‹œí€€ìŠ¤ íŒ¨ë”© ë° ì •ê·œí™”
        feature_dim = len(frame_features)
        seq_array = np.zeros((1, seq_max_timesteps, feature_dim), dtype=np.float32)
        seq_len = len(user_buffer['buffer'])
        seq_array[0, :seq_len, :] = list(user_buffer['buffer'])
        
        # ì •ê·œí™” ì ìš©
        if seq_norm_mean is not None and seq_norm_std is not None:
            seq_array = (seq_array - seq_norm_mean) / seq_norm_std
            print(f"âœ… ì •ê·œí™” ì ìš© ì™„ë£Œ")
        else:
            print("âš ï¸ ì •ê·œí™” í†µê³„ ì—†ìŒ - ì •ê·œí™” ì—†ì´ ì˜ˆì¸¡")
        
        # 6. AI ëª¨ë¸ ì˜ˆì¸¡
        prediction = ksl_seq_model.predict(seq_array, verbose=0)
        
        # 7. ê²°ê³¼ ë¶„ì„
        predicted_idx = np.argmax(prediction)
        confidence_score = float(np.max(prediction))
        
        if 0 <= predicted_idx < len(labels_ksl_seq):
            predicted_sign = labels_ksl_seq[predicted_idx]
        else:
            predicted_sign = "UNKNOWN"
        
        # 8. ì •í™•ë„ ê³„ì‚°
        is_correct = predicted_sign == target_sign
        
        # ì •í™•ë„ ê³„ì‚° (ì—„ê²©í•˜ê²Œ)
        if is_correct:
            accuracy = confidence_score * 100
        else:
            # í‹€ë ¸ìœ¼ë©´ ë‚®ì€ ì ìˆ˜
            accuracy = confidence_score * 50
        
        # 9. í”¼ë“œë°± ìƒì„±
        feedback = generate_detailed_feedback(accuracy, target_sign, language)
        
        # í‹€ë ¸ì„ ë•Œ ë©”ì‹œì§€
        if not is_correct:
            feedback['message'] = f'"{predicted_sign}"ì´(ê°€) ì¸ì‹ë˜ì—ˆìŠµë‹ˆë‹¤. "{target_sign}"ì„(ë¥¼) ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”'
            feedback['suggestions'] = [
                f'ì˜ˆì¸¡: {predicted_sign} â‰  ëª©í‘œ: {target_sign}',
                'ë™ì‘ì„ ì²œì²œíˆ ì •í™•í•˜ê²Œ ìˆ˜í–‰í•˜ì„¸ìš”',
                'ì°¸ê³  ì˜ìƒì„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”'
            ]
        
        return {
            'accuracy': round(accuracy, 1),
            'confidence': round(confidence_score, 2),
            'feedback': feedback,
            'hand_detected': True,
            'target_sign': target_sign,
            'predicted_sign': predicted_sign,
            'is_correct': is_correct,
            'language': language,
            'model_type': 'sequence',
            'buffer_size': buffer_size
        }
        
    except Exception as e:
        print(f"âŒ ì‹œí€€ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        
        # ì—ëŸ¬ ì •ë³´ë¥¼ í¬í•¨í•œ fallback
        fallback_result = fallback_analysis(target_sign, language)
        fallback_result['error'] = str(e)
        fallback_result['error_type'] = 'sequence_analysis_error'
        return fallback_result

def analyze_static_sign(image_data, target_sign, language):
    """ì •ì  ëª¨ë¸ì„ ì‚¬ìš©í•œ ìˆ˜ì–´ ë¶„ì„ (ê¸°ë³¸ ììŒ/ëª¨ìŒ)"""
    
    try:
        # 1. ì´ë¯¸ì§€ ë””ì½”ë”©
        if not image_data:
            print("âš ï¸ ì´ë¯¸ì§€ ë°ì´í„° ì—†ìŒ")
            return fallback_analysis(target_sign, language)
        
        image = decode_base64_image(image_data)
        if image is None:
            print("âš ï¸ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨")
            return fallback_analysis(target_sign, language)
        
        # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # 3. MediaPipeë¡œ ì† ì¸ì‹
        results = hands.process(image_rgb)
        
        if not results.multi_hand_landmarks:
            return {
                'accuracy': 0.0,
                'confidence': 0.0,
                'feedback': generate_detailed_feedback(0.0, target_sign, language),
                'hand_detected': False,
                'target_sign': target_sign,
                'language': language,
                'error': 'ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'
            }
        
        # 4. ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
        hand_landmarks = results.multi_hand_landmarks[0]
        coords = []
        for landmark in hand_landmarks.landmark:
            coords.extend([landmark.x, landmark.y])
        
        # 5. ì •ê·œí™” ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ)
        input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
        if ksl_norm_mean is not None and ksl_norm_std is not None:
            input_data = (input_data - ksl_norm_mean) / ksl_norm_std
        
        # 6. AI ëª¨ë¸ ì˜ˆì¸¡ (H5 ëª¨ë¸)
        prediction = ksl_model.predict(input_data, verbose=0)
        
        # 7. ê²°ê³¼ ë¶„ì„
        predicted_idx = np.argmax(prediction)
        confidence_score = float(np.max(prediction))
        
        if 0 <= predicted_idx < len(labels_ksl):
            predicted_sign = labels_ksl[predicted_idx]
        else:
            predicted_sign = "UNKNOWN"
        
        # 8. ìŒììŒ ì²˜ë¦¬ ë¡œì§
        # ëª©í‘œê°€ ìŒììŒì´ê³ , ì˜ˆì¸¡ì´ ê¸°ë³¸ ììŒì¸ ê²½ìš° ì²˜ë¦¬
        is_double_consonant_target = target_sign in DOUBLE_CONSONANT_MAP.values()
        base_consonant = None
        
        if is_double_consonant_target:
            # ìŒììŒì˜ ê¸°ë³¸ ììŒ ì°¾ê¸° (ì˜ˆ: ã„¸ â†’ ã„·)
            for base, double in DOUBLE_CONSONANT_MAP.items():
                if double == target_sign:
                    base_consonant = base
                    break
            
            # ê¸°ë³¸ ììŒì„ ì¸ì‹í•œ ê²½ìš°ë„ ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬
            if predicted_sign == base_consonant:
                print(f"ğŸ¯ ìŒììŒ í•™ìŠµ: {target_sign} ëª©í‘œ, {predicted_sign} ì¸ì‹ â†’ ë¶€ë¶„ ì ìˆ˜")
                # ê¸°ë³¸ ììŒ ì¸ì‹ ì‹œ 70% ì •í™•ë„ ë¶€ì—¬
                accuracy = confidence_score * 70
                is_correct = False  # ì™„ì „íˆ ë§ì§€ëŠ” ì•ŠìŒ
                feedback = generate_detailed_feedback(accuracy, target_sign, language)
                feedback['message'] = f'"{predicted_sign}" ëª¨ì–‘ì´ ë§ì•„ìš”! ì¡°ê¸ˆ ë” ê°•í•˜ê²Œ í•´ì„œ "{target_sign}"ì„ ë§Œë“¤ì–´ë³´ì„¸ìš” ğŸ’ª'
                feedback['suggestions'] = [
                    f'{predicted_sign} ëª¨ì–‘ì—ì„œ ì†ì— ë” í˜ì„ ì£¼ì„¸ìš”',
                    f'ì†ê°€ë½ì„ ë” êµ½í˜€ì„œ {target_sign}ì„ í‘œí˜„í•˜ì„¸ìš”',
                    'ìŒììŒì€ ê¸°ë³¸ ììŒë³´ë‹¤ ê°•í•œ ëŠë‚Œì…ë‹ˆë‹¤'
                ]
                
                return {
                    'accuracy': round(accuracy, 1),
                    'confidence': round(confidence_score, 2),
                    'feedback': feedback,
                    'hand_detected': True,
                    'target_sign': target_sign,
                    'predicted_sign': predicted_sign,
                    'is_correct': is_correct,
                    'is_partial_match': True,
                    'base_consonant': base_consonant,
                    'language': language
                }
        
        # 9. ì •í™•ë„ ê³„ì‚° (ì¼ë°˜ ì¼€ì´ìŠ¤)
        is_correct = predicted_sign == target_sign
        accuracy = confidence_score * 100 if is_correct else max(0, confidence_score * 50)
        
        # 10. í”¼ë“œë°± ìƒì„±
        feedback = generate_detailed_feedback(accuracy, target_sign, language)
        
        return {
            'accuracy': round(accuracy, 1),
            'confidence': round(confidence_score, 2),
            'feedback': feedback,
            'hand_detected': True,
            'target_sign': target_sign,
            'predicted_sign': predicted_sign,
            'is_correct': is_correct,
            'language': language
        }
        
    except Exception as e:
        print(f"âŒ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return fallback_analysis(target_sign, language)

def fallback_analysis(target_sign, language):
    """AI ëª¨ë¸ ì‹¤íŒ¨ ì‹œ í´ë°± ë¶„ì„"""
    # ìˆ˜ì–´ë³„ ë‚œì´ë„ ì„¤ì •
    sign_difficulty = {
        'A': 0.9, 'B': 0.8, 'C': 0.7, 'D': 0.8, 'E': 0.9,
        'F': 0.7, 'G': 0.6, 'H': 0.8, 'I': 0.9, 'J': 0.6,
        'Hello': 0.6, 'Thank you': 0.5, 'Please': 0.6,
        'ã„±': 0.8, 'ã„´': 0.7, 'ã„·': 0.8, 'ã„¹': 0.6, 'ã…': 0.7,
        'ã„²': 0.6, 'ã„¸': 0.6, 'ã…ƒ': 0.6, 'ã…†': 0.7, 'ã…‰': 0.6,  # ìŒììŒì€ ë” ì–´ë ¤ì›€
        'ì•ˆë…•í•˜ì„¸ìš”': 0.5, 'ê°ì‚¬í•©ë‹ˆë‹¤': 0.4
    }
    
    # ê¸°ë³¸ ì •í™•ë„ ê³„ì‚° (í´ë°± ëª¨ë“œ)
    base_accuracy = 75.0
    difficulty_factor = sign_difficulty.get(target_sign, 0.7)
    random_factor = random.uniform(0.7, 1.3)
    language_factor = 1.0 if language == 'asl' else 0.95
    
    final_accuracy = min(100.0, base_accuracy * difficulty_factor * random_factor * language_factor)
    confidence = final_accuracy / 100.0
    
    # í”¼ë“œë°± ìƒì„±
    feedback = generate_detailed_feedback(final_accuracy, target_sign, language)
    
    return {
        'accuracy': round(final_accuracy, 1),
        'confidence': round(confidence, 2),
        'feedback': feedback,
        'hand_detected': True,
        'target_sign': target_sign,
        'language': language,
        'fallback_mode': True
    }

def generate_detailed_feedback(accuracy, target_sign, language):
    """ìƒì„¸ í”¼ë“œë°± ìƒì„±"""
    
    if accuracy >= 90:
        return {
            'level': 'excellent',
            'message': f'ì™„ë²½í•œ "{target_sign}" ìˆ˜ì–´ì…ë‹ˆë‹¤! ğŸ‰',
            'suggestions': ['í›Œë¥­í•´ìš”! ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”'],
            'color': 'green',
            'score': 'A+'
        }
    elif accuracy >= 80:
        return {
            'level': 'very_good',
            'message': f'ì•„ì£¼ ì¢‹ì€ "{target_sign}" ìˆ˜ì–´ì…ë‹ˆë‹¤! ğŸ‘',
            'suggestions': ['ê±°ì˜ ì™„ë²½í•´ìš”!', 'ì¡°ê¸ˆë§Œ ë” ì—°ìŠµí•˜ë©´ ì™„ë²½í•  ê±°ì˜ˆìš”'],
            'color': 'lightgreen',
            'score': 'A'
        }
    elif accuracy >= 70:
        return {
            'level': 'good',
            'message': f'ì¢‹ì€ "{target_sign}" ìˆ˜ì–´ì…ë‹ˆë‹¤! ğŸ’ª',
            'suggestions': [
                'ì†ê°€ë½ ìœ„ì¹˜ë¥¼ ì¡°ê¸ˆ ë” ì •í™•í•˜ê²Œ í•´ë³´ì„¸ìš”',
                'ì†ëª©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ì§€í•˜ì„¸ìš”'
            ],
            'color': 'blue',
            'score': 'B+'
        }
    elif accuracy >= 60:
        return {
            'level': 'fair',
            'message': f'"{target_sign}" ìˆ˜ì–´ë¥¼ ì—°ìŠµ ì¤‘ì´ë„¤ìš” ğŸ¤”',
            'suggestions': [
                'ì† ëª¨ì–‘ì„ ë” ëª…í™•í•˜ê²Œ í•´ë³´ì„¸ìš”',
                'ì°¸ê³  ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”',
                'ì²œì²œíˆ ì •í™•í•˜ê²Œ í•´ë³´ì„¸ìš”'
            ],
            'color': 'orange',
            'score': 'B'
        }
    else:
        return {
            'level': 'needs_improvement',
            'message': 'ì† ëª¨ì–‘ì„ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”',
            'suggestions': [
                'ì¹´ë©”ë¼ì™€ ì ì ˆí•œ ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ì„¸ìš”',
                'ì¡°ëª…ì´ ì¶©ë¶„í•œ ê³³ì—ì„œ ì‹œë„í•˜ì„¸ìš”',
                'ì†ì„ ì¹´ë©”ë¼ ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ì„¸ìš”'
            ],
            'color': 'red',
            'score': 'C'
        }

# ===== ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ API =====

@recognition_bp.route('/api/recognition/real-time', methods=['POST'])
@jwt_required()
def real_time_recognition():
    """ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ (ë‹¨ì¼ ì´ë¯¸ì§€)"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json()
        
        # í•„ìˆ˜ ë°ì´í„° í™•ì¸
        if not data.get('image_data'):
            return jsonify({'error': 'ì´ë¯¸ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
        
        language = data.get('language', 'ksl')
        
        # ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
        if not model_initialized or ksl_model is None:
            return jsonify({
                'error': 'AI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                'model_available': False
            }), 503
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¸ì‹
        result = recognize_sign_from_image(data['image_data'], language)
        
        return jsonify({
            'recognition_result': result,
            'timestamp': datetime.utcnow().isoformat(),
            'model_available': True
        }), 200
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def recognize_sign_from_image(image_data, language):
    """ì´ë¯¸ì§€ì—ì„œ ìˆ˜ì–´ ì¸ì‹"""
    try:
        # 1. ì´ë¯¸ì§€ ë””ì½”ë”©
        image = decode_base64_image(image_data)
        if image is None:
            return {
                'recognized_sign': None,
                'confidence': 0.0,
                'hand_detected': False,
                'error': 'ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨'
            }
        
        # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # 3. MediaPipeë¡œ ì† ì¸ì‹
        results = hands.process(image_rgb)
        
        if not results.multi_hand_landmarks:
            return {
                'recognized_sign': None,
                'confidence': 0.0,
                'hand_detected': False,
                'error': 'ì†ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ'
            }
        
        # 4. ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
        hand_landmarks = results.multi_hand_landmarks[0]
        coords = []
        for landmark in hand_landmarks.landmark:
            coords.extend([landmark.x, landmark.y])
        
        # 5. ì •ê·œí™” ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ)
        input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
        if ksl_norm_mean is not None and ksl_norm_std is not None:
            input_data = (input_data - ksl_norm_mean) / ksl_norm_std
        
        # 6. AI ëª¨ë¸ ì˜ˆì¸¡ (H5 ëª¨ë¸)
        prediction = ksl_model.predict(input_data, verbose=0)
        
        # 7. ê²°ê³¼ ë¶„ì„
        predicted_idx = np.argmax(prediction)
        confidence_score = float(np.max(prediction))
        
        if 0 <= predicted_idx < len(labels_ksl):
            recognized_sign = labels_ksl[predicted_idx]
        else:
            recognized_sign = "UNKNOWN"
        
        return {
            'recognized_sign': recognized_sign,
            'confidence': round(confidence_score, 3),
            'hand_detected': True,
            'prediction_index': int(predicted_idx),
            'all_predictions': prediction.tolist()
        }
        
    except Exception as e:
        return {
            'recognized_sign': None,
            'confidence': 0.0,
            'hand_detected': False,
            'error': str(e)
        }

# ===== ì†ëª¨ì–‘ ë¶„ì„ API =====

@recognition_bp.route('/api/recognition/analyze-hand', methods=['POST'])
@jwt_required()
def analyze_hand_shape():
    """ì†ëª¨ì–‘ ë¶„ì„ ë° ì •í™•ë„ ì¸¡ì • (í•˜ì´ë¸Œë¦¬ë“œ)"""
    try:
        user_id = get_jwt_identity()
        data = request.get_json()
        
        # í•„ìˆ˜ ë°ì´í„° í™•ì¸
        required_fields = ['target_sign', 'language']
        for field in required_fields:
            if not data.get(field):
                return jsonify({'error': f'{field}ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.'}), 400
        
        target_sign = data['target_sign']
        language = data['language']
        
        # ì´ë¯¸ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë³´ë‚´ê±°ë‚˜, ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        image_data = data.get('image_data', '')
        
        # ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ í”„ë ˆì„ ë¡œë“œ
        if not image_data:
            import tempfile
            import base64
            
            frame_path = os.path.join(tempfile.gettempdir(), f'ksl_frame_{language}.jpg')
            
            if os.path.exists(frame_path):
                # íŒŒì¼ì—ì„œ ì´ë¯¸ì§€ ì½ê¸°
                frame = cv2.imread(frame_path)
                if frame is not None:
                    # Base64ë¡œ ì¸ì½”ë”©
                    _, buffer = cv2.imencode('.jpg', frame)
                    image_data = 'data:image/jpeg;base64,' + base64.b64encode(buffer).decode('utf-8')
                    print(f"ğŸ“¸ íŒŒì¼ì—ì„œ í”„ë ˆì„ ë¡œë“œ: {frame.shape}")
                else:
                    print(f"âš ï¸ í”„ë ˆì„ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {frame_path}")
            else:
                print(f"âš ï¸ í”„ë ˆì„ íŒŒì¼ ì—†ìŒ: {frame_path}")
        
        # ì†ëª¨ì–‘ ë¶„ì„ ìˆ˜í–‰ (í•˜ì´ë¸Œë¦¬ë“œ)
        analysis_result = analyze_sign_accuracy(
            image_data,
            target_sign,
            language,
            user_id=user_id
        )
        
        # ëª¨ë¸ íƒ€ì… ê²°ì •
        model_type = 'sequence' if target_sign in SEQUENCE_SIGNS else 'static'
        
        return jsonify({
            'analysis': analysis_result,
            'message': 'ì†ëª¨ì–‘ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            'model_type': model_type,
            'is_sequence_sign': target_sign in SEQUENCE_SIGNS
        }), 200
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

# ===== ëª¨ë¸ ìƒíƒœ í™•ì¸ API =====

@recognition_bp.route('/api/recognition/model-status', methods=['GET'])
def get_model_status():
    """AI ëª¨ë¸ ìƒíƒœ í™•ì¸ (í•˜ì´ë¸Œë¦¬ë“œ)"""
    try:
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        files_exist = {
            'ksl_model.h5': os.path.exists(KSL_MODEL_PATH),
            'ksl_labels.npy': os.path.exists(KSL_LABELS_PATH),
            'ksl_model_sequence.h5': os.path.exists(KSL_SEQ_MODEL_PATH),
            'ksl_labels_sequence.npy': os.path.exists(KSL_SEQ_LABELS_PATH),
            'ksl_sequence_config.npy': os.path.exists(KSL_SEQ_CONFIG_PATH)
        }
        
        status = {
            'model_initialized': model_initialized,
            'hybrid_mode': True,
            'mediapipe_available': hands is not None,
            'files_exist': files_exist,
            
            # ì •ì  ëª¨ë¸
            'static_model': {
                'available': ksl_model is not None,
                'path': KSL_MODEL_PATH,
                'labels_count': len(labels_ksl) if labels_ksl is not None else 0,
                'labels': labels_ksl.tolist() if labels_ksl is not None else []
            },
            
            # ì‹œí€€ìŠ¤ ëª¨ë¸
            'sequence_model': {
                'available': ksl_seq_model is not None,
                'path': KSL_SEQ_MODEL_PATH,
                'labels_count': len(labels_ksl_seq) if labels_ksl_seq is not None else 0,
                'labels': labels_ksl_seq.tolist() if labels_ksl_seq is not None else [],
                'max_timesteps': seq_max_timesteps
            },
            
            'sequence_signs': SEQUENCE_SIGNS,
            
            # ë””ë²„ê¹… ì •ë³´
            'debug': {
                'base_dir': BASE_DIR,
                'model_dir': MODEL_DIR,
                'seq_model_loaded': ksl_seq_model is not None,
                'seq_labels_loaded': labels_ksl_seq is not None,
                'seq_config_loaded': seq_max_timesteps is not None
            }
        }
        
        return jsonify(status), 200
        
    except Exception as e:
        import traceback
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

@recognition_bp.route('/api/recognition/clear-buffer', methods=['POST'])
@jwt_required()
def clear_sequence_buffer():
    """ì‹œí€€ìŠ¤ ë²„í¼ ì´ˆê¸°í™”"""
    try:
        user_id = get_jwt_identity()
        
        if user_id in sequence_buffers:
            sequence_buffers[user_id]['buffer'].clear()
            sequence_buffers[user_id]['prev_xy'].clear()
            return jsonify({
                'message': 'ì‹œí€€ìŠ¤ ë²„í¼ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.',
                'user_id': user_id
            }), 200
        else:
            return jsonify({
                'message': 'ì´ˆê¸°í™”í•  ë²„í¼ê°€ ì—†ìŠµë‹ˆë‹¤.',
                'user_id': user_id
            }), 200
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500