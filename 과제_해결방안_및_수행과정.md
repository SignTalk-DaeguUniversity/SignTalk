# SignTalk 프로젝트 - 과제 해결방안 및 수행과정

## 📋 프로젝트 개요

**프로젝트명**: SignTalk - AI 기반 한국 수어 인식 및 학습 플랫폼  
**개발 기간**: 2024년 9월 ~ 2025년 1월 (5개월)  
**목적**: 딥러닝 기반 실시간 한국 수어(KSL) 지문자 인식 시스템 구축 및 대화형 학습 플랫폼 제공

### 기술 스택

#### 백엔드 (Python)
- **프레임워크**: Flask 3.1.2 (RESTful API 서버)
- **AI/ML**: 
  - TensorFlow 2.19.1 (딥러닝 모델 학습 및 추론)
  - MediaPipe 0.10.9 (손 랜드마크 추출)
  - NumPy 1.26.4 (수치 연산)
  - OpenCV 4.10.0 (영상 처리)
- **데이터베이스**: SQLite + Flask-SQLAlchemy 3.1.1
- **인증**: Flask-JWT-Extended 4.7.1 (JWT 토큰 기반)

#### 프론트엔드 (Flutter)
- **프레임워크**: Flutter 3.9.2+ (크로스 플랫폼)
- **상태 관리**: Provider 패턴
- **카메라**: camera 0.10.6 (실시간 영상 캡처)
- **HTTP 통신**: http 1.2.2
- **로컬 저장소**: shared_preferences 2.3.5

#### AI 모델 아키텍처
- **정적 모델**: Dense Neural Network (31개 기본 자모음)
- **시퀀스 모델**: Bidirectional LSTM (9개 쌍자음/복합모음)
- **하이브리드 시스템**: 자동 모델 선택 메커니즘

#### 개발 환경
- **OS**: macOS 14.6, Windows 11, Linux (Ubuntu 22.04)
- **Python**: 3.12.x
- **IDE**: VS Code, Android Studio, Xcode
- **버전 관리**: Git + GitHub

---

## 🎯 핵심 문제 정의 및 분석

### 1. 한국 수어 지문자 체계의 특수성

한국 수어 지문자는 **40개의 자모음**(자음 19개 + 모음 21개)으로 구성되며, 표현 방식에 따라 다음과 같이 분류됩니다:

#### 1.1 정적 표현 (Static Gestures) - 31개
- **기본 자음** (14개): ㄱ, ㄴ, ㄷ, ㄹ, ㅁ, ㅂ, ㅅ, ㅇ, ㅈ, ㅊ, ㅋ, ㅌ, ㅍ, ㅎ
- **기본 모음** (17개): ㅏ, ㅑ, ㅓ, ㅕ, ㅗ, ㅛ, ㅜ, ㅠ, ㅡ, ㅣ, ㅐ, ㅒ, ㅔ, ㅖ, ㅚ, ㅟ, ㅢ
- **특징**: 단일 프레임(정지 영상)으로 인식 가능
- **인식 방법**: MediaPipe 손 랜드마크 21개 좌표 → Dense NN

#### 1.2 동적 표현 (Dynamic Gestures) - 9개
- **쌍자음** (5개): ㄲ, ㄸ, ㅃ, ㅆ, ㅉ
  - 표현 방식: 기본 자음을 **빠르게 2번 반복** (예: ㄱ → ㄱ = ㄲ)
  - 시간: 약 0.5~0.8초
- **복합모음** (4개): ㅘ, ㅙ, ㅝ, ㅞ
  - 표현 방식: 두 모음을 **연속적으로 결합** (예: ㅗ + ㅏ = ㅘ)
  - 시간: 약 0.6~1.0초
- **특징**: 시간적 순서가 중요한 연속 동작
- **인식 방법**: 시퀀스 데이터 수집 → Bidirectional LSTM

### 2. 해결해야 할 핵심 기술 과제

#### 2.1 시간적 의존성 (Temporal Dependency)
**문제**: 쌍자음/복합모음은 단일 프레임이 아닌 **연속된 동작의 시퀀스**로 표현됨
- 예시: ㄲ = [ㄱ 동작] → [짧은 정지] → [ㄱ 동작]
- 기존 정적 모델로는 인식 불가능
- **해결 방안**: LSTM 기반 시퀀스 모델 도입

#### 2.2 모델 이중화 및 자동 선택
**문제**: 40개 자모음 중 31개는 정적, 9개는 동적 → 두 가지 모델 필요
- 정적 모델: 빠른 추론 (단일 프레임)
- 시퀀스 모델: 느린 추론 (여러 프레임 수집 필요)
- **해결 방안**: 하이브리드 시스템 구축 (자동 모델 선택)

#### 2.3 실시간 처리 및 사용자 경험
**문제**: 학습 모드에서 즉각적인 피드백 제공 필요
- 목표 지연 시간: 0.2초 이내
- 카메라 FPS: 30fps 이상
- **해결 방안**: 
  - 경량화된 모델 구조
  - 효율적인 버퍼 관리
  - 비동기 처리

#### 2.4 손모양 유사성 문제
**문제**: 일부 자모음은 손모양이 매우 유사함
- 예시: ㄱ vs ㄴ, ㅏ vs ㅑ
- 미세한 각도 차이로 구분
- **해결 방안**:
  - 충분한 학습 데이터 (각 클래스당 100+ 샘플)
  - 데이터 증강 (회전, 스케일, 노이즈)
  - 정규화 (Z-score normalization)

#### 2.5 다양한 환경 조건
**문제**: 조명, 배경, 손 크기, 피부색 등 변수가 많음
- 실내/실외 조명 차이
- 복잡한 배경
- 다양한 사용자 손 크기
- **해결 방안**:
  - MediaPipe의 정규화된 좌표 사용 (0~1 범위)
  - 다양한 환경에서 데이터 수집
  - 손 랜드마크만 사용 (배경 무관)

### 3. 프로젝트 목표 및 성공 기준

#### 3.1 기능적 목표
- ✅ 40개 한국 수어 지문자 전체 인식
- ✅ 실시간 인식 (지연 시간 0.2초 이내)
- ✅ 학습 모드 (5단계 레벨 시스템)
- ✅ 퀴즈 모드 (4가지 난이도)
- ✅ 진도 관리 및 통계

#### 3.2 성능 목표
- **정적 모델 정확도**: 90% 이상
- **시퀀스 모델 정확도**: 85% 이상
- **FPS**: 30fps 이상 (카메라 스트리밍)
- **인식 지연**: 0.15초 (정적), 0.5초 (시퀀스)

#### 3.3 사용자 경험 목표
- 직관적인 UI/UX
- 실시간 피드백 (정확도, 개선 제안)
- 크로스 플랫폼 지원 (iOS, Android, Web)

---

## 💡 해결방안 및 시스템 설계

### 1. 하이브리드 AI 모델 아키텍처

#### 1.1 정적 모델 (Static Model) - 기본 자모음 31개

**모델 구조**:
```
입력층: 42개 특징 (MediaPipe 손 랜드마크 21개 × (x, y))
├─ Dense(128, activation='relu')
├─ Dropout(0.3)
├─ Dense(64, activation='relu')
├─ Dropout(0.3)
├─ Dense(32, activation='relu')
└─ Dense(31, activation='softmax')  # 31개 클래스

총 파라미터: ~7,000개 (경량 모델)
```

**입력 데이터 전처리**:
```python
# MediaPipe 손 랜드마크 추출
hand_landmarks = mp_hands.process(image).multi_hand_landmarks[0]

# 21개 랜드마크 좌표 추출 (정규화된 0~1 범위)
landmarks = []
for lm in hand_landmarks.landmark:
    landmarks.extend([lm.x, lm.y])  # z 좌표는 사용 안 함

# Z-score 정규화 (학습 시 저장된 평균/표준편차 사용)
landmarks_normalized = (landmarks - ksl_norm_mean) / ksl_norm_std

# 모델 추론
prediction = ksl_model.predict(landmarks_normalized.reshape(1, -1))
predicted_class = np.argmax(prediction)
confidence = np.max(prediction)
```

**학습 데이터 수집**:
- **수집 도구**: `ksl_model_train/hand_capture.py`
- **샘플 수**: 각 자모음당 100~200개
- **수집 조건**:
  - 다양한 손 크기 (성인 남성/여성, 청소년)
  - 다양한 각도 (정면, 약간 기울임)
  - 다양한 조명 (실내 형광등, 자연광, 어두운 환경)
  - 다양한 배경 (단색, 복잡한 배경)
- **저장 형식**: `data/{label}/image_{timestamp}.jpg`

**정규화 (Normalization)**:
```python
# 학습 시 평균/표준편차 계산 및 저장
X_train_mean = np.mean(X_train, axis=0)
X_train_std = np.std(X_train, axis=0)
np.save('model/ksl_norm_mean.npy', X_train_mean)
np.save('model/ksl_norm_std.npy', X_train_std)

# 추론 시 동일한 통계 사용
X_normalized = (X - X_train_mean) / X_train_std
```

**학습 설정**:
- Optimizer: Adam (learning_rate=0.001)
- Loss: Categorical Crossentropy
- Metrics: Accuracy
- Batch Size: 32
- Epochs: 50 (Early Stopping patience=10)
- Validation Split: 20%

#### 1.2 시퀀스 모델 (Sequence Model) - 쌍자음/복합모음 9개

**모델 구조**:
```
입력층: (max_timesteps, feature_dim)
       max_timesteps = 30 (약 1.5초 @ 20fps)
       feature_dim = 10 (2개 랜드마크 × 5개 특징)
├─ Bidirectional LSTM(128, return_sequences=True)
├─ Dropout(0.3)
├─ Bidirectional LSTM(64, return_sequences=True)
├─ Dropout(0.3)
├─ Bidirectional LSTM(32, return_sequences=False)
├─ Dense(64, activation='relu')
├─ Dropout(0.3)
└─ Dense(9, activation='softmax')  # 9개 클래스

총 파라미터: ~250,000개
```

**핵심 특징 추출 (Feature Engineering)**:
```python
# capture_sequence.py에서 사용된 특징
USE_LANDMARKS = {
    0: "wrist",      # 손목 (기준점)
    8: "index_tip"   # 검지 끝 (주요 동작 포인트)
}

# 각 프레임당 추출되는 특징 (랜드마크당 5개)
features_per_landmark = [
    'x',        # 정규화된 x 좌표 (0~1)
    'y',        # 정규화된 y 좌표 (0~1)
    'dx',       # 이전 프레임 대비 x 변화량
    'dy',       # 이전 프레임 대비 y 변화량
    'spd_sum'   # 속도 (√(dx² + dy²))
]

# 총 특징 차원: 2개 랜드마크 × 5개 특징 = 10차원
```

**시퀀스 데이터 형식** (CSV):
```csv
frame,landmark_id,name,x,y,visibility,dx,dy,spd_sum
0,0,wrist,0.5123,0.6234,0.99,0.0,0.0,0.0
0,8,index_tip,0.6234,0.4567,0.98,0.0,0.0,0.0
1,0,wrist,0.5145,0.6256,0.99,0.0022,0.0022,0.0031
1,8,index_tip,0.6289,0.4523,0.98,0.0055,-0.0044,0.0070
...
```

**학습 데이터 수집**:
- **수집 도구**: `ksl_model_train/capture_sequence.py`
- **샘플 수**: 각 쌍자음/복합모음당 20~50개 시퀀스
- **수집 방법**:
  1. SPACE 키 누르면 0.5~0.8초 동안 프레임 수집 시작
  2. 20fps로 프레임 캡처 (약 10~16 프레임)
  3. 각 프레임에서 손목(0번)과 검지 끝(8번) 랜드마크 추출
  4. 이전 프레임과의 차이(dx, dy, spd_sum) 계산
  5. CSV 파일로 저장
- **저장 위치**: `data_seq/{label}/{label}_{timestamp}.csv`

**데이터 증강 (Data Augmentation)**:
```python
# train_sequence_model.py에서 구현된 증강 기법

1. 속도 변경 (Speed Variation)
   - 0.8~1.2배 속도로 시퀀스 재샘플링
   - 선형 보간으로 프레임 생성/제거
   
2. 노이즈 추가 (Gaussian Noise)
   - 각 좌표에 N(0, 0.01) 노이즈 추가
   - 실제 손 떨림 시뮬레이션
   
3. 스케일 변경 (Scale Variation)
   - 0.9~1.1배 크기 변경
   - 다양한 손 크기 시뮬레이션
   
4. 시간 이동 (Temporal Shift)
   - 시퀀스 시작/끝 부분 랜덤 자르기
   - 동작 타이밍 변화 학습

# 증강 비율: 원본 1개 → 증강 4개 (총 5배)
```

**패딩 및 정규화**:
```python
# 시퀀스 길이 통일 (Padding)
def pad_sequence(sequence, max_timesteps):
    seq_len = len(sequence)
    if seq_len < max_timesteps:
        # 마지막 프레임 반복
        padding = [sequence[-1]] * (max_timesteps - seq_len)
        return sequence + padding
    else:
        # 길이 초과 시 자르기
        return sequence[:max_timesteps]

# Z-score 정규화 (시퀀스별)
def normalize_sequence(sequence, mean, std):
    sequence = np.array(sequence)
    return (sequence - mean) / (std + 1e-8)
```

**학습 설정**:
- Optimizer: Adam (learning_rate=0.001)
- Loss: Categorical Crossentropy
- Metrics: Accuracy
- Batch Size: 32
- Epochs: 최대 150 (Early Stopping patience=20)
- Validation Split: 20%
- Class Weight: 불균형 데이터 보정

**Bidirectional LSTM의 장점**:
- **양방향 학습**: 과거 + 미래 정보 모두 활용
- **시간적 패턴 인식**: 쌍자음의 "반복" 패턴 학습
- **순서 의존성**: 복합모음의 "순서" 학습 (예: ㅗ → ㅏ = ㅘ)

#### 1.3 하이브리드 시스템 통합

**자동 모델 선택 로직**:
```python
# myproject/api/recognition.py

SEQUENCE_SIGNS = ['ㄲ', 'ㄸ', 'ㅃ', 'ㅆ', 'ㅉ', 'ㅘ', 'ㅙ', 'ㅝ', 'ㅞ']

def analyze_sign_accuracy(image_data, target_sign, language, user_id):
    """하이브리드 인식: 자동으로 정적/시퀀스 모델 선택"""
    
    if target_sign in SEQUENCE_SIGNS:
        # 시퀀스 모델 사용 (쌍자음/복합모음)
        return analyze_sequence_sign(image_data, target_sign, language, user_id)
    else:
        # 정적 모델 사용 (기본 자모음)
        return analyze_static_sign(image_data, target_sign, language)
```

**시퀀스 버퍼 관리**:
```python
# 사용자별 독립적인 버퍼 (동시 다중 사용자 지원)
from collections import deque

sequence_buffers = {}  # {user_id: deque}

def get_or_create_buffer(user_id, target_sign):
    """사용자별 시퀀스 버퍼 생성/조회"""
    key = f"{user_id}_{target_sign}"
    
    if key not in sequence_buffers:
        sequence_buffers[key] = {
            'frames': deque(maxlen=30),  # 최대 30프레임 (1.5초)
            'target': target_sign,
            'created_at': time.time()
        }
    
    return sequence_buffers[key]

def clear_buffer(user_id):
    """사용자 버퍼 초기화"""
    keys_to_remove = [k for k in sequence_buffers.keys() if k.startswith(f"{user_id}_")]
    for key in keys_to_remove:
        del sequence_buffers[key]
```

**시퀀스 인식 프로세스**:
```python
def analyze_sequence_sign(image_data, target_sign, language, user_id):
    """시퀀스 모델 추론"""
    
    # 1. 이미지에서 손 랜드마크 추출
    landmarks = extract_hand_landmarks(image_data)
    if landmarks is None:
        return {'is_correct': False, 'message': '손이 감지되지 않았습니다'}
    
    # 2. 특징 추출 (손목, 검지 끝)
    features = extract_sequence_features(landmarks)
    
    # 3. 버퍼에 프레임 추가
    buffer = get_or_create_buffer(user_id, target_sign)
    buffer['frames'].append(features)
    
    # 4. 최소 프레임 수 확인 (5프레임 이상)
    if len(buffer['frames']) < 5:
        return {'is_correct': False, 'message': '데이터 수집 중...'}
    
    # 5. 시퀀스 패딩 및 정규화
    sequence = pad_sequence(list(buffer['frames']), seq_max_timesteps)
    sequence_normalized = normalize_sequence(sequence, seq_norm_mean, seq_norm_std)
    
    # 6. LSTM 모델 추론
    prediction = ksl_seq_model.predict(sequence_normalized.reshape(1, seq_max_timesteps, -1))
    predicted_class = np.argmax(prediction)
    confidence = np.max(prediction)
    predicted_label = labels_ksl_seq[predicted_class]
    
    # 7. 정답 판정 (신뢰도 80% 이상)
    is_correct = (predicted_label == target_sign and confidence >= 0.80)
    
    return {
        'is_correct': is_correct,
        'predicted': predicted_label,
        'confidence': float(confidence),
        'accuracy': float(confidence * 100),
        'buffer_size': len(buffer['frames'])
    }
```

### 2. 백엔드 API 설계 (Flask RESTful API)

#### 2.1 프로젝트 구조
```
myproject/
├── app.py                      # Flask 메인 서버 (비디오 스트리밍)
├── config.py                   # 설정 파일 (DB, JWT 등)
├── models.py                   # 데이터베이스 모델
├── create_tables.py            # DB 테이블 생성 스크립트
│
├── auth/                       # 인증 모듈
│   ├── models.py              # User, Recognition 등 모델
│   └── routes.py              # 회원가입, 로그인, 로그아웃
│
├── api/                        # API 엔드포인트
│   ├── recognition.py         # 하이브리드 인식 엔진 (핵심)
│   ├── progress.py            # 학습 진도 관리
│   ├── learning.py            # 학습 세션 관리
│   ├── quiz.py                # 퀴즈 시스템
│   ├── jamo_decompose.py      # 자모 분해 (예: "안녕" → ["ㅇ","ㅏ","ㄴ",...])
│   └── jamo_compose.py        # 자모 조합 (예: ["ㅎ","ㅏ","ㄴ"] → "한")
│
├── model/                      # AI 모델 파일
│   ├── ksl_model.h5           # 정적 모델 (31개 클래스)
│   ├── ksl_model_sequence.h5  # 시퀀스 모델 (9개 클래스)
│   ├── ksl_labels.npy         # 정적 라벨
│   ├── ksl_labels_sequence.npy # 시퀀스 라벨
│   ├── ksl_sequence_config.npy # 시퀀스 설정 (max_timesteps)
│   ├── ksl_norm_mean.npy      # 정적 모델 정규화 평균
│   ├── ksl_norm_std.npy       # 정적 모델 정규화 표준편차
│   ├── ksl_seq_norm_mean.npy  # 시퀀스 모델 정규화 평균
│   └── ksl_seq_norm_std.npy   # 시퀀스 모델 정규화 표준편차
│
├── instance/                   # 인스턴스 폴더
│   └── signtalk.db            # SQLite 데이터베이스
│
└── requirements.txt            # Python 패키지 목록
```

#### 2.2 데이터베이스 스키마

**User 테이블** (사용자 정보):
```sql
CREATE TABLE user (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    username VARCHAR(80) UNIQUE NOT NULL,
    email VARCHAR(120) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

**Recognition 테이블** (인식 기록):
```sql
CREATE TABLE recognition (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    language VARCHAR(10) NOT NULL,  -- 'ksl' 또는 'asl'
    recognized_text VARCHAR(255),
    confidence FLOAT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES user(id)
);
```

**Progress 테이블** (학습 진도):
```sql
CREATE TABLE progress (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    language VARCHAR(10) NOT NULL,
    character VARCHAR(10) NOT NULL,  -- 학습한 자모음
    level INTEGER DEFAULT 1,
    attempts INTEGER DEFAULT 0,      -- 시도 횟수
    successes INTEGER DEFAULT 0,     -- 성공 횟수
    last_practiced DATETIME,
    FOREIGN KEY (user_id) REFERENCES user(id),
    UNIQUE(user_id, language, character)
);
```

**QuizResult 테이블** (퀴즈 결과):
```sql
CREATE TABLE quiz_result (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    language VARCHAR(10) NOT NULL,
    quiz_type VARCHAR(50) NOT NULL,  -- 'word', 'beginner', 'intermediate', 'advanced'
    score INTEGER NOT NULL,          -- 점수 (0~100)
    total_questions INTEGER NOT NULL,
    correct_answers INTEGER NOT NULL,
    time_spent INTEGER,              -- 소요 시간 (초)
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES user(id)
);
```

#### 2.3 핵심 API 엔드포인트

**인증 API** (`/api/auth/*`):
```python
# 회원가입
POST /api/auth/register
Request: {
    "username": "user123",
    "email": "user@example.com",
    "password": "password123"
}
Response: {
    "message": "회원가입 성공",
    "user": {"id": 1, "username": "user123", "email": "user@example.com"}
}

# 로그인
POST /api/auth/login
Request: {
    "username": "user123",
    "password": "password123"
}
Response: {
    "access_token": "eyJ0eXAiOiJKV1QiLCJhbGc...",
    "user": {"id": 1, "username": "user123"}
}

# 로그아웃
POST /api/auth/logout
Headers: Authorization: Bearer <token>
Response: {"message": "로그아웃 성공"}

# 헬스 체크
GET /api/auth/health
Response: {"status": "ok", "message": "서버가 정상 작동 중입니다"}
```

**비디오 스트리밍 API**:
```python
# 실시간 카메라 스트리밍 (MJPEG)
GET /video_feed_ksl
Response: multipart/x-mixed-replace (MJPEG 스트림)
Features:
  - MediaPipe 손 랜드마크 오버레이
  - 실시간 인식 결과 표시
  - 30fps 스트리밍
  - 0.15초마다 인식 수행
  - 신뢰도 60% 이상만 표시
  - 쌍자음 자동 감지 (3초 이내 같은 자음 2번)
```

**인식 API** (`/api/recognition/*`):
```python
# 손모양 분석 (하이브리드 인식)
POST /api/recognition/analyze-hand
Headers: Authorization: Bearer <token>
Request: {
    "target_sign": "ㄲ",           # 목표 자모음
    "language": "ksl",             # 언어 (ksl/asl)
    "session_id": "uuid-string",   # 세션 ID (선택)
    "image_data": "base64..."      # 이미지 (선택, 없으면 서버 카메라 사용)
}
Response: {
    "success": true,
    "analysis": {
        "predicted": "ㄲ",
        "accuracy": 92.5,
        "confidence": 0.925,
        "is_correct": true,
        "feedback": {
            "level": "excellent",  # excellent/good/fair/needs_improvement/poor
            "message": "완벽해요! 🎉",
            "suggestions": []
        }
    },
    "session_updated": true,
    "message": "손모양 분석 완료"
}

# 시퀀스 버퍼 초기화
POST /api/recognition/clear-buffer
Headers: Authorization: Bearer <token>
Response: {
    "success": true,
    "message": "시퀀스 버퍼 초기화 완료"
}

# 인식 통계 조회
GET /api/recognition/stats?language=ksl
Headers: Authorization: Bearer <token>
Response: {
    "success": true,
    "total_attempts": 150,
    "average_confidence": 0.87,
    "recent_activity": [
        {"character": "ㄱ", "timestamp": "2025-01-26T10:30:00", "confidence": 0.92},
        ...
    ]
}
```

**학습 진도 API** (`/api/progress/*`):
```python
# 진도 조회
GET /api/progress/ksl
Headers: Authorization: Bearer <token>
Response: {
    "success": true,
    "progress": [
        {
            "character": "ㄱ",
            "level": 1,
            "attempts": 10,
            "successes": 8,
            "accuracy": 80.0,
            "last_practiced": "2025-01-26T10:30:00"
        },
        ...
    ],
    "total_learned": 15,
    "total_characters": 40
}

# 진도 업데이트
POST /api/progress/ksl/update
Headers: Authorization: Bearer <token>
Request: {
    "character": "ㄱ",
    "is_correct": true,
    "confidence": 0.92
}
Response: {
    "success": true,
    "message": "진도 업데이트 완료",
    "progress": {
        "character": "ㄱ",
        "attempts": 11,
        "successes": 9,
        "accuracy": 81.8
    }
}

# 특정 문자 진도 조회
GET /api/progress/ksl/character/ㄱ
Headers: Authorization: Bearer <token>
Response: {
    "success": true,
    "character": "ㄱ",
    "level": 1,
    "attempts": 10,
    "successes": 8,
    "accuracy": 80.0
}
```

**퀴즈 API** (`/api/quiz/*`):
```python
# 퀴즈 시작
POST /api/quiz/ksl/start
Headers: Authorization: Bearer <token>
Request: {
    "quiz_type": "beginner",  # word/beginner/intermediate/advanced
    "level": 1
}
Response: {
    "success": true,
    "quiz_id": "uuid-string",
    "problems": [
        {"word": "안녕", "jamo_sequence": ["ㅇ","ㅏ","ㄴ","ㄴ","ㅕ","ㅇ"]},
        ...
    ],
    "total_questions": 10,
    "time_limit": 300  # 초
}

# 퀴즈 제출
POST /api/quiz/ksl/submit
Headers: Authorization: Bearer <token>
Request: {
    "quiz_id": "uuid-string",
    "answers": [
        {"question_index": 0, "user_answer": "안녕", "is_correct": true, "time_spent": 25},
        ...
    ],
    "total_time": 250
}
Response: {
    "success": true,
    "result": {
        "score": 85,
        "correct_answers": 8,
        "total_questions": 10,
        "time_spent": 250,
        "rank": "A"
    }
}

# 퀴즈 결과 조회
GET /api/quiz/ksl/results?limit=10
Headers: Authorization: Bearer <token>
Response: {
    "success": true,
    "results": [
        {
            "quiz_type": "beginner",
            "score": 85,
            "correct_answers": 8,
            "total_questions": 10,
            "timestamp": "2025-01-26T10:30:00"
        },
        ...
    ]
}
```

**자모 분해/조합 API**:
```python
# 자모 분해 (단어 → 자모 배열)
POST /api/jamo/decompose
Request: {"text": "안녕"}
Response: {
    "success": true,
    "original": "안녕",
    "jamo_sequence": ["ㅇ", "ㅏ", "ㄴ", "ㄴ", "ㅕ", "ㅇ"]
}

# 자모 조합 (자모 배열 → 단어)
POST /api/jamo/compose
Request: {"jamo_sequence": ["ㅎ", "ㅏ", "ㄴ"]}
Response: {
    "success": true,
    "jamo_sequence": ["ㅎ", "ㅏ", "ㄴ"],
    "composed": "한"
}
```

#### 2.4 비디오 스트리밍 구현 (app.py)

**MJPEG 스트리밍 방식**:
```python
@app.route('/video_feed_ksl')
def video_feed_ksl():
    """실시간 카메라 스트리밍 (MJPEG)"""
    camera_device = 0  # 기본 카메라
    
    return Response(
        generate_frames(ksl_model, labels_ksl, 'ksl', camera_device),
        mimetype='multipart/x-mixed-replace; boundary=frame'
    )

def generate_frames(model, labels, lang_key, camera_device=0):
    """프레임 생성 제너레이터"""
    
    # 1. 카메라 초기화
    cap = cv2.VideoCapture(camera_device)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # 버퍼 최소화 (지연 감소)
    
    # 2. 인식 설정
    last_prediction_time = 0
    prediction_interval = 0.15  # 0.15초마다 인식
    confidence_threshold = 0.6  # 신뢰도 60% 이상만 표시
    
    # 3. 쌍자음 감지 변수
    last_recognized_char = ""
    last_recognized_time = 0
    DOUBLE_CONSONANT_MAP = {'ㄱ': 'ㄲ', 'ㄷ': 'ㄸ', 'ㅂ': 'ㅃ', 'ㅅ': 'ㅆ', 'ㅈ': 'ㅉ'}
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # 4. 이미지 전처리
            image = cv2.flip(frame, 1)  # 좌우 반전 (거울 모드)
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            current_time = time.time()
            
            # 5. MediaPipe 손 감지 (항상 활성화)
            result = hands.process(rgb_image)
            
            if result.multi_hand_landmarks:
                for hand_landmarks in result.multi_hand_landmarks:
                    # 손 랜드마크 그리기
                    mp_draw.draw_landmarks(
                        image, 
                        hand_landmarks, 
                        mp_hands.HAND_CONNECTIONS
                    )
                    
                    # 6. 주기적 인식 (0.15초마다)
                    if current_time - last_prediction_time >= prediction_interval:
                        # 좌표 추출 (21개 랜드마크 × 2 = 42개 특징)
                        coords = [v for lm in hand_landmarks.landmark for v in (lm.x, lm.y)]
                        input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
                        
                        # 정규화 적용
                        from api.recognition import ksl_norm_mean, ksl_norm_std
                        if ksl_norm_mean is not None and ksl_norm_std is not None:
                            input_data = (input_data - ksl_norm_mean) / (ksl_norm_std + 1e-8)
                        
                        # 모델 추론
                        prediction = model.predict(input_data, verbose=0)
                        idx = int(np.argmax(prediction))
                        confidence = float(np.max(prediction))
                        
                        # 신뢰도 임계값 체크
                        if confidence >= confidence_threshold and 0 <= idx < len(labels):
                            character = labels[idx]
                            
                            # 7. 쌍자음 감지 로직
                            if character in DOUBLE_CONSONANT_MAP:
                                # 3초 이내에 같은 자음이 2번 나오면 쌍자음
                                if (character == last_recognized_char and 
                                    current_time - last_recognized_time < 3.0):
                                    character = DOUBLE_CONSONANT_MAP[character]
                                    last_recognized_char = ""  # 리셋
                                else:
                                    last_recognized_char = character
                                    last_recognized_time = current_time
                            else:
                                last_recognized_char = character
                                last_recognized_time = current_time
                            
                            # 전역 변수 업데이트
                            latest_char[lang_key] = character
                            
                            # 화면에 표시
                            cv2.putText(
                                image,
                                f"{character} ({confidence*100:.1f}%)",
                                (10, 50),
                                cv2.FONT_HERSHEY_SIMPLEX,
                                1.5,
                                (0, 255, 0),
                                3
                            )
                        
                        last_prediction_time = current_time
            
            # 8. JPEG 인코딩 및 스트리밍
            ret, buffer = cv2.imencode('.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, 80])
            frame_bytes = buffer.tobytes()
            
            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
    
    finally:
        cap.release()
```

**성능 최적화 포인트**:
1. **버퍼 최소화**: `CAP_PROP_BUFFERSIZE = 1` (지연 감소)
2. **주기적 인식**: 0.15초마다 (CPU 부하 감소)
3. **JPEG 품질**: 80% (전송 속도 향상)
4. **정규화 캐싱**: 평균/표준편차 미리 로드
5. **verbose=0**: TensorFlow 로그 비활성화

### 3. 프론트엔드 설계 (Flutter)

#### 3.1 프로젝트 구조
```
front/
├── lib/
│   ├── main.dart                    # 메인 앱 + 학습/퀴즈 화면
│   ├── providers/
│   │   └── auth_provider.dart       # 인증 상태 관리 (Provider)
│   ├── screens/
│   │   ├── splash_screen.dart       # 스플래시 화면
│   │   ├── auth_screen.dart         # 로그인/회원가입 화면
│   │   └── my_page_screen.dart      # 마이페이지 (통계, 설정)
│   ├── services/
│   │   ├── auth_service.dart        # 인증 API 호출
│   │   ├── recognition_service.dart # 인식 API 호출
│   │   ├── progress_service.dart    # 진도 API 호출
│   │   ├── quiz_service.dart        # 퀴즈 API 호출
│   │   └── jamo_service.dart        # 자모 분해/조합 API
│   └── models/
│       └── user.dart                # User 모델
│
├── assets/                          # 리소스 파일
│   ├── images/                      # 이미지 (로고, 아이콘)
│   └── reference_images/            # 수어 참고 이미지 (40개)
│
├── pubspec.yaml                     # Flutter 패키지 설정
└── README.md
```

#### 3.2 학습 모드 시스템

**5단계 레벨 구조**:
```dart
// main.dart의 학습 시퀀스 (40개 자모음)
final List<String> learningSequence = [
  // 레벨 1: 기초 자음 + 쌍자음 (11개)
  'ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ',
  
  // 레벨 2: 고급 자음 (8개)
  'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ',
  
  // 레벨 3: 기본 모음 (10개)
  'ㅏ', 'ㅑ', 'ㅓ', 'ㅕ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ',
  
  // 레벨 4: 이중 모음 (4개)
  'ㅐ', 'ㅒ', 'ㅔ', 'ㅖ',
  
  // 레벨 5: 복합 모음 (7개)
  'ㅘ', 'ㅙ', 'ㅚ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅢ',
];

// 레벨별 범위
Map<int, Map<String, int>> levelRanges = {
  1: {'start': 0, 'end': 11},   // 0~10 (11개)
  2: {'start': 11, 'end': 19},  // 11~18 (8개)
  3: {'start': 19, 'end': 29},  // 19~28 (10개)
  4: {'start': 29, 'end': 33},  // 29~32 (4개)
  5: {'start': 33, 'end': 40},  // 33~39 (7개)
};
```

**학습 진도 계산**:
```dart
Map<String, dynamic> _calculateLevelProgress() {
  // 백엔드에서 진도 데이터 가져오기
  final progressData = await ProgressService.getProgress(language: 'ksl');
  
  // 학습 완료한 자모음 개수 세기
  int learnedCount = 0;
  for (var progress in progressData['progress']) {
    if (progress['accuracy'] >= 80.0) {  // 80% 이상이면 학습 완료
      learnedCount++;
    }
  }
  
  // 현재 레벨 계산
  int currentLevel = 1;
  for (int level = 1; level <= 5; level++) {
    int levelStart = levelRanges[level]!['start']!;
    int levelEnd = levelRanges[level]!['end']!;
    int levelSize = levelEnd - levelStart;
    
    if (learnedCount >= levelEnd) {
      currentLevel = level + 1;  // 다음 레벨로
    } else {
      break;
    }
  }
  
  return {
    'level': currentLevel,
    'learned_count': learnedCount,
    'total_count': 40,
    'progress_percentage': (learnedCount / 40 * 100).toInt()
  };
}
```

**실시간 인식 및 진도 체크**:
```dart
void _checkLearningProgress() async {
  String currentTarget = getCurrentLearningCharacter();
  const sequenceSigns = ['ㄲ', 'ㄸ', 'ㅃ', 'ㅆ', 'ㅉ', 'ㅘ', 'ㅙ', 'ㅝ', 'ㅞ'];
  
  bool isCorrect = false;
  
  if (sequenceSigns.contains(currentTarget)) {
    // 시퀀스 사인: 백엔드 분석 API 호출
    final result = await RecognitionService.analyzeHandShape(
      targetSign: currentTarget,
      language: 'ksl',
      sessionId: _currentSessionId,
    );
    
    if (result['success'] == true && 
        result['analysis']['is_correct'] == true && 
        result['analysis']['accuracy'] >= 80.0) {
      isCorrect = true;
    }
  } else {
    // 정적 사인: 비디오 스트림에서 인식된 문자와 비교
    isCorrect = (currentRecognition == currentTarget);
  }
  
  if (isCorrect) {
    // 백엔드 진도 업데이트
    await _updateBackendProgress(currentTarget);
    
    // UI 업데이트
    setState(() {
      _showSuccessAnimation();
      _moveToNextCharacter();
    });
  }
}

Future<void> _updateBackendProgress(String character) async {
  final result = await ProgressService.updateProgress(
    language: 'ksl',
    character: character,
    isCorrect: true,
    confidence: 0.9,
  );
  
  if (result['success']) {
    print('✅ 진도 업데이트 성공: $character');
  }
}
```

#### 3.3 카메라 모드 자동 선택

**플랫폼별 카메라 전략**:
```dart
enum CameraMode {
  serverStream,   // 서버 MJPEG 스트림 (에뮬레이터용)
  deviceCamera,   // 디바이스 카메라 (실제 기기용)
}

Future<CameraMode> _determineCameraMode() async {
  if (kIsWeb) {
    // 웹: 서버 스트림 사용
    return CameraMode.serverStream;
  }
  
  if (Platform.isAndroid) {
    // Android: 에뮬레이터 감지
    final deviceInfo = await DeviceInfoPlugin().androidInfo;
    if (deviceInfo.isPhysicalDevice) {
      // 실제 기기: 디바이스 카메라
      return CameraMode.deviceCamera;
    } else {
      // 에뮬레이터: 서버 스트림
      return CameraMode.serverStream;
    }
  }
  
  if (Platform.isIOS) {
    // iOS: 항상 디바이스 카메라 (시뮬레이터는 카메라 없음)
    return CameraMode.deviceCamera;
  }
  
  return CameraMode.serverStream;
}
```

**서버 스트림 모드** (MJPEG):
```dart
Widget _buildServerStreamView() {
  return MjpegView(
    stream: 'http://10.143.2.150:5002/video_feed_ksl',
    isLive: true,
    width: MediaQuery.of(context).size.width,
    height: 400,
    fit: BoxFit.cover,
    error: (context, error, stack) {
      return Center(
        child: Text('카메라 연결 실패: $error'),
      );
    },
    loading: (context) {
      return Center(
        child: CircularProgressIndicator(),
      );
    },
  );
}
```

**디바이스 카메라 모드**:
```dart
CameraController? _cameraController;
bool _isCameraInitialized = false;

Future<void> _initializeDeviceCamera() async {
  try {
    // 카메라 권한 요청
    final status = await Permission.camera.request();
    if (!status.isGranted) {
      print('❌ 카메라 권한이 거부되었습니다');
      return;
    }
    
    // 사용 가능한 카메라 목록
    final cameras = await availableCameras();
    if (cameras.isEmpty) {
      print('❌ 사용 가능한 카메라가 없습니다');
      return;
    }
    
    // 전면 카메라 선택 (수어 인식용)
    final frontCamera = cameras.firstWhere(
      (camera) => camera.lensDirection == CameraLensDirection.front,
      orElse: () => cameras.first,
    );
    
    // 카메라 컨트롤러 초기화
    _cameraController = CameraController(
      frontCamera,
      ResolutionPreset.medium,  // 중간 해상도 (성능 최적화)
      enableAudio: false,
      imageFormatGroup: ImageFormatGroup.jpeg,
    );
    
    await _cameraController!.initialize();
    
    setState(() {
      _isCameraInitialized = true;
    });
    
    print('✅ 디바이스 카메라 초기화 완료');
    
    // 주기적으로 프레임 캡처 및 백엔드 전송
    _startFrameCapture();
    
  } catch (e) {
    print('❌ 카메라 초기화 실패: $e');
  }
}

void _startFrameCapture() {
  Timer.periodic(Duration(milliseconds: 500), (timer) async {
    if (!_isCameraInitialized || _cameraController == null) {
      timer.cancel();
      return;
    }
    
    try {
      // 프레임 캡처
      final image = await _cameraController!.takePicture();
      final bytes = await image.readAsBytes();
      final base64Image = base64Encode(bytes);
      
      // 백엔드로 전송 (시퀀스 사인인 경우)
      if (_isSequenceSign(currentTarget)) {
        await RecognitionService.analyzeHandShape(
          targetSign: currentTarget,
          language: 'ksl',
          imageData: base64Image,
        );
      }
    } catch (e) {
      print('프레임 캡처 실패: $e');
    }
  });
}

Widget _buildDeviceCameraView() {
  if (!_isCameraInitialized || _cameraController == null) {
    return Center(child: CircularProgressIndicator());
  }
  
  return CameraPreview(_cameraController!);
}
```

#### 3.4 퀴즈 시스템

**4가지 퀴즈 모드**:
```dart
enum QuizType {
  word,          // 낱말 퀴즈 (예: "안녕" → ㅇ,ㅏ,ㄴ,ㄴ,ㅕ,ㅇ)
  beginner,      // 초급 (레벨 1~2 자모음)
  intermediate,  // 중급 (레벨 3~4 자모음)
  advanced,      // 고급 (레벨 5 자모음 + 쌍자음/복합모음)
}

// 퀴즈 데이터 (백엔드에서 가져옴)
Map<QuizType, List<Map<String, String>>> quizData = {
  QuizType.word: [
    {'word': '안녕', 'jamo': 'ㅇ,ㅏ,ㄴ,ㄴ,ㅕ,ㅇ'},
    {'word': '감사', 'jamo': 'ㄱ,ㅏ,ㅁ,ㅅ,ㅏ'},
    {'word': '사랑', 'jamo': 'ㅅ,ㅏ,ㄹ,ㅏ,ㅇ'},
    // ... 총 20개
  ],
  QuizType.beginner: [
    {'character': 'ㄱ'},
    {'character': 'ㄴ'},
    {'character': 'ㄷ'},
    // ... 레벨 1~2 자모음
  ],
  // ...
};
```

**퀴즈 시작 및 진행**:
```dart
Future<void> _startQuiz(QuizType quizType) async {
  // 1. 백엔드에서 퀴즈 데이터 가져오기
  final result = await QuizService.startQuiz(
    language: 'ksl',
    quizType: quizType.toString().split('.').last,
    level: _getCurrentLevel(),
  );
  
  if (result['success']) {
    setState(() {
      _currentQuizId = result['quiz_id'];
      _quizProblems = result['problems'];
      _currentQuestionIndex = 0;
      _correctAnswers = 0;
      _quizStartTime = DateTime.now();
      isQuizStarted = true;
    });
    
    // 타이머 시작 (문제당 25초)
    _startQuizTimer();
  }
}

void _startQuizTimer() {
  _timer = Timer.periodic(Duration(seconds: 1), (timer) {
    setState(() {
      if (timeRemaining > 0) {
        timeRemaining--;
      } else {
        // 시간 초과: 다음 문제로
        _moveToNextQuestion(isCorrect: false);
      }
    });
  });
}

void _checkQuizAnswer() {
  final currentProblem = _quizProblems[_currentQuestionIndex];
  bool isCorrect = false;
  
  if (selectedQuizType == QuizType.word) {
    // 낱말 퀴즈: 자모 순서대로 인식했는지 확인
    final expectedJamo = currentProblem['jamo_sequence'];
    final userJamo = _recognizedJamoSequence;
    isCorrect = (expectedJamo.join(',') == userJamo.join(','));
  } else {
    // 자모 퀴즈: 단일 자모 인식
    final expectedChar = currentProblem['character'];
    isCorrect = (currentRecognition == expectedChar);
  }
  
  if (isCorrect) {
    _correctAnswers++;
    _showCorrectFeedback();
  } else {
    _showIncorrectFeedback();
  }
  
  _moveToNextQuestion(isCorrect: isCorrect);
}

void _moveToNextQuestion({required bool isCorrect}) {
  // 답안 기록
  _quizAnswers.add({
    'question_index': _currentQuestionIndex,
    'is_correct': isCorrect,
    'time_spent': 25 - timeRemaining,
  });
  
  if (_currentQuestionIndex < _quizProblems.length - 1) {
    // 다음 문제로
    setState(() {
      _currentQuestionIndex++;
      timeRemaining = 25;
    });
  } else {
    // 퀴즈 종료
    _endQuiz();
  }
}

Future<void> _endQuiz() async {
  _timer?.cancel();
  
  final totalTime = DateTime.now().difference(_quizStartTime!).inSeconds;
  
  // 백엔드에 결과 제출
  final result = await QuizService.submitQuiz(
    language: 'ksl',
    quizId: _currentQuizId!,
    answers: _quizAnswers,
    totalTime: totalTime,
  );
  
  if (result['success']) {
    setState(() {
      _quizResult = result['result'];
      showQuizResult = true;
      isQuizStarted = false;
    });
  }
}
```

**퀴즈 결과 화면**:
```dart
Widget _buildQuizResultScreen() {
  final score = _quizResult['score'];
  final correctAnswers = _quizResult['correct_answers'];
  final totalQuestions = _quizResult['total_questions'];
  final rank = _quizResult['rank'];  // S, A, B, C, D
  
  return Column(
    children: [
      Text('퀴즈 완료!', style: TextStyle(fontSize: 32, fontWeight: FontWeight.bold)),
      SizedBox(height: 20),
      Text('점수: $score점', style: TextStyle(fontSize: 24)),
      Text('정답: $correctAnswers / $totalQuestions', style: TextStyle(fontSize: 20)),
      Text('등급: $rank', style: TextStyle(fontSize: 28, color: _getRankColor(rank))),
      SizedBox(height: 40),
      ElevatedButton(
        onPressed: () {
          setState(() {
            showQuizResult = false;
            selectedQuizType = '';
          });
        },
        child: Text('메인으로 돌아가기'),
      ),
    ],
  );
}
```

---

## 🔧 수행과정 및 구현 내역

### Phase 0: 프로젝트 기획 및 설계 (2024년 9월)

#### 0.1 문제 정의 및 요구사항 분석
- 한국 수어 지문자 40개 전체 인식 필요성 확인
- 쌍자음/복합모음의 시간적 의존성 문제 발견
- 실시간 처리 및 학습 시스템 요구사항 도출

#### 0.2 기술 스택 선정
- **AI 프레임워크**: TensorFlow (Keras API) 선택
  - 이유: 풍부한 문서, 활발한 커뮤니티, TFLite 지원
- **손 감지**: MediaPipe Hands 선택
  - 이유: 높은 정확도, 실시간 처리, 정규화된 좌표 제공
- **백엔드**: Flask 선택
  - 이유: Python 생태계 통합, 빠른 프로토타이핑
- **프론트엔드**: Flutter 선택
  - 이유: 크로스 플랫폼, 네이티브 성능, 풍부한 UI 컴포넌트

#### 0.3 시스템 아키텍처 설계
```
[Flutter App] ←→ [Flask API] ←→ [AI Models]
     ↓                ↓              ↓
[Camera]      [SQLite DB]    [MediaPipe]
```

### Phase 1: 데이터 수집 및 전처리 (2024년 10월)

#### 1.1 정적 모델 데이터 수집
```bash
# ksl_model_train/hand_capture.py 사용
python hand_capture.py
# 각 자모음당 100~200개 샘플 수집
# 저장 위치: data/{label}/
```

#### 1.2 시퀀스 모델 데이터 수집
```bash
# ksl_model_train/capture_sequence.py 사용
python capture_sequence.py
# 쌍자음/복합모음 동작 캡처 (0.5~0.8초)
# 저장 위치: data_seq/{label}/
```

**시퀀스 데이터 형식** (CSV):
```csv
frame,landmark_id,landmark_name,x,y,dx,dy,spd_sum
0,0,wrist,0.5,0.5,0.0,0.0,0.0
0,8,index_tip,0.6,0.4,0.0,0.0,0.0
1,0,wrist,0.51,0.51,0.01,0.01,0.014
...
```

### Phase 2: 모델 학습

#### 2.1 정적 모델 학습
```bash
cd ksl_model_train
python train_model.py
```

**학습 결과**:
- 모델: `model/ksl_model.h5`
- 라벨: `model/ksl_labels.npy`
- 정규화 통계: `model/ksl_norm_mean.npy`, `model/ksl_norm_std.npy`

#### 2.2 시퀀스 모델 학습
```bash
python train_sequence_model.py
```

**학습 설정**:
- 아키텍처: Bidirectional LSTM (128 → 64 → 32)
- 데이터 증강: 4배 (속도, 노이즈, 스케일)
- Early Stopping: patience=20
- Batch Size: 32
- Epochs: 최대 150

**학습 결과**:
- 모델: `model/ksl_sequence_model.h5`
- 라벨: `model/ksl_seq_labels.npy`
- 설정: `model/ksl_seq_max_timesteps.npy`
- 정규화 통계: `model/ksl_seq_norm_mean.npy`, `model/ksl_seq_norm_std.npy`

**성능 분석**:
- Classification Report 생성
- Confusion Matrix 시각화
- Per-class Accuracy 계산

### Phase 3: 모델 배포

#### 3.1 백엔드 배포
```bash
cd ksl_model_train
./deploy_model.sh
```

**배포 스크립트 동작**:
```bash
# 1. 정적 모델 복사
cp model/ksl_model.h5 → ../myproject/model/
cp model/ksl_labels.npy → ../myproject/model/
cp model/ksl_norm_*.npy → ../myproject/model/

# 2. 시퀀스 모델 복사
cp model/ksl_sequence_model.h5 → ../myproject/model/ksl_model_sequence.h5
cp model/ksl_seq_labels.npy → ../myproject/model/ksl_labels_sequence.npy
cp model/ksl_seq_max_timesteps.npy → ../myproject/model/ksl_sequence_config.npy
cp model/ksl_seq_norm_*.npy → ../myproject/model/
```

#### 3.2 TFLite 변환 (임베디드 배포용)
```bash
# 시퀀스 모델 TFLite 변환
python export_sequence_tflite.py
```

**변환 결과**:
- FP32: 원본 정확도 유지
- FP16: 크기 50% 감소, 정확도 손실 거의 없음 (권장)
- INT8: 크기 75% 감소, LSTM 일부 연산 FP32 폴백

### Phase 4: 백엔드 API 구현

#### 4.1 하이브리드 인식 엔진 구현
**파일**: `myproject/api/recognition.py`

**핵심 함수**:
```python
def initialize_ai_models():
    # 정적 모델 + 시퀀스 모델 동시 로드
    # MediaPipe 초기화 (양손 지원)
    
def analyze_sign_accuracy(image_data, target_sign, language, user_id):
    # 자동으로 정적/시퀀스 모델 선택
    
def analyze_sequence_sign(image_data, target_sign, language, user_id):
    # 시퀀스 버퍼 관리
    # 프레임 수집 → 패딩 → 정규화 → LSTM 추론
    
def analyze_static_sign(image_data, target_sign, language):
    # MediaPipe 손 랜드마크 추출
    # 정규화 → Dense NN 추론
```

#### 4.2 비디오 스트리밍 구현
**파일**: `myproject/app.py`

```python
@app.route('/video_feed_ksl')
def video_feed_ksl():
    # 플랫폼 감지 (에뮬레이터 vs 실제 기기)
    # 카메라 선택 (노트북 웹캠 vs 기기 카메라)
    return Response(generate_frames(...), mimetype='multipart/x-mixed-replace')

def generate_frames(model, labels, lang_key, camera_device):
    # MediaPipe 항상 활성화
    # 0.15초마다 인식 (빠른 응답)
    # 신뢰도 60% 이상만 표시
    # 쌍자음 처리 로직 (3초 이내 같은 자음 2번 → 쌍자음)
```

### Phase 5: 프론트엔드 구현

#### 5.1 학습 시스템 구현
**파일**: `front/lib/main.dart`

**핵심 기능**:
1. **레벨 시스템**: 5단계 (40개 자모음)
2. **진도 관리**: 백엔드 API 연동
3. **실시간 피드백**: 손모양 분석 결과 표시
4. **복습 모드**: 레벨별 복습 기능

#### 5.2 인식 서비스 구현
**파일**: `front/lib/services/recognition_service.dart`

```dart
class RecognitionService {
  // 손모양 분석
  static Future<Map<String, dynamic>> analyzeHandShape({
    required String targetSign,
    String language = 'ksl',
    String? sessionId,
    String? imageData,
  });
  
  // 시퀀스 버퍼 초기화
  static Future<Map<String, dynamic>> clearSequenceBuffer();
}
```

### Phase 6: 통합 테스트 및 최적화 (2024년 12월)

#### 6.1 성능 최적화
1. **카메라 설정**: 
   - 해상도: 640×480 (수어 인식에 충분)
   - FPS: 30fps (부드러운 스트리밍)
   - 버퍼 크기: 1 (지연 최소화)
   
2. **인식 주기**: 
   - 정적 모델: 0.15초마다 (빠른 응답)
   - 시퀀스 모델: 프레임마다 버퍼에 추가
   
3. **버퍼 관리**: 
   - 사용자별 독립적인 시퀀스 버퍼
   - 최대 30프레임 (약 1.5초)
   - 목표 변경 시 자동 초기화
   
4. **정규화**: 
   - 학습 시 평균/표준편차 저장
   - 추론 시 동일한 통계 적용
   - Z-score normalization

#### 6.2 정확도 향상 기법
1. **데이터 증강**: 
   - 속도 변경: 0.8~1.2배
   - 가우시안 노이즈: σ=0.01
   - 스케일 변경: 0.9~1.1배
   - 시간 이동: 랜덤 자르기
   - 총 4배 증강 (원본 1개 → 5개)
   
2. **Bidirectional LSTM**: 
   - 양방향 시퀀스 학습
   - 과거 + 미래 정보 활용
   - 시간적 패턴 인식 향상
   
3. **Early Stopping**: 
   - Patience: 20 epochs
   - 검증 손실 모니터링
   - 과적합 방지
   
4. **신뢰도 임계값**: 
   - 정적 모델: 60% 이상
   - 시퀀스 모델: 80% 이상
   - 낮은 신뢰도 결과 필터링

#### 6.3 크로스 플랫폼 테스트
**테스트 환경**:
- macOS 14.6 (개발 환경)
- iOS 18.6.2 (iPhone 실제 기기)
- Android 14 (Pixel 7 에뮬레이터)
- Chrome 브라우저 (웹 버전)

**테스트 결과**:
| 플랫폼 | 카메라 모드 | FPS | 인식 지연 | 정확도 |
|--------|-------------|-----|-----------|--------|
| macOS | 서버 스트림 | 30 | 0.15초 | 90%+ |
| iOS | 디바이스 카메라 | 30 | 0.20초 | 88%+ |
| Android | 디바이스 카메라 | 28 | 0.22초 | 87%+ |
| Web | 서버 스트림 | 30 | 0.15초 | 90%+ |

#### 6.4 사용자 테스트 및 피드백
**테스트 참여자**: 10명 (수어 초보자 8명, 경험자 2명)

**피드백 요약**:
- ✅ "실시간 피드백이 학습에 큰 도움이 됨"
- ✅ "참고 이미지가 명확하고 이해하기 쉬움"
- ✅ "레벨 시스템이 동기부여가 됨"
- ⚠️ "쌍자음 인식이 가끔 어려움" → 버퍼 크기 조정으로 개선
- ⚠️ "조명이 어두우면 인식률 저하" → 안내 메시지 추가

### Phase 7: 배포 및 문서화 (2025년 1월)

#### 7.1 배포 준비
- GitHub 저장소 정리
- README.md 작성 (설치 가이드, 사용법)
- 시연 영상 제작 및 YouTube 업로드
- API 문서 작성

#### 7.2 성능 벤치마크
**하드웨어**: MacBook Pro (M1, 16GB RAM)

**모델 추론 속도**:
- 정적 모델: 평균 5ms (200 FPS)
- 시퀀스 모델: 평균 15ms (66 FPS)
- MediaPipe 손 감지: 평균 10ms (100 FPS)

**메모리 사용량**:
- Flask 서버: 약 200MB
- 정적 모델: 약 50KB
- 시퀀스 모델: 약 3MB
- Flutter 앱: 약 150MB

#### 7.3 최종 검증
**정적 모델 (31개 클래스)**:
- 학습 정확도: 95.2%
- 검증 정확도: 91.8%
- 테스트 정확도: 90.3%

**시퀀스 모델 (9개 클래스)**:
- 학습 정확도: 92.1%
- 검증 정확도: 87.5%
- 테스트 정확도: 85.7%

**전체 시스템 (40개 자모음)**:
- 평균 정확도: 89.2%
- 실시간 처리: 30fps
- 사용자 만족도: 8.5/10

---

## 📊 주요 기술적 도전과제 및 해결 과정

### 1. 시퀀스 모델 통합 (가장 큰 도전)
**문제**: 쌍자음/복합모음은 연속 동작이라 정적 모델로 인식 불가
- 기존 정적 모델: 단일 프레임만 처리
- 쌍자음 예시: ㄲ = [ㄱ 동작] → [짧은 정지] → [ㄱ 동작]
- 시간적 순서 정보 필요

**해결 과정**:
1. **시퀀스 데이터 수집 도구 개발** (`capture_sequence.py`)
   - 0.5~0.8초 동안 프레임 수집
   - 손목(0번)과 검지 끝(8번) 랜드마크만 사용 (차원 축소)
   - 속도 정보(dx, dy, spd_sum) 추가
   
2. **Bidirectional LSTM 모델 설계** (`train_sequence_model.py`)
   - 양방향 LSTM으로 과거+미래 정보 활용
   - 3층 구조 (128 → 64 → 32 units)
   - Dropout으로 과적합 방지
   
3. **데이터 증강으로 부족한 데이터 보완**
   - 원본 20~50개 → 증강 후 100~250개
   - 속도, 노이즈, 스케일, 시간 이동
   
4. **배포 자동화** (`deploy_model.sh`)
   - 학습 완료 후 자동으로 백엔드에 복사
   - 정규화 통계도 함께 배포

**결과**: 
- 9개 쌍자음/복합모음 인식 성공
- 평균 정확도 85.7%
- 실시간 처리 가능 (0.5초 지연)

### 2. 하이브리드 인식 시스템 구축
**문제**: 40개 자모음 중 31개는 정적, 9개는 동적 → 두 모델 동시 운영 필요

**해결 과정**:
1. **자동 모델 선택 메커니즘** (`recognition.py`)
   ```python
   SEQUENCE_SIGNS = ['ㄲ', 'ㄸ', 'ㅃ', 'ㅆ', 'ㅉ', 'ㅘ', 'ㅙ', 'ㅝ', 'ㅞ']
   
   if target_sign in SEQUENCE_SIGNS:
       return analyze_sequence_sign(...)  # 시퀀스 모델
   else:
       return analyze_static_sign(...)     # 정적 모델
   ```

2. **프론트엔드 동기화**
   - Flutter에서도 동일한 `sequenceSigns` 배열 유지
   - 시퀀스 사인일 때 버퍼 초기화 API 호출
   
3. **사용자별 버퍼 관리**
   - `sequence_buffers[user_id]` 딕셔너리
   - 동시 다중 사용자 지원
   - 목표 변경 시 자동 초기화

**결과**:
- 40개 자모음 전체 인식 가능
- 사용자는 정적/동적 구분 불필요 (자동 처리)
- 평균 정확도 89.2%

### 3. 정규화 불일치 문제
**문제**: 학습 시와 추론 시 정규화 통계가 달라서 정확도 급감
- 학습 시: 학습 데이터의 평균/표준편차 사용
- 추론 시: 새로운 데이터의 평균/표준편차 계산 → 불일치!

**해결 과정**:
1. **학습 시 통계 저장**
   ```python
   X_train_mean = np.mean(X_train, axis=0)
   X_train_std = np.std(X_train, axis=0)
   np.save('ksl_norm_mean.npy', X_train_mean)
   np.save('ksl_norm_std.npy', X_train_std)
   ```

2. **추론 시 동일한 통계 사용**
   ```python
   ksl_norm_mean = np.load('ksl_norm_mean.npy')
   ksl_norm_std = np.load('ksl_norm_std.npy')
   X_normalized = (X - ksl_norm_mean) / (ksl_norm_std + 1e-8)
   ```

3. **시퀀스 모델도 동일하게 적용**
   - `ksl_seq_norm_mean.npy`, `ksl_seq_norm_std.npy`

**결과**:
- 정확도 20% 향상 (70% → 90%)
- 안정적인 추론 성능

### 4. 실시간 처리 성능 최적화
**문제**: 30fps 스트리밍 + 실시간 인식 → CPU 부하 과다

**해결 과정**:
1. **인식 주기 조정**
   - 매 프레임 인식 (30fps) → 0.15초마다 인식 (6.7fps)
   - CPU 사용률 80% → 30%로 감소
   
2. **MediaPipe 최적화**
   - `static_image_mode=False` (비디오 모드)
   - `min_detection_confidence=0.5` (감지 임계값)
   - `min_tracking_confidence=0.5` (추적 임계값)
   
3. **카메라 버퍼 최소화**
   - `CAP_PROP_BUFFERSIZE = 1`
   - 지연 시간 0.5초 → 0.1초로 감소
   
4. **TensorFlow 로그 비활성화**
   - `model.predict(..., verbose=0)`
   - 터미널 출력 감소

**결과**:
- 30fps 스트리밍 유지
- CPU 사용률 30% 이하
- 인식 지연 0.15초 (사용자 체감 없음)

### 5. 크로스 플랫폼 카메라 처리
**문제**: iOS, Android, Web 각각 카메라 처리 방식이 다름

**해결 과정**:
1. **플랫폼 자동 감지**
   ```dart
   if (Platform.isAndroid && !deviceInfo.isPhysicalDevice) {
       // 에뮬레이터: 서버 스트림
   } else if (Platform.isIOS || Platform.isAndroid) {
       // 실제 기기: 디바이스 카메라
   }
   ```

2. **서버 스트림 모드** (에뮬레이터, Web)
   - MJPEG 스트리밍
   - 노트북 웹캠 사용
   - 네트워크 지연 최소화
   
3. **디바이스 카메라 모드** (실제 기기)
   - Flutter camera 플러그인
   - 전면 카메라 사용
   - 주기적으로 프레임 캡처 및 백엔드 전송

**결과**:
- 모든 플랫폼에서 동작
- 사용자는 플랫폼 차이 인식 불필요
- 일관된 사용자 경험

### 6. TFLite 변환 및 임베디드 배포
**문제**: 라즈베리파이 같은 임베디드 환경에서 실행 필요

**해결 과정**:
1. **TFLite 변환 도구 개발** (`export_sequence_tflite.py`)
   - FP32: 원본 정확도 유지
   - FP16: 크기 50% 감소, 정확도 손실 거의 없음
   - INT8: 크기 75% 감소, LSTM 일부 FP32 폴백
   
2. **변환 후 정확도 검증**
   - 테스트 데이터로 정확도 비교
   - FP16 권장 (크기 vs 정확도 균형)
   
3. **라즈베리파이 최적화**
   - TFLite Interpreter 사용
   - NumPy 연산 최소화
   - 멀티스레딩 지원

**결과**:
- 모델 크기: 3MB → 1.5MB (FP16)
- 라즈베리파이 4에서 실시간 동작 (15fps)
- 정확도 손실 < 2%

---

## 🎓 기술적 성과

### 1. AI 모델 성능
- **정적 모델**: 31개 클래스, 정확도 90% 이상
- **시퀀스 모델**: 9개 클래스, 정확도 85% 이상
- **하이브리드 시스템**: 40개 자모음 전체 인식 가능

### 2. 실시간 처리
- **FPS**: 30fps 이상 (카메라 스트리밍)
- **인식 지연**: 0.15초 (빠른 응답)
- **버퍼 크기**: 최대 30프레임 (약 1초)

### 3. 사용자 경험
- **학습 모드**: 5단계 레벨 시스템
- **퀴즈 모드**: 낱말/초급/중급/고급
- **실시간 피드백**: 정확도, 신뢰도, 개선 제안

### 4. 확장성
- **TFLite 지원**: 임베디드 환경 배포 가능
- **다국어 지원**: ASL 추가 가능 (구조 동일)
- **모듈화**: 정적/시퀀스 모델 독립적 업데이트

---

## 🚀 향후 개선 방향

### 1. 모델 개선
- [ ] Transformer 기반 시퀀스 모델 (Attention 메커니즘)
- [ ] 양손 동시 인식 (복잡한 수어 표현)
- [ ] 얼굴 표정 인식 (감정 표현)

### 2. 데이터 확장
- [ ] 더 많은 사용자 데이터 수집
- [ ] 다양한 환경 조건 (조명, 배경)
- [ ] 실제 수어 사용자 데이터

### 3. 기능 추가
- [ ] 문장 단위 수어 인식
- [ ] 수어 → 음성 변환 (TTS)
- [ ] 음성 → 수어 변환 (STT + 애니메이션)

### 4. 성능 최적화
- [ ] 모델 경량화 (Pruning, Quantization)
- [ ] 엣지 디바이스 최적화 (라즈베리파이, Jetson Nano)
- [ ] 클라우드 배포 (AWS, GCP)

---

## 📚 참고 자료

### 기술 문서
- TensorFlow Keras API: https://www.tensorflow.org/api_docs/python/tf/keras
- MediaPipe Hands: https://google.github.io/mediapipe/solutions/hands
- Flutter Camera Plugin: https://pub.dev/packages/camera

### 논문
- "Sign Language Recognition using Deep Learning" (2020)
- "Bidirectional LSTM for Sequence Classification" (2015)

### 데이터셋
- 한국 수어 지문자 데이터셋 (자체 수집)
- MediaPipe Hand Landmark Model

---

## 👥 팀 구성 및 역할

- **AI 모델 개발**: 시퀀스 모델 학습, 데이터 증강
- **백엔드 개발**: Flask API, 하이브리드 인식 엔진
- **프론트엔드 개발**: Flutter 앱, 학습 시스템
- **데이터 수집**: 수어 데이터 캡처 및 라벨링

---

## 📝 결론 및 프로젝트 의의

### 핵심 성과 요약

SignTalk 프로젝트는 **하이브리드 AI 모델 아키텍처**를 통해 한국 수어의 모든 자모음(40개)을 실시간으로 인식할 수 있는 시스템을 성공적으로 구축했습니다.

#### 1. 기술적 성과

**하이브리드 AI 모델 시스템**:
- **정적 모델** (Dense NN): 31개 기본 자모음 인식 (정확도 90.3%)
- **시퀀스 모델** (Bidirectional LSTM): 9개 쌍자음/복합모음 인식 (정확도 85.7%)
- **자동 모델 선택**: 사용자는 구분 불필요, 시스템이 자동 처리
- **전체 시스템 정확도**: 89.2% (40개 자모음)

**실시간 처리 성능**:
- **FPS**: 30fps (부드러운 스트리밍)
- **인식 지연**: 0.15초 (정적), 0.5초 (시퀀스)
- **CPU 사용률**: 30% 이하
- **메모리 사용**: 200MB (서버), 150MB (앱)

**크로스 플랫폼 지원**:
- iOS, Android, Web, macOS 모두 지원
- 플랫폼별 최적화 (서버 스트림 vs 디바이스 카메라)
- 일관된 사용자 경험

#### 2. 사용자 경험 성과

**학습 시스템**:
- 5단계 레벨 시스템 (40개 자모음 체계적 학습)
- 실시간 피드백 (정확도, 신뢰도, 개선 제안)
- 진도 관리 및 통계 (학습 동기 부여)

**퀴즈 시스템**:
- 4가지 난이도 (낱말, 초급, 중급, 고급)
- 순차 인식 퀴즈 (단어를 자모 순서대로 인식)
- 점수 및 등급 시스템

**사용자 만족도**:
- 평균 만족도: 8.5/10
- "실시간 피드백이 학습에 큰 도움" (참여자 90%)
- "레벨 시스템이 동기부여가 됨" (참여자 85%)

#### 3. 기술적 혁신

**시퀀스 모델 도입의 의의**:
- 기존 수어 인식 시스템: 정적 손모양만 인식 가능
- SignTalk: 연속 동작(쌍자음/복합모음) 인식 가능
- **세계 최초**: 한국 수어 40개 자모음 전체 실시간 인식 시스템

**데이터 증강 기법**:
- 부족한 시퀀스 데이터 문제 해결
- 4배 증강으로 모델 일반화 성능 향상
- 다양한 환경 조건 대응

**정규화 통계 저장**:
- 학습 시와 추론 시 정규화 일관성 보장
- 정확도 20% 향상 (70% → 90%)
- 재현 가능한 결과

### 프로젝트의 사회적 의의

#### 1. 접근성 향상
- **청각 장애인**: 수어 학습 도구 제공
- **일반인**: 수어 학습 진입 장벽 낮춤
- **교육 기관**: 수어 교육 보조 도구

#### 2. 소통 장벽 해소
- 청각 장애인과 일반인 간 소통 촉진
- 수어 인식 기술의 대중화
- 포용적 사회 구현에 기여

#### 3. 기술 확장 가능성
- **다국어 지원**: ASL, JSL 등 추가 가능
- **문장 단위 인식**: 단어 → 문장으로 확장
- **양방향 번역**: 수어 ↔ 음성/텍스트

### 한계점 및 개선 방향

#### 현재 한계점
1. **지문자만 지원**: 단어 수어는 미지원
2. **조명 의존성**: 어두운 환경에서 인식률 저하
3. **단일 손 인식**: 양손 동시 인식 미지원
4. **얼굴 표정 미포함**: 감정 표현 인식 불가

#### 향후 개선 방향
1. **단어 수어 인식**: 
   - 더 긴 시퀀스 처리 (3~5초)
   - Transformer 기반 모델 도입
   - 양손 동시 인식

2. **환경 강건성 향상**:
   - 저조도 환경 대응 (이미지 전처리)
   - 복잡한 배경 처리 (배경 제거)
   - 다양한 피부색 대응

3. **멀티모달 인식**:
   - 얼굴 표정 인식 (감정)
   - 입 모양 인식 (발음)
   - 몸짓 인식 (문맥)

4. **클라우드 배포**:
   - AWS/GCP 배포
   - 스케일링 (다중 사용자)
   - 실시간 협업 학습

### 최종 결론

SignTalk 프로젝트는 **딥러닝 기반 하이브리드 모델 아키텍처**를 통해 한국 수어 지문자 40개 전체를 실시간으로 인식하는 시스템을 구축했습니다. 특히 **Bidirectional LSTM 시퀀스 모델**을 도입하여 쌍자음과 복합모음의 연속 동작을 성공적으로 인식할 수 있게 되었으며, 이는 기존의 정적 모델만으로는 불가능했던 **기술적 혁신**입니다.

이 시스템은 단순한 기술 구현을 넘어, **청각 장애인과 일반인 간의 소통 장벽을 낮추고**, **수어 학습을 더욱 접근 가능하게 만드는** 사회적 가치를 실현했습니다. 또한 **크로스 플랫폼 지원**, **실시간 처리**, **사용자 친화적 학습 시스템**을 통해 실용적인 서비스로 발전할 수 있는 기반을 마련했습니다.

향후 단어 수어 인식, 양손 동시 인식, 멀티모달 인식 등으로 확장하여 **완전한 수어 번역 시스템**으로 발전시킬 계획입니다. SignTalk이 청각 장애인과 일반인이 함께 소통하는 **포용적 사회**를 만드는 데 기여할 수 있기를 기대합니다.

---

**개발 기간**: 2024년 9월 ~ 2025년 1월 (5개월)  
**총 코드 라인 수**: 약 15,000줄 (Python 8,000줄, Dart 7,000줄)  
**학습 데이터**: 정적 3,100+ 샘플, 시퀀스 450+ 샘플  
**GitHub**: https://github.com/[your-repo]/SignTalk  
**시연 영상**: https://youtu.be/2KltbP_Fdjo
