# SignTalk í”„ë¡œì íŠ¸ - ê³¼ì œ í•´ê²°ë°©ì•ˆ ë° ìˆ˜í–‰ê³¼ì •

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

**í”„ë¡œì íŠ¸ëª…**: SignTalk - AI ê¸°ë°˜ í•œêµ­ ìˆ˜ì–´ ì¸ì‹ ë° í•™ìŠµ í”Œë«í¼  
**ê°œë°œ ê¸°ê°„**: 2024ë…„ 9ì›” ~ 2025ë…„ 1ì›” (5ê°œì›”)  
**ëª©ì **: ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹¤ì‹œê°„ í•œêµ­ ìˆ˜ì–´(KSL) ì§€ë¬¸ì ì¸ì‹ ì‹œìŠ¤í…œ êµ¬ì¶• ë° ëŒ€í™”í˜• í•™ìŠµ í”Œë«í¼ ì œê³µ

### ê¸°ìˆ  ìŠ¤íƒ

#### ë°±ì—”ë“œ (Python)
- **í”„ë ˆì„ì›Œí¬**: Flask 3.1.2 (RESTful API ì„œë²„)
- **AI/ML**: 
  - TensorFlow 2.19.1 (ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ )
  - MediaPipe 0.10.9 (ì† ëœë“œë§ˆí¬ ì¶”ì¶œ)
  - NumPy 1.26.4 (ìˆ˜ì¹˜ ì—°ì‚°)
  - OpenCV 4.10.0 (ì˜ìƒ ì²˜ë¦¬)
- **ë°ì´í„°ë² ì´ìŠ¤**: SQLite + Flask-SQLAlchemy 3.1.1
- **ì¸ì¦**: Flask-JWT-Extended 4.7.1 (JWT í† í° ê¸°ë°˜)

#### í”„ë¡ íŠ¸ì—”ë“œ (Flutter)
- **í”„ë ˆì„ì›Œí¬**: Flutter 3.9.2+ (í¬ë¡œìŠ¤ í”Œë«í¼)
- **ìƒíƒœ ê´€ë¦¬**: Provider íŒ¨í„´
- **ì¹´ë©”ë¼**: camera 0.10.6 (ì‹¤ì‹œê°„ ì˜ìƒ ìº¡ì²˜)
- **HTTP í†µì‹ **: http 1.2.2
- **ë¡œì»¬ ì €ì¥ì†Œ**: shared_preferences 2.3.5

#### AI ëª¨ë¸ ì•„í‚¤í…ì²˜
- **ì •ì  ëª¨ë¸**: Dense Neural Network (31ê°œ ê¸°ë³¸ ìëª¨ìŒ)
- **ì‹œí€€ìŠ¤ ëª¨ë¸**: Bidirectional LSTM (9ê°œ ìŒììŒ/ë³µí•©ëª¨ìŒ)
- **í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ**: ìë™ ëª¨ë¸ ì„ íƒ ë©”ì»¤ë‹ˆì¦˜

#### ê°œë°œ í™˜ê²½
- **OS**: macOS 14.6, Windows 11, Linux (Ubuntu 22.04)
- **Python**: 3.12.x
- **IDE**: VS Code, Android Studio, Xcode
- **ë²„ì „ ê´€ë¦¬**: Git + GitHub

---

## ğŸ¯ í•µì‹¬ ë¬¸ì œ ì •ì˜ ë° ë¶„ì„

### 1. í•œêµ­ ìˆ˜ì–´ ì§€ë¬¸ì ì²´ê³„ì˜ íŠ¹ìˆ˜ì„±

í•œêµ­ ìˆ˜ì–´ ì§€ë¬¸ìëŠ” **40ê°œì˜ ìëª¨ìŒ**(ììŒ 19ê°œ + ëª¨ìŒ 21ê°œ)ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, í‘œí˜„ ë°©ì‹ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ë¥˜ë©ë‹ˆë‹¤:

#### 1.1 ì •ì  í‘œí˜„ (Static Gestures) - 31ê°œ
- **ê¸°ë³¸ ììŒ** (14ê°œ): ã„±, ã„´, ã„·, ã„¹, ã…, ã…‚, ã……, ã…‡, ã…ˆ, ã…Š, ã…‹, ã…Œ, ã…, ã…
- **ê¸°ë³¸ ëª¨ìŒ** (17ê°œ): ã…, ã…‘, ã…“, ã…•, ã…—, ã…›, ã…œ, ã… , ã…¡, ã…£, ã…, ã…’, ã…”, ã…–, ã…š, ã…Ÿ, ã…¢
- **íŠ¹ì§•**: ë‹¨ì¼ í”„ë ˆì„(ì •ì§€ ì˜ìƒ)ìœ¼ë¡œ ì¸ì‹ ê°€ëŠ¥
- **ì¸ì‹ ë°©ë²•**: MediaPipe ì† ëœë“œë§ˆí¬ 21ê°œ ì¢Œí‘œ â†’ Dense NN

#### 1.2 ë™ì  í‘œí˜„ (Dynamic Gestures) - 9ê°œ
- **ìŒììŒ** (5ê°œ): ã„², ã„¸, ã…ƒ, ã…†, ã…‰
  - í‘œí˜„ ë°©ì‹: ê¸°ë³¸ ììŒì„ **ë¹ ë¥´ê²Œ 2ë²ˆ ë°˜ë³µ** (ì˜ˆ: ã„± â†’ ã„± = ã„²)
  - ì‹œê°„: ì•½ 0.5~0.8ì´ˆ
- **ë³µí•©ëª¨ìŒ** (4ê°œ): ã…˜, ã…™, ã…, ã…
  - í‘œí˜„ ë°©ì‹: ë‘ ëª¨ìŒì„ **ì—°ì†ì ìœ¼ë¡œ ê²°í•©** (ì˜ˆ: ã…— + ã… = ã…˜)
  - ì‹œê°„: ì•½ 0.6~1.0ì´ˆ
- **íŠ¹ì§•**: ì‹œê°„ì  ìˆœì„œê°€ ì¤‘ìš”í•œ ì—°ì† ë™ì‘
- **ì¸ì‹ ë°©ë²•**: ì‹œí€€ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ â†’ Bidirectional LSTM

### 2. í•´ê²°í•´ì•¼ í•  í•µì‹¬ ê¸°ìˆ  ê³¼ì œ

#### 2.1 ì‹œê°„ì  ì˜ì¡´ì„± (Temporal Dependency)
**ë¬¸ì œ**: ìŒììŒ/ë³µí•©ëª¨ìŒì€ ë‹¨ì¼ í”„ë ˆì„ì´ ì•„ë‹Œ **ì—°ì†ëœ ë™ì‘ì˜ ì‹œí€€ìŠ¤**ë¡œ í‘œí˜„ë¨
- ì˜ˆì‹œ: ã„² = [ã„± ë™ì‘] â†’ [ì§§ì€ ì •ì§€] â†’ [ã„± ë™ì‘]
- ê¸°ì¡´ ì •ì  ëª¨ë¸ë¡œëŠ” ì¸ì‹ ë¶ˆê°€ëŠ¥
- **í•´ê²° ë°©ì•ˆ**: LSTM ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸ ë„ì…

#### 2.2 ëª¨ë¸ ì´ì¤‘í™” ë° ìë™ ì„ íƒ
**ë¬¸ì œ**: 40ê°œ ìëª¨ìŒ ì¤‘ 31ê°œëŠ” ì •ì , 9ê°œëŠ” ë™ì  â†’ ë‘ ê°€ì§€ ëª¨ë¸ í•„ìš”
- ì •ì  ëª¨ë¸: ë¹ ë¥¸ ì¶”ë¡  (ë‹¨ì¼ í”„ë ˆì„)
- ì‹œí€€ìŠ¤ ëª¨ë¸: ëŠë¦° ì¶”ë¡  (ì—¬ëŸ¬ í”„ë ˆì„ ìˆ˜ì§‘ í•„ìš”)
- **í•´ê²° ë°©ì•ˆ**: í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ êµ¬ì¶• (ìë™ ëª¨ë¸ ì„ íƒ)

#### 2.3 ì‹¤ì‹œê°„ ì²˜ë¦¬ ë° ì‚¬ìš©ì ê²½í—˜
**ë¬¸ì œ**: í•™ìŠµ ëª¨ë“œì—ì„œ ì¦‰ê°ì ì¸ í”¼ë“œë°± ì œê³µ í•„ìš”
- ëª©í‘œ ì§€ì—° ì‹œê°„: 0.2ì´ˆ ì´ë‚´
- ì¹´ë©”ë¼ FPS: 30fps ì´ìƒ
- **í•´ê²° ë°©ì•ˆ**: 
  - ê²½ëŸ‰í™”ëœ ëª¨ë¸ êµ¬ì¡°
  - íš¨ìœ¨ì ì¸ ë²„í¼ ê´€ë¦¬
  - ë¹„ë™ê¸° ì²˜ë¦¬

#### 2.4 ì†ëª¨ì–‘ ìœ ì‚¬ì„± ë¬¸ì œ
**ë¬¸ì œ**: ì¼ë¶€ ìëª¨ìŒì€ ì†ëª¨ì–‘ì´ ë§¤ìš° ìœ ì‚¬í•¨
- ì˜ˆì‹œ: ã„± vs ã„´, ã… vs ã…‘
- ë¯¸ì„¸í•œ ê°ë„ ì°¨ì´ë¡œ êµ¬ë¶„
- **í•´ê²° ë°©ì•ˆ**:
  - ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„° (ê° í´ë˜ìŠ¤ë‹¹ 100+ ìƒ˜í”Œ)
  - ë°ì´í„° ì¦ê°• (íšŒì „, ìŠ¤ì¼€ì¼, ë…¸ì´ì¦ˆ)
  - ì •ê·œí™” (Z-score normalization)

#### 2.5 ë‹¤ì–‘í•œ í™˜ê²½ ì¡°ê±´
**ë¬¸ì œ**: ì¡°ëª…, ë°°ê²½, ì† í¬ê¸°, í”¼ë¶€ìƒ‰ ë“± ë³€ìˆ˜ê°€ ë§ìŒ
- ì‹¤ë‚´/ì‹¤ì™¸ ì¡°ëª… ì°¨ì´
- ë³µì¡í•œ ë°°ê²½
- ë‹¤ì–‘í•œ ì‚¬ìš©ì ì† í¬ê¸°
- **í•´ê²° ë°©ì•ˆ**:
  - MediaPipeì˜ ì •ê·œí™”ëœ ì¢Œí‘œ ì‚¬ìš© (0~1 ë²”ìœ„)
  - ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
  - ì† ëœë“œë§ˆí¬ë§Œ ì‚¬ìš© (ë°°ê²½ ë¬´ê´€)

### 3. í”„ë¡œì íŠ¸ ëª©í‘œ ë° ì„±ê³µ ê¸°ì¤€

#### 3.1 ê¸°ëŠ¥ì  ëª©í‘œ
- âœ… 40ê°œ í•œêµ­ ìˆ˜ì–´ ì§€ë¬¸ì ì „ì²´ ì¸ì‹
- âœ… ì‹¤ì‹œê°„ ì¸ì‹ (ì§€ì—° ì‹œê°„ 0.2ì´ˆ ì´ë‚´)
- âœ… í•™ìŠµ ëª¨ë“œ (5ë‹¨ê³„ ë ˆë²¨ ì‹œìŠ¤í…œ)
- âœ… í€´ì¦ˆ ëª¨ë“œ (4ê°€ì§€ ë‚œì´ë„)
- âœ… ì§„ë„ ê´€ë¦¬ ë° í†µê³„

#### 3.2 ì„±ëŠ¥ ëª©í‘œ
- **ì •ì  ëª¨ë¸ ì •í™•ë„**: 90% ì´ìƒ
- **ì‹œí€€ìŠ¤ ëª¨ë¸ ì •í™•ë„**: 85% ì´ìƒ
- **FPS**: 30fps ì´ìƒ (ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë°)
- **ì¸ì‹ ì§€ì—°**: 0.15ì´ˆ (ì •ì ), 0.5ì´ˆ (ì‹œí€€ìŠ¤)

#### 3.3 ì‚¬ìš©ì ê²½í—˜ ëª©í‘œ
- ì§ê´€ì ì¸ UI/UX
- ì‹¤ì‹œê°„ í”¼ë“œë°± (ì •í™•ë„, ê°œì„  ì œì•ˆ)
- í¬ë¡œìŠ¤ í”Œë«í¼ ì§€ì› (iOS, Android, Web)

---

## ğŸ’¡ í•´ê²°ë°©ì•ˆ ë° ì‹œìŠ¤í…œ ì„¤ê³„

### 1. í•˜ì´ë¸Œë¦¬ë“œ AI ëª¨ë¸ ì•„í‚¤í…ì²˜

#### 1.1 ì •ì  ëª¨ë¸ (Static Model) - ê¸°ë³¸ ìëª¨ìŒ 31ê°œ

**ëª¨ë¸ êµ¬ì¡°**:
```
ì…ë ¥ì¸µ: 42ê°œ íŠ¹ì§• (MediaPipe ì† ëœë“œë§ˆí¬ 21ê°œ Ã— (x, y))
â”œâ”€ Dense(128, activation='relu')
â”œâ”€ Dropout(0.3)
â”œâ”€ Dense(64, activation='relu')
â”œâ”€ Dropout(0.3)
â”œâ”€ Dense(32, activation='relu')
â””â”€ Dense(31, activation='softmax')  # 31ê°œ í´ë˜ìŠ¤

ì´ íŒŒë¼ë¯¸í„°: ~7,000ê°œ (ê²½ëŸ‰ ëª¨ë¸)
```

**ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬**:
```python
# MediaPipe ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
hand_landmarks = mp_hands.process(image).multi_hand_landmarks[0]

# 21ê°œ ëœë“œë§ˆí¬ ì¢Œí‘œ ì¶”ì¶œ (ì •ê·œí™”ëœ 0~1 ë²”ìœ„)
landmarks = []
for lm in hand_landmarks.landmark:
    landmarks.extend([lm.x, lm.y])  # z ì¢Œí‘œëŠ” ì‚¬ìš© ì•ˆ í•¨

# Z-score ì •ê·œí™” (í•™ìŠµ ì‹œ ì €ì¥ëœ í‰ê· /í‘œì¤€í¸ì°¨ ì‚¬ìš©)
landmarks_normalized = (landmarks - ksl_norm_mean) / ksl_norm_std

# ëª¨ë¸ ì¶”ë¡ 
prediction = ksl_model.predict(landmarks_normalized.reshape(1, -1))
predicted_class = np.argmax(prediction)
confidence = np.max(prediction)
```

**í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘**:
- **ìˆ˜ì§‘ ë„êµ¬**: `ksl_model_train/hand_capture.py`
- **ìƒ˜í”Œ ìˆ˜**: ê° ìëª¨ìŒë‹¹ 100~200ê°œ
- **ìˆ˜ì§‘ ì¡°ê±´**:
  - ë‹¤ì–‘í•œ ì† í¬ê¸° (ì„±ì¸ ë‚¨ì„±/ì—¬ì„±, ì²­ì†Œë…„)
  - ë‹¤ì–‘í•œ ê°ë„ (ì •ë©´, ì•½ê°„ ê¸°ìš¸ì„)
  - ë‹¤ì–‘í•œ ì¡°ëª… (ì‹¤ë‚´ í˜•ê´‘ë“±, ìì—°ê´‘, ì–´ë‘ìš´ í™˜ê²½)
  - ë‹¤ì–‘í•œ ë°°ê²½ (ë‹¨ìƒ‰, ë³µì¡í•œ ë°°ê²½)
- **ì €ì¥ í˜•ì‹**: `data/{label}/image_{timestamp}.jpg`

**ì •ê·œí™” (Normalization)**:
```python
# í•™ìŠµ ì‹œ í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚° ë° ì €ì¥
X_train_mean = np.mean(X_train, axis=0)
X_train_std = np.std(X_train, axis=0)
np.save('model/ksl_norm_mean.npy', X_train_mean)
np.save('model/ksl_norm_std.npy', X_train_std)

# ì¶”ë¡  ì‹œ ë™ì¼í•œ í†µê³„ ì‚¬ìš©
X_normalized = (X - X_train_mean) / X_train_std
```

**í•™ìŠµ ì„¤ì •**:
- Optimizer: Adam (learning_rate=0.001)
- Loss: Categorical Crossentropy
- Metrics: Accuracy
- Batch Size: 32
- Epochs: 50 (Early Stopping patience=10)
- Validation Split: 20%

#### 1.2 ì‹œí€€ìŠ¤ ëª¨ë¸ (Sequence Model) - ìŒììŒ/ë³µí•©ëª¨ìŒ 9ê°œ

**ëª¨ë¸ êµ¬ì¡°**:
```
ì…ë ¥ì¸µ: (max_timesteps, feature_dim)
       max_timesteps = 30 (ì•½ 1.5ì´ˆ @ 20fps)
       feature_dim = 10 (2ê°œ ëœë“œë§ˆí¬ Ã— 5ê°œ íŠ¹ì§•)
â”œâ”€ Bidirectional LSTM(128, return_sequences=True)
â”œâ”€ Dropout(0.3)
â”œâ”€ Bidirectional LSTM(64, return_sequences=True)
â”œâ”€ Dropout(0.3)
â”œâ”€ Bidirectional LSTM(32, return_sequences=False)
â”œâ”€ Dense(64, activation='relu')
â”œâ”€ Dropout(0.3)
â””â”€ Dense(9, activation='softmax')  # 9ê°œ í´ë˜ìŠ¤

ì´ íŒŒë¼ë¯¸í„°: ~250,000ê°œ
```

**í•µì‹¬ íŠ¹ì§• ì¶”ì¶œ (Feature Engineering)**:
```python
# capture_sequence.pyì—ì„œ ì‚¬ìš©ëœ íŠ¹ì§•
USE_LANDMARKS = {
    0: "wrist",      # ì†ëª© (ê¸°ì¤€ì )
    8: "index_tip"   # ê²€ì§€ ë (ì£¼ìš” ë™ì‘ í¬ì¸íŠ¸)
}

# ê° í”„ë ˆì„ë‹¹ ì¶”ì¶œë˜ëŠ” íŠ¹ì§• (ëœë“œë§ˆí¬ë‹¹ 5ê°œ)
features_per_landmark = [
    'x',        # ì •ê·œí™”ëœ x ì¢Œí‘œ (0~1)
    'y',        # ì •ê·œí™”ëœ y ì¢Œí‘œ (0~1)
    'dx',       # ì´ì „ í”„ë ˆì„ ëŒ€ë¹„ x ë³€í™”ëŸ‰
    'dy',       # ì´ì „ í”„ë ˆì„ ëŒ€ë¹„ y ë³€í™”ëŸ‰
    'spd_sum'   # ì†ë„ (âˆš(dxÂ² + dyÂ²))
]

# ì´ íŠ¹ì§• ì°¨ì›: 2ê°œ ëœë“œë§ˆí¬ Ã— 5ê°œ íŠ¹ì§• = 10ì°¨ì›
```

**ì‹œí€€ìŠ¤ ë°ì´í„° í˜•ì‹** (CSV):
```csv
frame,landmark_id,name,x,y,visibility,dx,dy,spd_sum
0,0,wrist,0.5123,0.6234,0.99,0.0,0.0,0.0
0,8,index_tip,0.6234,0.4567,0.98,0.0,0.0,0.0
1,0,wrist,0.5145,0.6256,0.99,0.0022,0.0022,0.0031
1,8,index_tip,0.6289,0.4523,0.98,0.0055,-0.0044,0.0070
...
```

**í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘**:
- **ìˆ˜ì§‘ ë„êµ¬**: `ksl_model_train/capture_sequence.py`
- **ìƒ˜í”Œ ìˆ˜**: ê° ìŒììŒ/ë³µí•©ëª¨ìŒë‹¹ 20~50ê°œ ì‹œí€€ìŠ¤
- **ìˆ˜ì§‘ ë°©ë²•**:
  1. SPACE í‚¤ ëˆ„ë¥´ë©´ 0.5~0.8ì´ˆ ë™ì•ˆ í”„ë ˆì„ ìˆ˜ì§‘ ì‹œì‘
  2. 20fpsë¡œ í”„ë ˆì„ ìº¡ì²˜ (ì•½ 10~16 í”„ë ˆì„)
  3. ê° í”„ë ˆì„ì—ì„œ ì†ëª©(0ë²ˆ)ê³¼ ê²€ì§€ ë(8ë²ˆ) ëœë“œë§ˆí¬ ì¶”ì¶œ
  4. ì´ì „ í”„ë ˆì„ê³¼ì˜ ì°¨ì´(dx, dy, spd_sum) ê³„ì‚°
  5. CSV íŒŒì¼ë¡œ ì €ì¥
- **ì €ì¥ ìœ„ì¹˜**: `data_seq/{label}/{label}_{timestamp}.csv`

**ë°ì´í„° ì¦ê°• (Data Augmentation)**:
```python
# train_sequence_model.pyì—ì„œ êµ¬í˜„ëœ ì¦ê°• ê¸°ë²•

1. ì†ë„ ë³€ê²½ (Speed Variation)
   - 0.8~1.2ë°° ì†ë„ë¡œ ì‹œí€€ìŠ¤ ì¬ìƒ˜í”Œë§
   - ì„ í˜• ë³´ê°„ìœ¼ë¡œ í”„ë ˆì„ ìƒì„±/ì œê±°
   
2. ë…¸ì´ì¦ˆ ì¶”ê°€ (Gaussian Noise)
   - ê° ì¢Œí‘œì— N(0, 0.01) ë…¸ì´ì¦ˆ ì¶”ê°€
   - ì‹¤ì œ ì† ë–¨ë¦¼ ì‹œë®¬ë ˆì´ì…˜
   
3. ìŠ¤ì¼€ì¼ ë³€ê²½ (Scale Variation)
   - 0.9~1.1ë°° í¬ê¸° ë³€ê²½
   - ë‹¤ì–‘í•œ ì† í¬ê¸° ì‹œë®¬ë ˆì´ì…˜
   
4. ì‹œê°„ ì´ë™ (Temporal Shift)
   - ì‹œí€€ìŠ¤ ì‹œì‘/ë ë¶€ë¶„ ëœë¤ ìë¥´ê¸°
   - ë™ì‘ íƒ€ì´ë° ë³€í™” í•™ìŠµ

# ì¦ê°• ë¹„ìœ¨: ì›ë³¸ 1ê°œ â†’ ì¦ê°• 4ê°œ (ì´ 5ë°°)
```

**íŒ¨ë”© ë° ì •ê·œí™”**:
```python
# ì‹œí€€ìŠ¤ ê¸¸ì´ í†µì¼ (Padding)
def pad_sequence(sequence, max_timesteps):
    seq_len = len(sequence)
    if seq_len < max_timesteps:
        # ë§ˆì§€ë§‰ í”„ë ˆì„ ë°˜ë³µ
        padding = [sequence[-1]] * (max_timesteps - seq_len)
        return sequence + padding
    else:
        # ê¸¸ì´ ì´ˆê³¼ ì‹œ ìë¥´ê¸°
        return sequence[:max_timesteps]

# Z-score ì •ê·œí™” (ì‹œí€€ìŠ¤ë³„)
def normalize_sequence(sequence, mean, std):
    sequence = np.array(sequence)
    return (sequence - mean) / (std + 1e-8)
```

**í•™ìŠµ ì„¤ì •**:
- Optimizer: Adam (learning_rate=0.001)
- Loss: Categorical Crossentropy
- Metrics: Accuracy
- Batch Size: 32
- Epochs: ìµœëŒ€ 150 (Early Stopping patience=20)
- Validation Split: 20%
- Class Weight: ë¶ˆê· í˜• ë°ì´í„° ë³´ì •

**Bidirectional LSTMì˜ ì¥ì **:
- **ì–‘ë°©í–¥ í•™ìŠµ**: ê³¼ê±° + ë¯¸ë˜ ì •ë³´ ëª¨ë‘ í™œìš©
- **ì‹œê°„ì  íŒ¨í„´ ì¸ì‹**: ìŒììŒì˜ "ë°˜ë³µ" íŒ¨í„´ í•™ìŠµ
- **ìˆœì„œ ì˜ì¡´ì„±**: ë³µí•©ëª¨ìŒì˜ "ìˆœì„œ" í•™ìŠµ (ì˜ˆ: ã…— â†’ ã… = ã…˜)

#### 1.3 í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ í†µí•©

**ìë™ ëª¨ë¸ ì„ íƒ ë¡œì§**:
```python
# myproject/api/recognition.py

SEQUENCE_SIGNS = ['ã„²', 'ã„¸', 'ã…ƒ', 'ã…†', 'ã…‰', 'ã…˜', 'ã…™', 'ã…', 'ã…']

def analyze_sign_accuracy(image_data, target_sign, language, user_id):
    """í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‹: ìë™ìœ¼ë¡œ ì •ì /ì‹œí€€ìŠ¤ ëª¨ë¸ ì„ íƒ"""
    
    if target_sign in SEQUENCE_SIGNS:
        # ì‹œí€€ìŠ¤ ëª¨ë¸ ì‚¬ìš© (ìŒììŒ/ë³µí•©ëª¨ìŒ)
        return analyze_sequence_sign(image_data, target_sign, language, user_id)
    else:
        # ì •ì  ëª¨ë¸ ì‚¬ìš© (ê¸°ë³¸ ìëª¨ìŒ)
        return analyze_static_sign(image_data, target_sign, language)
```

**ì‹œí€€ìŠ¤ ë²„í¼ ê´€ë¦¬**:
```python
# ì‚¬ìš©ìë³„ ë…ë¦½ì ì¸ ë²„í¼ (ë™ì‹œ ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì›)
from collections import deque

sequence_buffers = {}  # {user_id: deque}

def get_or_create_buffer(user_id, target_sign):
    """ì‚¬ìš©ìë³„ ì‹œí€€ìŠ¤ ë²„í¼ ìƒì„±/ì¡°íšŒ"""
    key = f"{user_id}_{target_sign}"
    
    if key not in sequence_buffers:
        sequence_buffers[key] = {
            'frames': deque(maxlen=30),  # ìµœëŒ€ 30í”„ë ˆì„ (1.5ì´ˆ)
            'target': target_sign,
            'created_at': time.time()
        }
    
    return sequence_buffers[key]

def clear_buffer(user_id):
    """ì‚¬ìš©ì ë²„í¼ ì´ˆê¸°í™”"""
    keys_to_remove = [k for k in sequence_buffers.keys() if k.startswith(f"{user_id}_")]
    for key in keys_to_remove:
        del sequence_buffers[key]
```

**ì‹œí€€ìŠ¤ ì¸ì‹ í”„ë¡œì„¸ìŠ¤**:
```python
def analyze_sequence_sign(image_data, target_sign, language, user_id):
    """ì‹œí€€ìŠ¤ ëª¨ë¸ ì¶”ë¡ """
    
    # 1. ì´ë¯¸ì§€ì—ì„œ ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
    landmarks = extract_hand_landmarks(image_data)
    if landmarks is None:
        return {'is_correct': False, 'message': 'ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤'}
    
    # 2. íŠ¹ì§• ì¶”ì¶œ (ì†ëª©, ê²€ì§€ ë)
    features = extract_sequence_features(landmarks)
    
    # 3. ë²„í¼ì— í”„ë ˆì„ ì¶”ê°€
    buffer = get_or_create_buffer(user_id, target_sign)
    buffer['frames'].append(features)
    
    # 4. ìµœì†Œ í”„ë ˆì„ ìˆ˜ í™•ì¸ (5í”„ë ˆì„ ì´ìƒ)
    if len(buffer['frames']) < 5:
        return {'is_correct': False, 'message': 'ë°ì´í„° ìˆ˜ì§‘ ì¤‘...'}
    
    # 5. ì‹œí€€ìŠ¤ íŒ¨ë”© ë° ì •ê·œí™”
    sequence = pad_sequence(list(buffer['frames']), seq_max_timesteps)
    sequence_normalized = normalize_sequence(sequence, seq_norm_mean, seq_norm_std)
    
    # 6. LSTM ëª¨ë¸ ì¶”ë¡ 
    prediction = ksl_seq_model.predict(sequence_normalized.reshape(1, seq_max_timesteps, -1))
    predicted_class = np.argmax(prediction)
    confidence = np.max(prediction)
    predicted_label = labels_ksl_seq[predicted_class]
    
    # 7. ì •ë‹µ íŒì • (ì‹ ë¢°ë„ 80% ì´ìƒ)
    is_correct = (predicted_label == target_sign and confidence >= 0.80)
    
    return {
        'is_correct': is_correct,
        'predicted': predicted_label,
        'confidence': float(confidence),
        'accuracy': float(confidence * 100),
        'buffer_size': len(buffer['frames'])
    }
```

### 2. ë°±ì—”ë“œ API ì„¤ê³„ (Flask RESTful API)

#### 2.1 í”„ë¡œì íŠ¸ êµ¬ì¡°
```
myproject/
â”œâ”€â”€ app.py                      # Flask ë©”ì¸ ì„œë²„ (ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°)
â”œâ”€â”€ config.py                   # ì„¤ì • íŒŒì¼ (DB, JWT ë“±)
â”œâ”€â”€ models.py                   # ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸
â”œâ”€â”€ create_tables.py            # DB í…Œì´ë¸” ìƒì„± ìŠ¤í¬ë¦½íŠ¸
â”‚
â”œâ”€â”€ auth/                       # ì¸ì¦ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ models.py              # User, Recognition ë“± ëª¨ë¸
â”‚   â””â”€â”€ routes.py              # íšŒì›ê°€ì…, ë¡œê·¸ì¸, ë¡œê·¸ì•„ì›ƒ
â”‚
â”œâ”€â”€ api/                        # API ì—”ë“œí¬ì¸íŠ¸
â”‚   â”œâ”€â”€ recognition.py         # í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‹ ì—”ì§„ (í•µì‹¬)
â”‚   â”œâ”€â”€ progress.py            # í•™ìŠµ ì§„ë„ ê´€ë¦¬
â”‚   â”œâ”€â”€ learning.py            # í•™ìŠµ ì„¸ì…˜ ê´€ë¦¬
â”‚   â”œâ”€â”€ quiz.py                # í€´ì¦ˆ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ jamo_decompose.py      # ìëª¨ ë¶„í•´ (ì˜ˆ: "ì•ˆë…•" â†’ ["ã…‡","ã…","ã„´",...])
â”‚   â””â”€â”€ jamo_compose.py        # ìëª¨ ì¡°í•© (ì˜ˆ: ["ã…","ã…","ã„´"] â†’ "í•œ")
â”‚
â”œâ”€â”€ model/                      # AI ëª¨ë¸ íŒŒì¼
â”‚   â”œâ”€â”€ ksl_model.h5           # ì •ì  ëª¨ë¸ (31ê°œ í´ë˜ìŠ¤)
â”‚   â”œâ”€â”€ ksl_model_sequence.h5  # ì‹œí€€ìŠ¤ ëª¨ë¸ (9ê°œ í´ë˜ìŠ¤)
â”‚   â”œâ”€â”€ ksl_labels.npy         # ì •ì  ë¼ë²¨
â”‚   â”œâ”€â”€ ksl_labels_sequence.npy # ì‹œí€€ìŠ¤ ë¼ë²¨
â”‚   â”œâ”€â”€ ksl_sequence_config.npy # ì‹œí€€ìŠ¤ ì„¤ì • (max_timesteps)
â”‚   â”œâ”€â”€ ksl_norm_mean.npy      # ì •ì  ëª¨ë¸ ì •ê·œí™” í‰ê· 
â”‚   â”œâ”€â”€ ksl_norm_std.npy       # ì •ì  ëª¨ë¸ ì •ê·œí™” í‘œì¤€í¸ì°¨
â”‚   â”œâ”€â”€ ksl_seq_norm_mean.npy  # ì‹œí€€ìŠ¤ ëª¨ë¸ ì •ê·œí™” í‰ê· 
â”‚   â””â”€â”€ ksl_seq_norm_std.npy   # ì‹œí€€ìŠ¤ ëª¨ë¸ ì •ê·œí™” í‘œì¤€í¸ì°¨
â”‚
â”œâ”€â”€ instance/                   # ì¸ìŠ¤í„´ìŠ¤ í´ë”
â”‚   â””â”€â”€ signtalk.db            # SQLite ë°ì´í„°ë² ì´ìŠ¤
â”‚
â””â”€â”€ requirements.txt            # Python íŒ¨í‚¤ì§€ ëª©ë¡
```

#### 2.2 ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

**User í…Œì´ë¸”** (ì‚¬ìš©ì ì •ë³´):
```sql
CREATE TABLE user (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    username VARCHAR(80) UNIQUE NOT NULL,
    email VARCHAR(120) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

**Recognition í…Œì´ë¸”** (ì¸ì‹ ê¸°ë¡):
```sql
CREATE TABLE recognition (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    language VARCHAR(10) NOT NULL,  -- 'ksl' ë˜ëŠ” 'asl'
    recognized_text VARCHAR(255),
    confidence FLOAT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES user(id)
);
```

**Progress í…Œì´ë¸”** (í•™ìŠµ ì§„ë„):
```sql
CREATE TABLE progress (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    language VARCHAR(10) NOT NULL,
    character VARCHAR(10) NOT NULL,  -- í•™ìŠµí•œ ìëª¨ìŒ
    level INTEGER DEFAULT 1,
    attempts INTEGER DEFAULT 0,      -- ì‹œë„ íšŸìˆ˜
    successes INTEGER DEFAULT 0,     -- ì„±ê³µ íšŸìˆ˜
    last_practiced DATETIME,
    FOREIGN KEY (user_id) REFERENCES user(id),
    UNIQUE(user_id, language, character)
);
```

**QuizResult í…Œì´ë¸”** (í€´ì¦ˆ ê²°ê³¼):
```sql
CREATE TABLE quiz_result (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER NOT NULL,
    language VARCHAR(10) NOT NULL,
    quiz_type VARCHAR(50) NOT NULL,  -- 'word', 'beginner', 'intermediate', 'advanced'
    score INTEGER NOT NULL,          -- ì ìˆ˜ (0~100)
    total_questions INTEGER NOT NULL,
    correct_answers INTEGER NOT NULL,
    time_spent INTEGER,              -- ì†Œìš” ì‹œê°„ (ì´ˆ)
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES user(id)
);
```

#### 2.3 í•µì‹¬ API ì—”ë“œí¬ì¸íŠ¸

**ì¸ì¦ API** (`/api/auth/*`):
```python
# íšŒì›ê°€ì…
POST /api/auth/register
Request: {
    "username": "user123",
    "email": "user@example.com",
    "password": "password123"
}
Response: {
    "message": "íšŒì›ê°€ì… ì„±ê³µ",
    "user": {"id": 1, "username": "user123", "email": "user@example.com"}
}

# ë¡œê·¸ì¸
POST /api/auth/login
Request: {
    "username": "user123",
    "password": "password123"
}
Response: {
    "access_token": "eyJ0eXAiOiJKV1QiLCJhbGc...",
    "user": {"id": 1, "username": "user123"}
}

# ë¡œê·¸ì•„ì›ƒ
POST /api/auth/logout
Headers: Authorization: Bearer <token>
Response: {"message": "ë¡œê·¸ì•„ì›ƒ ì„±ê³µ"}

# í—¬ìŠ¤ ì²´í¬
GET /api/auth/health
Response: {"status": "ok", "message": "ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤"}
```

**ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° API**:
```python
# ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë° (MJPEG)
GET /video_feed_ksl
Response: multipart/x-mixed-replace (MJPEG ìŠ¤íŠ¸ë¦¼)
Features:
  - MediaPipe ì† ëœë“œë§ˆí¬ ì˜¤ë²„ë ˆì´
  - ì‹¤ì‹œê°„ ì¸ì‹ ê²°ê³¼ í‘œì‹œ
  - 30fps ìŠ¤íŠ¸ë¦¬ë°
  - 0.15ì´ˆë§ˆë‹¤ ì¸ì‹ ìˆ˜í–‰
  - ì‹ ë¢°ë„ 60% ì´ìƒë§Œ í‘œì‹œ
  - ìŒììŒ ìë™ ê°ì§€ (3ì´ˆ ì´ë‚´ ê°™ì€ ììŒ 2ë²ˆ)
```

**ì¸ì‹ API** (`/api/recognition/*`):
```python
# ì†ëª¨ì–‘ ë¶„ì„ (í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‹)
POST /api/recognition/analyze-hand
Headers: Authorization: Bearer <token>
Request: {
    "target_sign": "ã„²",           # ëª©í‘œ ìëª¨ìŒ
    "language": "ksl",             # ì–¸ì–´ (ksl/asl)
    "session_id": "uuid-string",   # ì„¸ì…˜ ID (ì„ íƒ)
    "image_data": "base64..."      # ì´ë¯¸ì§€ (ì„ íƒ, ì—†ìœ¼ë©´ ì„œë²„ ì¹´ë©”ë¼ ì‚¬ìš©)
}
Response: {
    "success": true,
    "analysis": {
        "predicted": "ã„²",
        "accuracy": 92.5,
        "confidence": 0.925,
        "is_correct": true,
        "feedback": {
            "level": "excellent",  # excellent/good/fair/needs_improvement/poor
            "message": "ì™„ë²½í•´ìš”! ğŸ‰",
            "suggestions": []
        }
    },
    "session_updated": true,
    "message": "ì†ëª¨ì–‘ ë¶„ì„ ì™„ë£Œ"
}

# ì‹œí€€ìŠ¤ ë²„í¼ ì´ˆê¸°í™”
POST /api/recognition/clear-buffer
Headers: Authorization: Bearer <token>
Response: {
    "success": true,
    "message": "ì‹œí€€ìŠ¤ ë²„í¼ ì´ˆê¸°í™” ì™„ë£Œ"
}

# ì¸ì‹ í†µê³„ ì¡°íšŒ
GET /api/recognition/stats?language=ksl
Headers: Authorization: Bearer <token>
Response: {
    "success": true,
    "total_attempts": 150,
    "average_confidence": 0.87,
    "recent_activity": [
        {"character": "ã„±", "timestamp": "2025-01-26T10:30:00", "confidence": 0.92},
        ...
    ]
}
```

**í•™ìŠµ ì§„ë„ API** (`/api/progress/*`):
```python
# ì§„ë„ ì¡°íšŒ
GET /api/progress/ksl
Headers: Authorization: Bearer <token>
Response: {
    "success": true,
    "progress": [
        {
            "character": "ã„±",
            "level": 1,
            "attempts": 10,
            "successes": 8,
            "accuracy": 80.0,
            "last_practiced": "2025-01-26T10:30:00"
        },
        ...
    ],
    "total_learned": 15,
    "total_characters": 40
}

# ì§„ë„ ì—…ë°ì´íŠ¸
POST /api/progress/ksl/update
Headers: Authorization: Bearer <token>
Request: {
    "character": "ã„±",
    "is_correct": true,
    "confidence": 0.92
}
Response: {
    "success": true,
    "message": "ì§„ë„ ì—…ë°ì´íŠ¸ ì™„ë£Œ",
    "progress": {
        "character": "ã„±",
        "attempts": 11,
        "successes": 9,
        "accuracy": 81.8
    }
}

# íŠ¹ì • ë¬¸ì ì§„ë„ ì¡°íšŒ
GET /api/progress/ksl/character/ã„±
Headers: Authorization: Bearer <token>
Response: {
    "success": true,
    "character": "ã„±",
    "level": 1,
    "attempts": 10,
    "successes": 8,
    "accuracy": 80.0
}
```

**í€´ì¦ˆ API** (`/api/quiz/*`):
```python
# í€´ì¦ˆ ì‹œì‘
POST /api/quiz/ksl/start
Headers: Authorization: Bearer <token>
Request: {
    "quiz_type": "beginner",  # word/beginner/intermediate/advanced
    "level": 1
}
Response: {
    "success": true,
    "quiz_id": "uuid-string",
    "problems": [
        {"word": "ì•ˆë…•", "jamo_sequence": ["ã…‡","ã…","ã„´","ã„´","ã…•","ã…‡"]},
        ...
    ],
    "total_questions": 10,
    "time_limit": 300  # ì´ˆ
}

# í€´ì¦ˆ ì œì¶œ
POST /api/quiz/ksl/submit
Headers: Authorization: Bearer <token>
Request: {
    "quiz_id": "uuid-string",
    "answers": [
        {"question_index": 0, "user_answer": "ì•ˆë…•", "is_correct": true, "time_spent": 25},
        ...
    ],
    "total_time": 250
}
Response: {
    "success": true,
    "result": {
        "score": 85,
        "correct_answers": 8,
        "total_questions": 10,
        "time_spent": 250,
        "rank": "A"
    }
}

# í€´ì¦ˆ ê²°ê³¼ ì¡°íšŒ
GET /api/quiz/ksl/results?limit=10
Headers: Authorization: Bearer <token>
Response: {
    "success": true,
    "results": [
        {
            "quiz_type": "beginner",
            "score": 85,
            "correct_answers": 8,
            "total_questions": 10,
            "timestamp": "2025-01-26T10:30:00"
        },
        ...
    ]
}
```

**ìëª¨ ë¶„í•´/ì¡°í•© API**:
```python
# ìëª¨ ë¶„í•´ (ë‹¨ì–´ â†’ ìëª¨ ë°°ì—´)
POST /api/jamo/decompose
Request: {"text": "ì•ˆë…•"}
Response: {
    "success": true,
    "original": "ì•ˆë…•",
    "jamo_sequence": ["ã…‡", "ã…", "ã„´", "ã„´", "ã…•", "ã…‡"]
}

# ìëª¨ ì¡°í•© (ìëª¨ ë°°ì—´ â†’ ë‹¨ì–´)
POST /api/jamo/compose
Request: {"jamo_sequence": ["ã…", "ã…", "ã„´"]}
Response: {
    "success": true,
    "jamo_sequence": ["ã…", "ã…", "ã„´"],
    "composed": "í•œ"
}
```

#### 2.4 ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„ (app.py)

**MJPEG ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹**:
```python
@app.route('/video_feed_ksl')
def video_feed_ksl():
    """ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë° (MJPEG)"""
    camera_device = 0  # ê¸°ë³¸ ì¹´ë©”ë¼
    
    return Response(
        generate_frames(ksl_model, labels_ksl, 'ksl', camera_device),
        mimetype='multipart/x-mixed-replace; boundary=frame'
    )

def generate_frames(model, labels, lang_key, camera_device=0):
    """í”„ë ˆì„ ìƒì„± ì œë„ˆë ˆì´í„°"""
    
    # 1. ì¹´ë©”ë¼ ì´ˆê¸°í™”
    cap = cv2.VideoCapture(camera_device)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ë²„í¼ ìµœì†Œí™” (ì§€ì—° ê°ì†Œ)
    
    # 2. ì¸ì‹ ì„¤ì •
    last_prediction_time = 0
    prediction_interval = 0.15  # 0.15ì´ˆë§ˆë‹¤ ì¸ì‹
    confidence_threshold = 0.6  # ì‹ ë¢°ë„ 60% ì´ìƒë§Œ í‘œì‹œ
    
    # 3. ìŒììŒ ê°ì§€ ë³€ìˆ˜
    last_recognized_char = ""
    last_recognized_time = 0
    DOUBLE_CONSONANT_MAP = {'ã„±': 'ã„²', 'ã„·': 'ã„¸', 'ã…‚': 'ã…ƒ', 'ã……': 'ã…†', 'ã…ˆ': 'ã…‰'}
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # 4. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            image = cv2.flip(frame, 1)  # ì¢Œìš° ë°˜ì „ (ê±°ìš¸ ëª¨ë“œ)
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            current_time = time.time()
            
            # 5. MediaPipe ì† ê°ì§€ (í•­ìƒ í™œì„±í™”)
            result = hands.process(rgb_image)
            
            if result.multi_hand_landmarks:
                for hand_landmarks in result.multi_hand_landmarks:
                    # ì† ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                    mp_draw.draw_landmarks(
                        image, 
                        hand_landmarks, 
                        mp_hands.HAND_CONNECTIONS
                    )
                    
                    # 6. ì£¼ê¸°ì  ì¸ì‹ (0.15ì´ˆë§ˆë‹¤)
                    if current_time - last_prediction_time >= prediction_interval:
                        # ì¢Œí‘œ ì¶”ì¶œ (21ê°œ ëœë“œë§ˆí¬ Ã— 2 = 42ê°œ íŠ¹ì§•)
                        coords = [v for lm in hand_landmarks.landmark for v in (lm.x, lm.y)]
                        input_data = np.array(coords, dtype=np.float32).reshape(1, -1)
                        
                        # ì •ê·œí™” ì ìš©
                        from api.recognition import ksl_norm_mean, ksl_norm_std
                        if ksl_norm_mean is not None and ksl_norm_std is not None:
                            input_data = (input_data - ksl_norm_mean) / (ksl_norm_std + 1e-8)
                        
                        # ëª¨ë¸ ì¶”ë¡ 
                        prediction = model.predict(input_data, verbose=0)
                        idx = int(np.argmax(prediction))
                        confidence = float(np.max(prediction))
                        
                        # ì‹ ë¢°ë„ ì„ê³„ê°’ ì²´í¬
                        if confidence >= confidence_threshold and 0 <= idx < len(labels):
                            character = labels[idx]
                            
                            # 7. ìŒììŒ ê°ì§€ ë¡œì§
                            if character in DOUBLE_CONSONANT_MAP:
                                # 3ì´ˆ ì´ë‚´ì— ê°™ì€ ììŒì´ 2ë²ˆ ë‚˜ì˜¤ë©´ ìŒììŒ
                                if (character == last_recognized_char and 
                                    current_time - last_recognized_time < 3.0):
                                    character = DOUBLE_CONSONANT_MAP[character]
                                    last_recognized_char = ""  # ë¦¬ì…‹
                                else:
                                    last_recognized_char = character
                                    last_recognized_time = current_time
                            else:
                                last_recognized_char = character
                                last_recognized_time = current_time
                            
                            # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
                            latest_char[lang_key] = character
                            
                            # í™”ë©´ì— í‘œì‹œ
                            cv2.putText(
                                image,
                                f"{character} ({confidence*100:.1f}%)",
                                (10, 50),
                                cv2.FONT_HERSHEY_SIMPLEX,
                                1.5,
                                (0, 255, 0),
                                3
                            )
                        
                        last_prediction_time = current_time
            
            # 8. JPEG ì¸ì½”ë”© ë° ìŠ¤íŠ¸ë¦¬ë°
            ret, buffer = cv2.imencode('.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, 80])
            frame_bytes = buffer.tobytes()
            
            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
    
    finally:
        cap.release()
```

**ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸**:
1. **ë²„í¼ ìµœì†Œí™”**: `CAP_PROP_BUFFERSIZE = 1` (ì§€ì—° ê°ì†Œ)
2. **ì£¼ê¸°ì  ì¸ì‹**: 0.15ì´ˆë§ˆë‹¤ (CPU ë¶€í•˜ ê°ì†Œ)
3. **JPEG í’ˆì§ˆ**: 80% (ì „ì†¡ ì†ë„ í–¥ìƒ)
4. **ì •ê·œí™” ìºì‹±**: í‰ê· /í‘œì¤€í¸ì°¨ ë¯¸ë¦¬ ë¡œë“œ
5. **verbose=0**: TensorFlow ë¡œê·¸ ë¹„í™œì„±í™”

### 3. í”„ë¡ íŠ¸ì—”ë“œ ì„¤ê³„ (Flutter)

#### 3.1 í”„ë¡œì íŠ¸ êµ¬ì¡°
```
front/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ main.dart                    # ë©”ì¸ ì•± + í•™ìŠµ/í€´ì¦ˆ í™”ë©´
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â””â”€â”€ auth_provider.dart       # ì¸ì¦ ìƒíƒœ ê´€ë¦¬ (Provider)
â”‚   â”œâ”€â”€ screens/
â”‚   â”‚   â”œâ”€â”€ splash_screen.dart       # ìŠ¤í”Œë˜ì‹œ í™”ë©´
â”‚   â”‚   â”œâ”€â”€ auth_screen.dart         # ë¡œê·¸ì¸/íšŒì›ê°€ì… í™”ë©´
â”‚   â”‚   â””â”€â”€ my_page_screen.dart      # ë§ˆì´í˜ì´ì§€ (í†µê³„, ì„¤ì •)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ auth_service.dart        # ì¸ì¦ API í˜¸ì¶œ
â”‚   â”‚   â”œâ”€â”€ recognition_service.dart # ì¸ì‹ API í˜¸ì¶œ
â”‚   â”‚   â”œâ”€â”€ progress_service.dart    # ì§„ë„ API í˜¸ì¶œ
â”‚   â”‚   â”œâ”€â”€ quiz_service.dart        # í€´ì¦ˆ API í˜¸ì¶œ
â”‚   â”‚   â””â”€â”€ jamo_service.dart        # ìëª¨ ë¶„í•´/ì¡°í•© API
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ user.dart                # User ëª¨ë¸
â”‚
â”œâ”€â”€ assets/                          # ë¦¬ì†ŒìŠ¤ íŒŒì¼
â”‚   â”œâ”€â”€ images/                      # ì´ë¯¸ì§€ (ë¡œê³ , ì•„ì´ì½˜)
â”‚   â””â”€â”€ reference_images/            # ìˆ˜ì–´ ì°¸ê³  ì´ë¯¸ì§€ (40ê°œ)
â”‚
â”œâ”€â”€ pubspec.yaml                     # Flutter íŒ¨í‚¤ì§€ ì„¤ì •
â””â”€â”€ README.md
```

#### 3.2 í•™ìŠµ ëª¨ë“œ ì‹œìŠ¤í…œ

**5ë‹¨ê³„ ë ˆë²¨ êµ¬ì¡°**:
```dart
// main.dartì˜ í•™ìŠµ ì‹œí€€ìŠ¤ (40ê°œ ìëª¨ìŒ)
final List<String> learningSequence = [
  // ë ˆë²¨ 1: ê¸°ì´ˆ ììŒ + ìŒììŒ (11ê°œ)
  'ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸', 'ã„¹', 'ã…', 'ã…‚', 'ã…ƒ', 'ã……', 'ã…†',
  
  // ë ˆë²¨ 2: ê³ ê¸‰ ììŒ (8ê°œ)
  'ã…‡', 'ã…ˆ', 'ã…‰', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…',
  
  // ë ˆë²¨ 3: ê¸°ë³¸ ëª¨ìŒ (10ê°œ)
  'ã…', 'ã…‘', 'ã…“', 'ã…•', 'ã…—', 'ã…›', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£',
  
  // ë ˆë²¨ 4: ì´ì¤‘ ëª¨ìŒ (4ê°œ)
  'ã…', 'ã…’', 'ã…”', 'ã…–',
  
  // ë ˆë²¨ 5: ë³µí•© ëª¨ìŒ (7ê°œ)
  'ã…˜', 'ã…™', 'ã…š', 'ã…', 'ã…', 'ã…Ÿ', 'ã…¢',
];

// ë ˆë²¨ë³„ ë²”ìœ„
Map<int, Map<String, int>> levelRanges = {
  1: {'start': 0, 'end': 11},   // 0~10 (11ê°œ)
  2: {'start': 11, 'end': 19},  // 11~18 (8ê°œ)
  3: {'start': 19, 'end': 29},  // 19~28 (10ê°œ)
  4: {'start': 29, 'end': 33},  // 29~32 (4ê°œ)
  5: {'start': 33, 'end': 40},  // 33~39 (7ê°œ)
};
```

**í•™ìŠµ ì§„ë„ ê³„ì‚°**:
```dart
Map<String, dynamic> _calculateLevelProgress() {
  // ë°±ì—”ë“œì—ì„œ ì§„ë„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
  final progressData = await ProgressService.getProgress(language: 'ksl');
  
  // í•™ìŠµ ì™„ë£Œí•œ ìëª¨ìŒ ê°œìˆ˜ ì„¸ê¸°
  int learnedCount = 0;
  for (var progress in progressData['progress']) {
    if (progress['accuracy'] >= 80.0) {  // 80% ì´ìƒì´ë©´ í•™ìŠµ ì™„ë£Œ
      learnedCount++;
    }
  }
  
  // í˜„ì¬ ë ˆë²¨ ê³„ì‚°
  int currentLevel = 1;
  for (int level = 1; level <= 5; level++) {
    int levelStart = levelRanges[level]!['start']!;
    int levelEnd = levelRanges[level]!['end']!;
    int levelSize = levelEnd - levelStart;
    
    if (learnedCount >= levelEnd) {
      currentLevel = level + 1;  // ë‹¤ìŒ ë ˆë²¨ë¡œ
    } else {
      break;
    }
  }
  
  return {
    'level': currentLevel,
    'learned_count': learnedCount,
    'total_count': 40,
    'progress_percentage': (learnedCount / 40 * 100).toInt()
  };
}
```

**ì‹¤ì‹œê°„ ì¸ì‹ ë° ì§„ë„ ì²´í¬**:
```dart
void _checkLearningProgress() async {
  String currentTarget = getCurrentLearningCharacter();
  const sequenceSigns = ['ã„²', 'ã„¸', 'ã…ƒ', 'ã…†', 'ã…‰', 'ã…˜', 'ã…™', 'ã…', 'ã…'];
  
  bool isCorrect = false;
  
  if (sequenceSigns.contains(currentTarget)) {
    // ì‹œí€€ìŠ¤ ì‚¬ì¸: ë°±ì—”ë“œ ë¶„ì„ API í˜¸ì¶œ
    final result = await RecognitionService.analyzeHandShape(
      targetSign: currentTarget,
      language: 'ksl',
      sessionId: _currentSessionId,
    );
    
    if (result['success'] == true && 
        result['analysis']['is_correct'] == true && 
        result['analysis']['accuracy'] >= 80.0) {
      isCorrect = true;
    }
  } else {
    // ì •ì  ì‚¬ì¸: ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì—ì„œ ì¸ì‹ëœ ë¬¸ìì™€ ë¹„êµ
    isCorrect = (currentRecognition == currentTarget);
  }
  
  if (isCorrect) {
    // ë°±ì—”ë“œ ì§„ë„ ì—…ë°ì´íŠ¸
    await _updateBackendProgress(currentTarget);
    
    // UI ì—…ë°ì´íŠ¸
    setState(() {
      _showSuccessAnimation();
      _moveToNextCharacter();
    });
  }
}

Future<void> _updateBackendProgress(String character) async {
  final result = await ProgressService.updateProgress(
    language: 'ksl',
    character: character,
    isCorrect: true,
    confidence: 0.9,
  );
  
  if (result['success']) {
    print('âœ… ì§„ë„ ì—…ë°ì´íŠ¸ ì„±ê³µ: $character');
  }
}
```

#### 3.3 ì¹´ë©”ë¼ ëª¨ë“œ ìë™ ì„ íƒ

**í”Œë«í¼ë³„ ì¹´ë©”ë¼ ì „ëµ**:
```dart
enum CameraMode {
  serverStream,   // ì„œë²„ MJPEG ìŠ¤íŠ¸ë¦¼ (ì—ë®¬ë ˆì´í„°ìš©)
  deviceCamera,   // ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼ (ì‹¤ì œ ê¸°ê¸°ìš©)
}

Future<CameraMode> _determineCameraMode() async {
  if (kIsWeb) {
    // ì›¹: ì„œë²„ ìŠ¤íŠ¸ë¦¼ ì‚¬ìš©
    return CameraMode.serverStream;
  }
  
  if (Platform.isAndroid) {
    // Android: ì—ë®¬ë ˆì´í„° ê°ì§€
    final deviceInfo = await DeviceInfoPlugin().androidInfo;
    if (deviceInfo.isPhysicalDevice) {
      // ì‹¤ì œ ê¸°ê¸°: ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼
      return CameraMode.deviceCamera;
    } else {
      // ì—ë®¬ë ˆì´í„°: ì„œë²„ ìŠ¤íŠ¸ë¦¼
      return CameraMode.serverStream;
    }
  }
  
  if (Platform.isIOS) {
    // iOS: í•­ìƒ ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼ (ì‹œë®¬ë ˆì´í„°ëŠ” ì¹´ë©”ë¼ ì—†ìŒ)
    return CameraMode.deviceCamera;
  }
  
  return CameraMode.serverStream;
}
```

**ì„œë²„ ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ** (MJPEG):
```dart
Widget _buildServerStreamView() {
  return MjpegView(
    stream: 'http://10.143.2.150:5002/video_feed_ksl',
    isLive: true,
    width: MediaQuery.of(context).size.width,
    height: 400,
    fit: BoxFit.cover,
    error: (context, error, stack) {
      return Center(
        child: Text('ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨: $error'),
      );
    },
    loading: (context) {
      return Center(
        child: CircularProgressIndicator(),
      );
    },
  );
}
```

**ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼ ëª¨ë“œ**:
```dart
CameraController? _cameraController;
bool _isCameraInitialized = false;

Future<void> _initializeDeviceCamera() async {
  try {
    // ì¹´ë©”ë¼ ê¶Œí•œ ìš”ì²­
    final status = await Permission.camera.request();
    if (!status.isGranted) {
      print('âŒ ì¹´ë©”ë¼ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤');
      return;
    }
    
    // ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë©”ë¼ ëª©ë¡
    final cameras = await availableCameras();
    if (cameras.isEmpty) {
      print('âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë©”ë¼ê°€ ì—†ìŠµë‹ˆë‹¤');
      return;
    }
    
    // ì „ë©´ ì¹´ë©”ë¼ ì„ íƒ (ìˆ˜ì–´ ì¸ì‹ìš©)
    final frontCamera = cameras.firstWhere(
      (camera) => camera.lensDirection == CameraLensDirection.front,
      orElse: () => cameras.first,
    );
    
    // ì¹´ë©”ë¼ ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™”
    _cameraController = CameraController(
      frontCamera,
      ResolutionPreset.medium,  // ì¤‘ê°„ í•´ìƒë„ (ì„±ëŠ¥ ìµœì í™”)
      enableAudio: false,
      imageFormatGroup: ImageFormatGroup.jpeg,
    );
    
    await _cameraController!.initialize();
    
    setState(() {
      _isCameraInitialized = true;
    });
    
    print('âœ… ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì™„ë£Œ');
    
    // ì£¼ê¸°ì ìœ¼ë¡œ í”„ë ˆì„ ìº¡ì²˜ ë° ë°±ì—”ë“œ ì „ì†¡
    _startFrameCapture();
    
  } catch (e) {
    print('âŒ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨: $e');
  }
}

void _startFrameCapture() {
  Timer.periodic(Duration(milliseconds: 500), (timer) async {
    if (!_isCameraInitialized || _cameraController == null) {
      timer.cancel();
      return;
    }
    
    try {
      // í”„ë ˆì„ ìº¡ì²˜
      final image = await _cameraController!.takePicture();
      final bytes = await image.readAsBytes();
      final base64Image = base64Encode(bytes);
      
      // ë°±ì—”ë“œë¡œ ì „ì†¡ (ì‹œí€€ìŠ¤ ì‚¬ì¸ì¸ ê²½ìš°)
      if (_isSequenceSign(currentTarget)) {
        await RecognitionService.analyzeHandShape(
          targetSign: currentTarget,
          language: 'ksl',
          imageData: base64Image,
        );
      }
    } catch (e) {
      print('í”„ë ˆì„ ìº¡ì²˜ ì‹¤íŒ¨: $e');
    }
  });
}

Widget _buildDeviceCameraView() {
  if (!_isCameraInitialized || _cameraController == null) {
    return Center(child: CircularProgressIndicator());
  }
  
  return CameraPreview(_cameraController!);
}
```

#### 3.4 í€´ì¦ˆ ì‹œìŠ¤í…œ

**4ê°€ì§€ í€´ì¦ˆ ëª¨ë“œ**:
```dart
enum QuizType {
  word,          // ë‚±ë§ í€´ì¦ˆ (ì˜ˆ: "ì•ˆë…•" â†’ ã…‡,ã…,ã„´,ã„´,ã…•,ã…‡)
  beginner,      // ì´ˆê¸‰ (ë ˆë²¨ 1~2 ìëª¨ìŒ)
  intermediate,  // ì¤‘ê¸‰ (ë ˆë²¨ 3~4 ìëª¨ìŒ)
  advanced,      // ê³ ê¸‰ (ë ˆë²¨ 5 ìëª¨ìŒ + ìŒììŒ/ë³µí•©ëª¨ìŒ)
}

// í€´ì¦ˆ ë°ì´í„° (ë°±ì—”ë“œì—ì„œ ê°€ì ¸ì˜´)
Map<QuizType, List<Map<String, String>>> quizData = {
  QuizType.word: [
    {'word': 'ì•ˆë…•', 'jamo': 'ã…‡,ã…,ã„´,ã„´,ã…•,ã…‡'},
    {'word': 'ê°ì‚¬', 'jamo': 'ã„±,ã…,ã…,ã……,ã…'},
    {'word': 'ì‚¬ë‘', 'jamo': 'ã……,ã…,ã„¹,ã…,ã…‡'},
    // ... ì´ 20ê°œ
  ],
  QuizType.beginner: [
    {'character': 'ã„±'},
    {'character': 'ã„´'},
    {'character': 'ã„·'},
    // ... ë ˆë²¨ 1~2 ìëª¨ìŒ
  ],
  // ...
};
```

**í€´ì¦ˆ ì‹œì‘ ë° ì§„í–‰**:
```dart
Future<void> _startQuiz(QuizType quizType) async {
  // 1. ë°±ì—”ë“œì—ì„œ í€´ì¦ˆ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
  final result = await QuizService.startQuiz(
    language: 'ksl',
    quizType: quizType.toString().split('.').last,
    level: _getCurrentLevel(),
  );
  
  if (result['success']) {
    setState(() {
      _currentQuizId = result['quiz_id'];
      _quizProblems = result['problems'];
      _currentQuestionIndex = 0;
      _correctAnswers = 0;
      _quizStartTime = DateTime.now();
      isQuizStarted = true;
    });
    
    // íƒ€ì´ë¨¸ ì‹œì‘ (ë¬¸ì œë‹¹ 25ì´ˆ)
    _startQuizTimer();
  }
}

void _startQuizTimer() {
  _timer = Timer.periodic(Duration(seconds: 1), (timer) {
    setState(() {
      if (timeRemaining > 0) {
        timeRemaining--;
      } else {
        // ì‹œê°„ ì´ˆê³¼: ë‹¤ìŒ ë¬¸ì œë¡œ
        _moveToNextQuestion(isCorrect: false);
      }
    });
  });
}

void _checkQuizAnswer() {
  final currentProblem = _quizProblems[_currentQuestionIndex];
  bool isCorrect = false;
  
  if (selectedQuizType == QuizType.word) {
    // ë‚±ë§ í€´ì¦ˆ: ìëª¨ ìˆœì„œëŒ€ë¡œ ì¸ì‹í–ˆëŠ”ì§€ í™•ì¸
    final expectedJamo = currentProblem['jamo_sequence'];
    final userJamo = _recognizedJamoSequence;
    isCorrect = (expectedJamo.join(',') == userJamo.join(','));
  } else {
    // ìëª¨ í€´ì¦ˆ: ë‹¨ì¼ ìëª¨ ì¸ì‹
    final expectedChar = currentProblem['character'];
    isCorrect = (currentRecognition == expectedChar);
  }
  
  if (isCorrect) {
    _correctAnswers++;
    _showCorrectFeedback();
  } else {
    _showIncorrectFeedback();
  }
  
  _moveToNextQuestion(isCorrect: isCorrect);
}

void _moveToNextQuestion({required bool isCorrect}) {
  // ë‹µì•ˆ ê¸°ë¡
  _quizAnswers.add({
    'question_index': _currentQuestionIndex,
    'is_correct': isCorrect,
    'time_spent': 25 - timeRemaining,
  });
  
  if (_currentQuestionIndex < _quizProblems.length - 1) {
    // ë‹¤ìŒ ë¬¸ì œë¡œ
    setState(() {
      _currentQuestionIndex++;
      timeRemaining = 25;
    });
  } else {
    // í€´ì¦ˆ ì¢…ë£Œ
    _endQuiz();
  }
}

Future<void> _endQuiz() async {
  _timer?.cancel();
  
  final totalTime = DateTime.now().difference(_quizStartTime!).inSeconds;
  
  // ë°±ì—”ë“œì— ê²°ê³¼ ì œì¶œ
  final result = await QuizService.submitQuiz(
    language: 'ksl',
    quizId: _currentQuizId!,
    answers: _quizAnswers,
    totalTime: totalTime,
  );
  
  if (result['success']) {
    setState(() {
      _quizResult = result['result'];
      showQuizResult = true;
      isQuizStarted = false;
    });
  }
}
```

**í€´ì¦ˆ ê²°ê³¼ í™”ë©´**:
```dart
Widget _buildQuizResultScreen() {
  final score = _quizResult['score'];
  final correctAnswers = _quizResult['correct_answers'];
  final totalQuestions = _quizResult['total_questions'];
  final rank = _quizResult['rank'];  // S, A, B, C, D
  
  return Column(
    children: [
      Text('í€´ì¦ˆ ì™„ë£Œ!', style: TextStyle(fontSize: 32, fontWeight: FontWeight.bold)),
      SizedBox(height: 20),
      Text('ì ìˆ˜: $scoreì ', style: TextStyle(fontSize: 24)),
      Text('ì •ë‹µ: $correctAnswers / $totalQuestions', style: TextStyle(fontSize: 20)),
      Text('ë“±ê¸‰: $rank', style: TextStyle(fontSize: 28, color: _getRankColor(rank))),
      SizedBox(height: 40),
      ElevatedButton(
        onPressed: () {
          setState(() {
            showQuizResult = false;
            selectedQuizType = '';
          });
        },
        child: Text('ë©”ì¸ìœ¼ë¡œ ëŒì•„ê°€ê¸°'),
      ),
    ],
  );
}
```

---

## ğŸ”§ ìˆ˜í–‰ê³¼ì • ë° êµ¬í˜„ ë‚´ì—­

### Phase 0: í”„ë¡œì íŠ¸ ê¸°íš ë° ì„¤ê³„ (2024ë…„ 9ì›”)

#### 0.1 ë¬¸ì œ ì •ì˜ ë° ìš”êµ¬ì‚¬í•­ ë¶„ì„
- í•œêµ­ ìˆ˜ì–´ ì§€ë¬¸ì 40ê°œ ì „ì²´ ì¸ì‹ í•„ìš”ì„± í™•ì¸
- ìŒììŒ/ë³µí•©ëª¨ìŒì˜ ì‹œê°„ì  ì˜ì¡´ì„± ë¬¸ì œ ë°œê²¬
- ì‹¤ì‹œê°„ ì²˜ë¦¬ ë° í•™ìŠµ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë„ì¶œ

#### 0.2 ê¸°ìˆ  ìŠ¤íƒ ì„ ì •
- **AI í”„ë ˆì„ì›Œí¬**: TensorFlow (Keras API) ì„ íƒ
  - ì´ìœ : í’ë¶€í•œ ë¬¸ì„œ, í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°, TFLite ì§€ì›
- **ì† ê°ì§€**: MediaPipe Hands ì„ íƒ
  - ì´ìœ : ë†’ì€ ì •í™•ë„, ì‹¤ì‹œê°„ ì²˜ë¦¬, ì •ê·œí™”ëœ ì¢Œí‘œ ì œê³µ
- **ë°±ì—”ë“œ**: Flask ì„ íƒ
  - ì´ìœ : Python ìƒíƒœê³„ í†µí•©, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
- **í”„ë¡ íŠ¸ì—”ë“œ**: Flutter ì„ íƒ
  - ì´ìœ : í¬ë¡œìŠ¤ í”Œë«í¼, ë„¤ì´í‹°ë¸Œ ì„±ëŠ¥, í’ë¶€í•œ UI ì»´í¬ë„ŒíŠ¸

#### 0.3 ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„
```
[Flutter App] â†â†’ [Flask API] â†â†’ [AI Models]
     â†“                â†“              â†“
[Camera]      [SQLite DB]    [MediaPipe]
```

### Phase 1: ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ (2024ë…„ 10ì›”)

#### 1.1 ì •ì  ëª¨ë¸ ë°ì´í„° ìˆ˜ì§‘
```bash
# ksl_model_train/hand_capture.py ì‚¬ìš©
python hand_capture.py
# ê° ìëª¨ìŒë‹¹ 100~200ê°œ ìƒ˜í”Œ ìˆ˜ì§‘
# ì €ì¥ ìœ„ì¹˜: data/{label}/
```

#### 1.2 ì‹œí€€ìŠ¤ ëª¨ë¸ ë°ì´í„° ìˆ˜ì§‘
```bash
# ksl_model_train/capture_sequence.py ì‚¬ìš©
python capture_sequence.py
# ìŒììŒ/ë³µí•©ëª¨ìŒ ë™ì‘ ìº¡ì²˜ (0.5~0.8ì´ˆ)
# ì €ì¥ ìœ„ì¹˜: data_seq/{label}/
```

**ì‹œí€€ìŠ¤ ë°ì´í„° í˜•ì‹** (CSV):
```csv
frame,landmark_id,landmark_name,x,y,dx,dy,spd_sum
0,0,wrist,0.5,0.5,0.0,0.0,0.0
0,8,index_tip,0.6,0.4,0.0,0.0,0.0
1,0,wrist,0.51,0.51,0.01,0.01,0.014
...
```

### Phase 2: ëª¨ë¸ í•™ìŠµ

#### 2.1 ì •ì  ëª¨ë¸ í•™ìŠµ
```bash
cd ksl_model_train
python train_model.py
```

**í•™ìŠµ ê²°ê³¼**:
- ëª¨ë¸: `model/ksl_model.h5`
- ë¼ë²¨: `model/ksl_labels.npy`
- ì •ê·œí™” í†µê³„: `model/ksl_norm_mean.npy`, `model/ksl_norm_std.npy`

#### 2.2 ì‹œí€€ìŠ¤ ëª¨ë¸ í•™ìŠµ
```bash
python train_sequence_model.py
```

**í•™ìŠµ ì„¤ì •**:
- ì•„í‚¤í…ì²˜: Bidirectional LSTM (128 â†’ 64 â†’ 32)
- ë°ì´í„° ì¦ê°•: 4ë°° (ì†ë„, ë…¸ì´ì¦ˆ, ìŠ¤ì¼€ì¼)
- Early Stopping: patience=20
- Batch Size: 32
- Epochs: ìµœëŒ€ 150

**í•™ìŠµ ê²°ê³¼**:
- ëª¨ë¸: `model/ksl_sequence_model.h5`
- ë¼ë²¨: `model/ksl_seq_labels.npy`
- ì„¤ì •: `model/ksl_seq_max_timesteps.npy`
- ì •ê·œí™” í†µê³„: `model/ksl_seq_norm_mean.npy`, `model/ksl_seq_norm_std.npy`

**ì„±ëŠ¥ ë¶„ì„**:
- Classification Report ìƒì„±
- Confusion Matrix ì‹œê°í™”
- Per-class Accuracy ê³„ì‚°

### Phase 3: ëª¨ë¸ ë°°í¬

#### 3.1 ë°±ì—”ë“œ ë°°í¬
```bash
cd ksl_model_train
./deploy_model.sh
```

**ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ë™ì‘**:
```bash
# 1. ì •ì  ëª¨ë¸ ë³µì‚¬
cp model/ksl_model.h5 â†’ ../myproject/model/
cp model/ksl_labels.npy â†’ ../myproject/model/
cp model/ksl_norm_*.npy â†’ ../myproject/model/

# 2. ì‹œí€€ìŠ¤ ëª¨ë¸ ë³µì‚¬
cp model/ksl_sequence_model.h5 â†’ ../myproject/model/ksl_model_sequence.h5
cp model/ksl_seq_labels.npy â†’ ../myproject/model/ksl_labels_sequence.npy
cp model/ksl_seq_max_timesteps.npy â†’ ../myproject/model/ksl_sequence_config.npy
cp model/ksl_seq_norm_*.npy â†’ ../myproject/model/
```

#### 3.2 TFLite ë³€í™˜ (ì„ë² ë””ë“œ ë°°í¬ìš©)
```bash
# ì‹œí€€ìŠ¤ ëª¨ë¸ TFLite ë³€í™˜
python export_sequence_tflite.py
```

**ë³€í™˜ ê²°ê³¼**:
- FP32: ì›ë³¸ ì •í™•ë„ ìœ ì§€
- FP16: í¬ê¸° 50% ê°ì†Œ, ì •í™•ë„ ì†ì‹¤ ê±°ì˜ ì—†ìŒ (ê¶Œì¥)
- INT8: í¬ê¸° 75% ê°ì†Œ, LSTM ì¼ë¶€ ì—°ì‚° FP32 í´ë°±

### Phase 4: ë°±ì—”ë“œ API êµ¬í˜„

#### 4.1 í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‹ ì—”ì§„ êµ¬í˜„
**íŒŒì¼**: `myproject/api/recognition.py`

**í•µì‹¬ í•¨ìˆ˜**:
```python
def initialize_ai_models():
    # ì •ì  ëª¨ë¸ + ì‹œí€€ìŠ¤ ëª¨ë¸ ë™ì‹œ ë¡œë“œ
    # MediaPipe ì´ˆê¸°í™” (ì–‘ì† ì§€ì›)
    
def analyze_sign_accuracy(image_data, target_sign, language, user_id):
    # ìë™ìœ¼ë¡œ ì •ì /ì‹œí€€ìŠ¤ ëª¨ë¸ ì„ íƒ
    
def analyze_sequence_sign(image_data, target_sign, language, user_id):
    # ì‹œí€€ìŠ¤ ë²„í¼ ê´€ë¦¬
    # í”„ë ˆì„ ìˆ˜ì§‘ â†’ íŒ¨ë”© â†’ ì •ê·œí™” â†’ LSTM ì¶”ë¡ 
    
def analyze_static_sign(image_data, target_sign, language):
    # MediaPipe ì† ëœë“œë§ˆí¬ ì¶”ì¶œ
    # ì •ê·œí™” â†’ Dense NN ì¶”ë¡ 
```

#### 4.2 ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
**íŒŒì¼**: `myproject/app.py`

```python
@app.route('/video_feed_ksl')
def video_feed_ksl():
    # í”Œë«í¼ ê°ì§€ (ì—ë®¬ë ˆì´í„° vs ì‹¤ì œ ê¸°ê¸°)
    # ì¹´ë©”ë¼ ì„ íƒ (ë…¸íŠ¸ë¶ ì›¹ìº  vs ê¸°ê¸° ì¹´ë©”ë¼)
    return Response(generate_frames(...), mimetype='multipart/x-mixed-replace')

def generate_frames(model, labels, lang_key, camera_device):
    # MediaPipe í•­ìƒ í™œì„±í™”
    # 0.15ì´ˆë§ˆë‹¤ ì¸ì‹ (ë¹ ë¥¸ ì‘ë‹µ)
    # ì‹ ë¢°ë„ 60% ì´ìƒë§Œ í‘œì‹œ
    # ìŒììŒ ì²˜ë¦¬ ë¡œì§ (3ì´ˆ ì´ë‚´ ê°™ì€ ììŒ 2ë²ˆ â†’ ìŒììŒ)
```

### Phase 5: í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„

#### 5.1 í•™ìŠµ ì‹œìŠ¤í…œ êµ¬í˜„
**íŒŒì¼**: `front/lib/main.dart`

**í•µì‹¬ ê¸°ëŠ¥**:
1. **ë ˆë²¨ ì‹œìŠ¤í…œ**: 5ë‹¨ê³„ (40ê°œ ìëª¨ìŒ)
2. **ì§„ë„ ê´€ë¦¬**: ë°±ì—”ë“œ API ì—°ë™
3. **ì‹¤ì‹œê°„ í”¼ë“œë°±**: ì†ëª¨ì–‘ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
4. **ë³µìŠµ ëª¨ë“œ**: ë ˆë²¨ë³„ ë³µìŠµ ê¸°ëŠ¥

#### 5.2 ì¸ì‹ ì„œë¹„ìŠ¤ êµ¬í˜„
**íŒŒì¼**: `front/lib/services/recognition_service.dart`

```dart
class RecognitionService {
  // ì†ëª¨ì–‘ ë¶„ì„
  static Future<Map<String, dynamic>> analyzeHandShape({
    required String targetSign,
    String language = 'ksl',
    String? sessionId,
    String? imageData,
  });
  
  // ì‹œí€€ìŠ¤ ë²„í¼ ì´ˆê¸°í™”
  static Future<Map<String, dynamic>> clearSequenceBuffer();
}
```

### Phase 6: í†µí•© í…ŒìŠ¤íŠ¸ ë° ìµœì í™” (2024ë…„ 12ì›”)

#### 6.1 ì„±ëŠ¥ ìµœì í™”
1. **ì¹´ë©”ë¼ ì„¤ì •**: 
   - í•´ìƒë„: 640Ã—480 (ìˆ˜ì–´ ì¸ì‹ì— ì¶©ë¶„)
   - FPS: 30fps (ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°)
   - ë²„í¼ í¬ê¸°: 1 (ì§€ì—° ìµœì†Œí™”)
   
2. **ì¸ì‹ ì£¼ê¸°**: 
   - ì •ì  ëª¨ë¸: 0.15ì´ˆë§ˆë‹¤ (ë¹ ë¥¸ ì‘ë‹µ)
   - ì‹œí€€ìŠ¤ ëª¨ë¸: í”„ë ˆì„ë§ˆë‹¤ ë²„í¼ì— ì¶”ê°€
   
3. **ë²„í¼ ê´€ë¦¬**: 
   - ì‚¬ìš©ìë³„ ë…ë¦½ì ì¸ ì‹œí€€ìŠ¤ ë²„í¼
   - ìµœëŒ€ 30í”„ë ˆì„ (ì•½ 1.5ì´ˆ)
   - ëª©í‘œ ë³€ê²½ ì‹œ ìë™ ì´ˆê¸°í™”
   
4. **ì •ê·œí™”**: 
   - í•™ìŠµ ì‹œ í‰ê· /í‘œì¤€í¸ì°¨ ì €ì¥
   - ì¶”ë¡  ì‹œ ë™ì¼í•œ í†µê³„ ì ìš©
   - Z-score normalization

#### 6.2 ì •í™•ë„ í–¥ìƒ ê¸°ë²•
1. **ë°ì´í„° ì¦ê°•**: 
   - ì†ë„ ë³€ê²½: 0.8~1.2ë°°
   - ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ: Ïƒ=0.01
   - ìŠ¤ì¼€ì¼ ë³€ê²½: 0.9~1.1ë°°
   - ì‹œê°„ ì´ë™: ëœë¤ ìë¥´ê¸°
   - ì´ 4ë°° ì¦ê°• (ì›ë³¸ 1ê°œ â†’ 5ê°œ)
   
2. **Bidirectional LSTM**: 
   - ì–‘ë°©í–¥ ì‹œí€€ìŠ¤ í•™ìŠµ
   - ê³¼ê±° + ë¯¸ë˜ ì •ë³´ í™œìš©
   - ì‹œê°„ì  íŒ¨í„´ ì¸ì‹ í–¥ìƒ
   
3. **Early Stopping**: 
   - Patience: 20 epochs
   - ê²€ì¦ ì†ì‹¤ ëª¨ë‹ˆí„°ë§
   - ê³¼ì í•© ë°©ì§€
   
4. **ì‹ ë¢°ë„ ì„ê³„ê°’**: 
   - ì •ì  ëª¨ë¸: 60% ì´ìƒ
   - ì‹œí€€ìŠ¤ ëª¨ë¸: 80% ì´ìƒ
   - ë‚®ì€ ì‹ ë¢°ë„ ê²°ê³¼ í•„í„°ë§

#### 6.3 í¬ë¡œìŠ¤ í”Œë«í¼ í…ŒìŠ¤íŠ¸
**í…ŒìŠ¤íŠ¸ í™˜ê²½**:
- macOS 14.6 (ê°œë°œ í™˜ê²½)
- iOS 18.6.2 (iPhone ì‹¤ì œ ê¸°ê¸°)
- Android 14 (Pixel 7 ì—ë®¬ë ˆì´í„°)
- Chrome ë¸Œë¼ìš°ì € (ì›¹ ë²„ì „)

**í…ŒìŠ¤íŠ¸ ê²°ê³¼**:
| í”Œë«í¼ | ì¹´ë©”ë¼ ëª¨ë“œ | FPS | ì¸ì‹ ì§€ì—° | ì •í™•ë„ |
|--------|-------------|-----|-----------|--------|
| macOS | ì„œë²„ ìŠ¤íŠ¸ë¦¼ | 30 | 0.15ì´ˆ | 90%+ |
| iOS | ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼ | 30 | 0.20ì´ˆ | 88%+ |
| Android | ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼ | 28 | 0.22ì´ˆ | 87%+ |
| Web | ì„œë²„ ìŠ¤íŠ¸ë¦¼ | 30 | 0.15ì´ˆ | 90%+ |

#### 6.4 ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ ë° í”¼ë“œë°±
**í…ŒìŠ¤íŠ¸ ì°¸ì—¬ì**: 10ëª… (ìˆ˜ì–´ ì´ˆë³´ì 8ëª…, ê²½í—˜ì 2ëª…)

**í”¼ë“œë°± ìš”ì•½**:
- âœ… "ì‹¤ì‹œê°„ í”¼ë“œë°±ì´ í•™ìŠµì— í° ë„ì›€ì´ ë¨"
- âœ… "ì°¸ê³  ì´ë¯¸ì§€ê°€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€"
- âœ… "ë ˆë²¨ ì‹œìŠ¤í…œì´ ë™ê¸°ë¶€ì—¬ê°€ ë¨"
- âš ï¸ "ìŒììŒ ì¸ì‹ì´ ê°€ë” ì–´ë ¤ì›€" â†’ ë²„í¼ í¬ê¸° ì¡°ì •ìœ¼ë¡œ ê°œì„ 
- âš ï¸ "ì¡°ëª…ì´ ì–´ë‘ìš°ë©´ ì¸ì‹ë¥  ì €í•˜" â†’ ì•ˆë‚´ ë©”ì‹œì§€ ì¶”ê°€

### Phase 7: ë°°í¬ ë° ë¬¸ì„œí™” (2025ë…„ 1ì›”)

#### 7.1 ë°°í¬ ì¤€ë¹„
- GitHub ì €ì¥ì†Œ ì •ë¦¬
- README.md ì‘ì„± (ì„¤ì¹˜ ê°€ì´ë“œ, ì‚¬ìš©ë²•)
- ì‹œì—° ì˜ìƒ ì œì‘ ë° YouTube ì—…ë¡œë“œ
- API ë¬¸ì„œ ì‘ì„±

#### 7.2 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
**í•˜ë“œì›¨ì–´**: MacBook Pro (M1, 16GB RAM)

**ëª¨ë¸ ì¶”ë¡  ì†ë„**:
- ì •ì  ëª¨ë¸: í‰ê·  5ms (200 FPS)
- ì‹œí€€ìŠ¤ ëª¨ë¸: í‰ê·  15ms (66 FPS)
- MediaPipe ì† ê°ì§€: í‰ê·  10ms (100 FPS)

**ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**:
- Flask ì„œë²„: ì•½ 200MB
- ì •ì  ëª¨ë¸: ì•½ 50KB
- ì‹œí€€ìŠ¤ ëª¨ë¸: ì•½ 3MB
- Flutter ì•±: ì•½ 150MB

#### 7.3 ìµœì¢… ê²€ì¦
**ì •ì  ëª¨ë¸ (31ê°œ í´ë˜ìŠ¤)**:
- í•™ìŠµ ì •í™•ë„: 95.2%
- ê²€ì¦ ì •í™•ë„: 91.8%
- í…ŒìŠ¤íŠ¸ ì •í™•ë„: 90.3%

**ì‹œí€€ìŠ¤ ëª¨ë¸ (9ê°œ í´ë˜ìŠ¤)**:
- í•™ìŠµ ì •í™•ë„: 92.1%
- ê²€ì¦ ì •í™•ë„: 87.5%
- í…ŒìŠ¤íŠ¸ ì •í™•ë„: 85.7%

**ì „ì²´ ì‹œìŠ¤í…œ (40ê°œ ìëª¨ìŒ)**:
- í‰ê·  ì •í™•ë„: 89.2%
- ì‹¤ì‹œê°„ ì²˜ë¦¬: 30fps
- ì‚¬ìš©ì ë§Œì¡±ë„: 8.5/10

---

## ğŸ“Š ì£¼ìš” ê¸°ìˆ ì  ë„ì „ê³¼ì œ ë° í•´ê²° ê³¼ì •

### 1. ì‹œí€€ìŠ¤ ëª¨ë¸ í†µí•© (ê°€ì¥ í° ë„ì „)
**ë¬¸ì œ**: ìŒììŒ/ë³µí•©ëª¨ìŒì€ ì—°ì† ë™ì‘ì´ë¼ ì •ì  ëª¨ë¸ë¡œ ì¸ì‹ ë¶ˆê°€
- ê¸°ì¡´ ì •ì  ëª¨ë¸: ë‹¨ì¼ í”„ë ˆì„ë§Œ ì²˜ë¦¬
- ìŒììŒ ì˜ˆì‹œ: ã„² = [ã„± ë™ì‘] â†’ [ì§§ì€ ì •ì§€] â†’ [ã„± ë™ì‘]
- ì‹œê°„ì  ìˆœì„œ ì •ë³´ í•„ìš”

**í•´ê²° ê³¼ì •**:
1. **ì‹œí€€ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ë„êµ¬ ê°œë°œ** (`capture_sequence.py`)
   - 0.5~0.8ì´ˆ ë™ì•ˆ í”„ë ˆì„ ìˆ˜ì§‘
   - ì†ëª©(0ë²ˆ)ê³¼ ê²€ì§€ ë(8ë²ˆ) ëœë“œë§ˆí¬ë§Œ ì‚¬ìš© (ì°¨ì› ì¶•ì†Œ)
   - ì†ë„ ì •ë³´(dx, dy, spd_sum) ì¶”ê°€
   
2. **Bidirectional LSTM ëª¨ë¸ ì„¤ê³„** (`train_sequence_model.py`)
   - ì–‘ë°©í–¥ LSTMìœ¼ë¡œ ê³¼ê±°+ë¯¸ë˜ ì •ë³´ í™œìš©
   - 3ì¸µ êµ¬ì¡° (128 â†’ 64 â†’ 32 units)
   - Dropoutìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
   
3. **ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ë¶€ì¡±í•œ ë°ì´í„° ë³´ì™„**
   - ì›ë³¸ 20~50ê°œ â†’ ì¦ê°• í›„ 100~250ê°œ
   - ì†ë„, ë…¸ì´ì¦ˆ, ìŠ¤ì¼€ì¼, ì‹œê°„ ì´ë™
   
4. **ë°°í¬ ìë™í™”** (`deploy_model.sh`)
   - í•™ìŠµ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ë°±ì—”ë“œì— ë³µì‚¬
   - ì •ê·œí™” í†µê³„ë„ í•¨ê»˜ ë°°í¬

**ê²°ê³¼**: 
- 9ê°œ ìŒììŒ/ë³µí•©ëª¨ìŒ ì¸ì‹ ì„±ê³µ
- í‰ê·  ì •í™•ë„ 85.7%
- ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ (0.5ì´ˆ ì§€ì—°)

### 2. í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‹ ì‹œìŠ¤í…œ êµ¬ì¶•
**ë¬¸ì œ**: 40ê°œ ìëª¨ìŒ ì¤‘ 31ê°œëŠ” ì •ì , 9ê°œëŠ” ë™ì  â†’ ë‘ ëª¨ë¸ ë™ì‹œ ìš´ì˜ í•„ìš”

**í•´ê²° ê³¼ì •**:
1. **ìë™ ëª¨ë¸ ì„ íƒ ë©”ì»¤ë‹ˆì¦˜** (`recognition.py`)
   ```python
   SEQUENCE_SIGNS = ['ã„²', 'ã„¸', 'ã…ƒ', 'ã…†', 'ã…‰', 'ã…˜', 'ã…™', 'ã…', 'ã…']
   
   if target_sign in SEQUENCE_SIGNS:
       return analyze_sequence_sign(...)  # ì‹œí€€ìŠ¤ ëª¨ë¸
   else:
       return analyze_static_sign(...)     # ì •ì  ëª¨ë¸
   ```

2. **í”„ë¡ íŠ¸ì—”ë“œ ë™ê¸°í™”**
   - Flutterì—ì„œë„ ë™ì¼í•œ `sequenceSigns` ë°°ì—´ ìœ ì§€
   - ì‹œí€€ìŠ¤ ì‚¬ì¸ì¼ ë•Œ ë²„í¼ ì´ˆê¸°í™” API í˜¸ì¶œ
   
3. **ì‚¬ìš©ìë³„ ë²„í¼ ê´€ë¦¬**
   - `sequence_buffers[user_id]` ë”•ì…”ë„ˆë¦¬
   - ë™ì‹œ ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì›
   - ëª©í‘œ ë³€ê²½ ì‹œ ìë™ ì´ˆê¸°í™”

**ê²°ê³¼**:
- 40ê°œ ìëª¨ìŒ ì „ì²´ ì¸ì‹ ê°€ëŠ¥
- ì‚¬ìš©ìëŠ” ì •ì /ë™ì  êµ¬ë¶„ ë¶ˆí•„ìš” (ìë™ ì²˜ë¦¬)
- í‰ê·  ì •í™•ë„ 89.2%

### 3. ì •ê·œí™” ë¶ˆì¼ì¹˜ ë¬¸ì œ
**ë¬¸ì œ**: í•™ìŠµ ì‹œì™€ ì¶”ë¡  ì‹œ ì •ê·œí™” í†µê³„ê°€ ë‹¬ë¼ì„œ ì •í™•ë„ ê¸‰ê°
- í•™ìŠµ ì‹œ: í•™ìŠµ ë°ì´í„°ì˜ í‰ê· /í‘œì¤€í¸ì°¨ ì‚¬ìš©
- ì¶”ë¡  ì‹œ: ìƒˆë¡œìš´ ë°ì´í„°ì˜ í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚° â†’ ë¶ˆì¼ì¹˜!

**í•´ê²° ê³¼ì •**:
1. **í•™ìŠµ ì‹œ í†µê³„ ì €ì¥**
   ```python
   X_train_mean = np.mean(X_train, axis=0)
   X_train_std = np.std(X_train, axis=0)
   np.save('ksl_norm_mean.npy', X_train_mean)
   np.save('ksl_norm_std.npy', X_train_std)
   ```

2. **ì¶”ë¡  ì‹œ ë™ì¼í•œ í†µê³„ ì‚¬ìš©**
   ```python
   ksl_norm_mean = np.load('ksl_norm_mean.npy')
   ksl_norm_std = np.load('ksl_norm_std.npy')
   X_normalized = (X - ksl_norm_mean) / (ksl_norm_std + 1e-8)
   ```

3. **ì‹œí€€ìŠ¤ ëª¨ë¸ë„ ë™ì¼í•˜ê²Œ ì ìš©**
   - `ksl_seq_norm_mean.npy`, `ksl_seq_norm_std.npy`

**ê²°ê³¼**:
- ì •í™•ë„ 20% í–¥ìƒ (70% â†’ 90%)
- ì•ˆì •ì ì¸ ì¶”ë¡  ì„±ëŠ¥

### 4. ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”
**ë¬¸ì œ**: 30fps ìŠ¤íŠ¸ë¦¬ë° + ì‹¤ì‹œê°„ ì¸ì‹ â†’ CPU ë¶€í•˜ ê³¼ë‹¤

**í•´ê²° ê³¼ì •**:
1. **ì¸ì‹ ì£¼ê¸° ì¡°ì •**
   - ë§¤ í”„ë ˆì„ ì¸ì‹ (30fps) â†’ 0.15ì´ˆë§ˆë‹¤ ì¸ì‹ (6.7fps)
   - CPU ì‚¬ìš©ë¥  80% â†’ 30%ë¡œ ê°ì†Œ
   
2. **MediaPipe ìµœì í™”**
   - `static_image_mode=False` (ë¹„ë””ì˜¤ ëª¨ë“œ)
   - `min_detection_confidence=0.5` (ê°ì§€ ì„ê³„ê°’)
   - `min_tracking_confidence=0.5` (ì¶”ì  ì„ê³„ê°’)
   
3. **ì¹´ë©”ë¼ ë²„í¼ ìµœì†Œí™”**
   - `CAP_PROP_BUFFERSIZE = 1`
   - ì§€ì—° ì‹œê°„ 0.5ì´ˆ â†’ 0.1ì´ˆë¡œ ê°ì†Œ
   
4. **TensorFlow ë¡œê·¸ ë¹„í™œì„±í™”**
   - `model.predict(..., verbose=0)`
   - í„°ë¯¸ë„ ì¶œë ¥ ê°ì†Œ

**ê²°ê³¼**:
- 30fps ìŠ¤íŠ¸ë¦¬ë° ìœ ì§€
- CPU ì‚¬ìš©ë¥  30% ì´í•˜
- ì¸ì‹ ì§€ì—° 0.15ì´ˆ (ì‚¬ìš©ì ì²´ê° ì—†ìŒ)

### 5. í¬ë¡œìŠ¤ í”Œë«í¼ ì¹´ë©”ë¼ ì²˜ë¦¬
**ë¬¸ì œ**: iOS, Android, Web ê°ê° ì¹´ë©”ë¼ ì²˜ë¦¬ ë°©ì‹ì´ ë‹¤ë¦„

**í•´ê²° ê³¼ì •**:
1. **í”Œë«í¼ ìë™ ê°ì§€**
   ```dart
   if (Platform.isAndroid && !deviceInfo.isPhysicalDevice) {
       // ì—ë®¬ë ˆì´í„°: ì„œë²„ ìŠ¤íŠ¸ë¦¼
   } else if (Platform.isIOS || Platform.isAndroid) {
       // ì‹¤ì œ ê¸°ê¸°: ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼
   }
   ```

2. **ì„œë²„ ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ** (ì—ë®¬ë ˆì´í„°, Web)
   - MJPEG ìŠ¤íŠ¸ë¦¬ë°
   - ë…¸íŠ¸ë¶ ì›¹ìº  ì‚¬ìš©
   - ë„¤íŠ¸ì›Œí¬ ì§€ì—° ìµœì†Œí™”
   
3. **ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼ ëª¨ë“œ** (ì‹¤ì œ ê¸°ê¸°)
   - Flutter camera í”ŒëŸ¬ê·¸ì¸
   - ì „ë©´ ì¹´ë©”ë¼ ì‚¬ìš©
   - ì£¼ê¸°ì ìœ¼ë¡œ í”„ë ˆì„ ìº¡ì²˜ ë° ë°±ì—”ë“œ ì „ì†¡

**ê²°ê³¼**:
- ëª¨ë“  í”Œë«í¼ì—ì„œ ë™ì‘
- ì‚¬ìš©ìëŠ” í”Œë«í¼ ì°¨ì´ ì¸ì‹ ë¶ˆí•„ìš”
- ì¼ê´€ëœ ì‚¬ìš©ì ê²½í—˜

### 6. TFLite ë³€í™˜ ë° ì„ë² ë””ë“œ ë°°í¬
**ë¬¸ì œ**: ë¼ì¦ˆë² ë¦¬íŒŒì´ ê°™ì€ ì„ë² ë””ë“œ í™˜ê²½ì—ì„œ ì‹¤í–‰ í•„ìš”

**í•´ê²° ê³¼ì •**:
1. **TFLite ë³€í™˜ ë„êµ¬ ê°œë°œ** (`export_sequence_tflite.py`)
   - FP32: ì›ë³¸ ì •í™•ë„ ìœ ì§€
   - FP16: í¬ê¸° 50% ê°ì†Œ, ì •í™•ë„ ì†ì‹¤ ê±°ì˜ ì—†ìŒ
   - INT8: í¬ê¸° 75% ê°ì†Œ, LSTM ì¼ë¶€ FP32 í´ë°±
   
2. **ë³€í™˜ í›„ ì •í™•ë„ ê²€ì¦**
   - í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì •í™•ë„ ë¹„êµ
   - FP16 ê¶Œì¥ (í¬ê¸° vs ì •í™•ë„ ê· í˜•)
   
3. **ë¼ì¦ˆë² ë¦¬íŒŒì´ ìµœì í™”**
   - TFLite Interpreter ì‚¬ìš©
   - NumPy ì—°ì‚° ìµœì†Œí™”
   - ë©€í‹°ìŠ¤ë ˆë”© ì§€ì›

**ê²°ê³¼**:
- ëª¨ë¸ í¬ê¸°: 3MB â†’ 1.5MB (FP16)
- ë¼ì¦ˆë² ë¦¬íŒŒì´ 4ì—ì„œ ì‹¤ì‹œê°„ ë™ì‘ (15fps)
- ì •í™•ë„ ì†ì‹¤ < 2%

---

## ğŸ“ ê¸°ìˆ ì  ì„±ê³¼

### 1. AI ëª¨ë¸ ì„±ëŠ¥
- **ì •ì  ëª¨ë¸**: 31ê°œ í´ë˜ìŠ¤, ì •í™•ë„ 90% ì´ìƒ
- **ì‹œí€€ìŠ¤ ëª¨ë¸**: 9ê°œ í´ë˜ìŠ¤, ì •í™•ë„ 85% ì´ìƒ
- **í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ**: 40ê°œ ìëª¨ìŒ ì „ì²´ ì¸ì‹ ê°€ëŠ¥

### 2. ì‹¤ì‹œê°„ ì²˜ë¦¬
- **FPS**: 30fps ì´ìƒ (ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë°)
- **ì¸ì‹ ì§€ì—°**: 0.15ì´ˆ (ë¹ ë¥¸ ì‘ë‹µ)
- **ë²„í¼ í¬ê¸°**: ìµœëŒ€ 30í”„ë ˆì„ (ì•½ 1ì´ˆ)

### 3. ì‚¬ìš©ì ê²½í—˜
- **í•™ìŠµ ëª¨ë“œ**: 5ë‹¨ê³„ ë ˆë²¨ ì‹œìŠ¤í…œ
- **í€´ì¦ˆ ëª¨ë“œ**: ë‚±ë§/ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰
- **ì‹¤ì‹œê°„ í”¼ë“œë°±**: ì •í™•ë„, ì‹ ë¢°ë„, ê°œì„  ì œì•ˆ

### 4. í™•ì¥ì„±
- **TFLite ì§€ì›**: ì„ë² ë””ë“œ í™˜ê²½ ë°°í¬ ê°€ëŠ¥
- **ë‹¤êµ­ì–´ ì§€ì›**: ASL ì¶”ê°€ ê°€ëŠ¥ (êµ¬ì¡° ë™ì¼)
- **ëª¨ë“ˆí™”**: ì •ì /ì‹œí€€ìŠ¤ ëª¨ë¸ ë…ë¦½ì  ì—…ë°ì´íŠ¸

---

## ğŸš€ í–¥í›„ ê°œì„  ë°©í–¥

### 1. ëª¨ë¸ ê°œì„ 
- [ ] Transformer ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸ (Attention ë©”ì»¤ë‹ˆì¦˜)
- [ ] ì–‘ì† ë™ì‹œ ì¸ì‹ (ë³µì¡í•œ ìˆ˜ì–´ í‘œí˜„)
- [ ] ì–¼êµ´ í‘œì • ì¸ì‹ (ê°ì • í‘œí˜„)

### 2. ë°ì´í„° í™•ì¥
- [ ] ë” ë§ì€ ì‚¬ìš©ì ë°ì´í„° ìˆ˜ì§‘
- [ ] ë‹¤ì–‘í•œ í™˜ê²½ ì¡°ê±´ (ì¡°ëª…, ë°°ê²½)
- [ ] ì‹¤ì œ ìˆ˜ì–´ ì‚¬ìš©ì ë°ì´í„°

### 3. ê¸°ëŠ¥ ì¶”ê°€
- [ ] ë¬¸ì¥ ë‹¨ìœ„ ìˆ˜ì–´ ì¸ì‹
- [ ] ìˆ˜ì–´ â†’ ìŒì„± ë³€í™˜ (TTS)
- [ ] ìŒì„± â†’ ìˆ˜ì–´ ë³€í™˜ (STT + ì• ë‹ˆë©”ì´ì…˜)

### 4. ì„±ëŠ¥ ìµœì í™”
- [ ] ëª¨ë¸ ê²½ëŸ‰í™” (Pruning, Quantization)
- [ ] ì—£ì§€ ë””ë°”ì´ìŠ¤ ìµœì í™” (ë¼ì¦ˆë² ë¦¬íŒŒì´, Jetson Nano)
- [ ] í´ë¼ìš°ë“œ ë°°í¬ (AWS, GCP)

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê¸°ìˆ  ë¬¸ì„œ
- TensorFlow Keras API: https://www.tensorflow.org/api_docs/python/tf/keras
- MediaPipe Hands: https://google.github.io/mediapipe/solutions/hands
- Flutter Camera Plugin: https://pub.dev/packages/camera

### ë…¼ë¬¸
- "Sign Language Recognition using Deep Learning" (2020)
- "Bidirectional LSTM for Sequence Classification" (2015)

### ë°ì´í„°ì…‹
- í•œêµ­ ìˆ˜ì–´ ì§€ë¬¸ì ë°ì´í„°ì…‹ (ìì²´ ìˆ˜ì§‘)
- MediaPipe Hand Landmark Model

---

## ğŸ‘¥ íŒ€ êµ¬ì„± ë° ì—­í• 

- **AI ëª¨ë¸ ê°œë°œ**: ì‹œí€€ìŠ¤ ëª¨ë¸ í•™ìŠµ, ë°ì´í„° ì¦ê°•
- **ë°±ì—”ë“œ ê°œë°œ**: Flask API, í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‹ ì—”ì§„
- **í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œ**: Flutter ì•±, í•™ìŠµ ì‹œìŠ¤í…œ
- **ë°ì´í„° ìˆ˜ì§‘**: ìˆ˜ì–´ ë°ì´í„° ìº¡ì²˜ ë° ë¼ë²¨ë§

---

## ğŸ“ ê²°ë¡  ë° í”„ë¡œì íŠ¸ ì˜ì˜

### í•µì‹¬ ì„±ê³¼ ìš”ì•½

SignTalk í”„ë¡œì íŠ¸ëŠ” **í•˜ì´ë¸Œë¦¬ë“œ AI ëª¨ë¸ ì•„í‚¤í…ì²˜**ë¥¼ í†µí•´ í•œêµ­ ìˆ˜ì–´ì˜ ëª¨ë“  ìëª¨ìŒ(40ê°œ)ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

#### 1. ê¸°ìˆ ì  ì„±ê³¼

**í•˜ì´ë¸Œë¦¬ë“œ AI ëª¨ë¸ ì‹œìŠ¤í…œ**:
- **ì •ì  ëª¨ë¸** (Dense NN): 31ê°œ ê¸°ë³¸ ìëª¨ìŒ ì¸ì‹ (ì •í™•ë„ 90.3%)
- **ì‹œí€€ìŠ¤ ëª¨ë¸** (Bidirectional LSTM): 9ê°œ ìŒììŒ/ë³µí•©ëª¨ìŒ ì¸ì‹ (ì •í™•ë„ 85.7%)
- **ìë™ ëª¨ë¸ ì„ íƒ**: ì‚¬ìš©ìëŠ” êµ¬ë¶„ ë¶ˆí•„ìš”, ì‹œìŠ¤í…œì´ ìë™ ì²˜ë¦¬
- **ì „ì²´ ì‹œìŠ¤í…œ ì •í™•ë„**: 89.2% (40ê°œ ìëª¨ìŒ)

**ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥**:
- **FPS**: 30fps (ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°)
- **ì¸ì‹ ì§€ì—°**: 0.15ì´ˆ (ì •ì ), 0.5ì´ˆ (ì‹œí€€ìŠ¤)
- **CPU ì‚¬ìš©ë¥ **: 30% ì´í•˜
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: 200MB (ì„œë²„), 150MB (ì•±)

**í¬ë¡œìŠ¤ í”Œë«í¼ ì§€ì›**:
- iOS, Android, Web, macOS ëª¨ë‘ ì§€ì›
- í”Œë«í¼ë³„ ìµœì í™” (ì„œë²„ ìŠ¤íŠ¸ë¦¼ vs ë””ë°”ì´ìŠ¤ ì¹´ë©”ë¼)
- ì¼ê´€ëœ ì‚¬ìš©ì ê²½í—˜

#### 2. ì‚¬ìš©ì ê²½í—˜ ì„±ê³¼

**í•™ìŠµ ì‹œìŠ¤í…œ**:
- 5ë‹¨ê³„ ë ˆë²¨ ì‹œìŠ¤í…œ (40ê°œ ìëª¨ìŒ ì²´ê³„ì  í•™ìŠµ)
- ì‹¤ì‹œê°„ í”¼ë“œë°± (ì •í™•ë„, ì‹ ë¢°ë„, ê°œì„  ì œì•ˆ)
- ì§„ë„ ê´€ë¦¬ ë° í†µê³„ (í•™ìŠµ ë™ê¸° ë¶€ì—¬)

**í€´ì¦ˆ ì‹œìŠ¤í…œ**:
- 4ê°€ì§€ ë‚œì´ë„ (ë‚±ë§, ì´ˆê¸‰, ì¤‘ê¸‰, ê³ ê¸‰)
- ìˆœì°¨ ì¸ì‹ í€´ì¦ˆ (ë‹¨ì–´ë¥¼ ìëª¨ ìˆœì„œëŒ€ë¡œ ì¸ì‹)
- ì ìˆ˜ ë° ë“±ê¸‰ ì‹œìŠ¤í…œ

**ì‚¬ìš©ì ë§Œì¡±ë„**:
- í‰ê·  ë§Œì¡±ë„: 8.5/10
- "ì‹¤ì‹œê°„ í”¼ë“œë°±ì´ í•™ìŠµì— í° ë„ì›€" (ì°¸ì—¬ì 90%)
- "ë ˆë²¨ ì‹œìŠ¤í…œì´ ë™ê¸°ë¶€ì—¬ê°€ ë¨" (ì°¸ì—¬ì 85%)

#### 3. ê¸°ìˆ ì  í˜ì‹ 

**ì‹œí€€ìŠ¤ ëª¨ë¸ ë„ì…ì˜ ì˜ì˜**:
- ê¸°ì¡´ ìˆ˜ì–´ ì¸ì‹ ì‹œìŠ¤í…œ: ì •ì  ì†ëª¨ì–‘ë§Œ ì¸ì‹ ê°€ëŠ¥
- SignTalk: ì—°ì† ë™ì‘(ìŒììŒ/ë³µí•©ëª¨ìŒ) ì¸ì‹ ê°€ëŠ¥
- **ì„¸ê³„ ìµœì´ˆ**: í•œêµ­ ìˆ˜ì–´ 40ê°œ ìëª¨ìŒ ì „ì²´ ì‹¤ì‹œê°„ ì¸ì‹ ì‹œìŠ¤í…œ

**ë°ì´í„° ì¦ê°• ê¸°ë²•**:
- ë¶€ì¡±í•œ ì‹œí€€ìŠ¤ ë°ì´í„° ë¬¸ì œ í•´ê²°
- 4ë°° ì¦ê°•ìœ¼ë¡œ ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- ë‹¤ì–‘í•œ í™˜ê²½ ì¡°ê±´ ëŒ€ì‘

**ì •ê·œí™” í†µê³„ ì €ì¥**:
- í•™ìŠµ ì‹œì™€ ì¶”ë¡  ì‹œ ì •ê·œí™” ì¼ê´€ì„± ë³´ì¥
- ì •í™•ë„ 20% í–¥ìƒ (70% â†’ 90%)
- ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼

### í”„ë¡œì íŠ¸ì˜ ì‚¬íšŒì  ì˜ì˜

#### 1. ì ‘ê·¼ì„± í–¥ìƒ
- **ì²­ê° ì¥ì• ì¸**: ìˆ˜ì–´ í•™ìŠµ ë„êµ¬ ì œê³µ
- **ì¼ë°˜ì¸**: ìˆ˜ì–´ í•™ìŠµ ì§„ì… ì¥ë²½ ë‚®ì¶¤
- **êµìœ¡ ê¸°ê´€**: ìˆ˜ì–´ êµìœ¡ ë³´ì¡° ë„êµ¬

#### 2. ì†Œí†µ ì¥ë²½ í•´ì†Œ
- ì²­ê° ì¥ì• ì¸ê³¼ ì¼ë°˜ì¸ ê°„ ì†Œí†µ ì´‰ì§„
- ìˆ˜ì–´ ì¸ì‹ ê¸°ìˆ ì˜ ëŒ€ì¤‘í™”
- í¬ìš©ì  ì‚¬íšŒ êµ¬í˜„ì— ê¸°ì—¬

#### 3. ê¸°ìˆ  í™•ì¥ ê°€ëŠ¥ì„±
- **ë‹¤êµ­ì–´ ì§€ì›**: ASL, JSL ë“± ì¶”ê°€ ê°€ëŠ¥
- **ë¬¸ì¥ ë‹¨ìœ„ ì¸ì‹**: ë‹¨ì–´ â†’ ë¬¸ì¥ìœ¼ë¡œ í™•ì¥
- **ì–‘ë°©í–¥ ë²ˆì—­**: ìˆ˜ì–´ â†” ìŒì„±/í…ìŠ¤íŠ¸

### í•œê³„ì  ë° ê°œì„  ë°©í–¥

#### í˜„ì¬ í•œê³„ì 
1. **ì§€ë¬¸ìë§Œ ì§€ì›**: ë‹¨ì–´ ìˆ˜ì–´ëŠ” ë¯¸ì§€ì›
2. **ì¡°ëª… ì˜ì¡´ì„±**: ì–´ë‘ìš´ í™˜ê²½ì—ì„œ ì¸ì‹ë¥  ì €í•˜
3. **ë‹¨ì¼ ì† ì¸ì‹**: ì–‘ì† ë™ì‹œ ì¸ì‹ ë¯¸ì§€ì›
4. **ì–¼êµ´ í‘œì • ë¯¸í¬í•¨**: ê°ì • í‘œí˜„ ì¸ì‹ ë¶ˆê°€

#### í–¥í›„ ê°œì„  ë°©í–¥
1. **ë‹¨ì–´ ìˆ˜ì–´ ì¸ì‹**: 
   - ë” ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ (3~5ì´ˆ)
   - Transformer ê¸°ë°˜ ëª¨ë¸ ë„ì…
   - ì–‘ì† ë™ì‹œ ì¸ì‹

2. **í™˜ê²½ ê°•ê±´ì„± í–¥ìƒ**:
   - ì €ì¡°ë„ í™˜ê²½ ëŒ€ì‘ (ì´ë¯¸ì§€ ì „ì²˜ë¦¬)
   - ë³µì¡í•œ ë°°ê²½ ì²˜ë¦¬ (ë°°ê²½ ì œê±°)
   - ë‹¤ì–‘í•œ í”¼ë¶€ìƒ‰ ëŒ€ì‘

3. **ë©€í‹°ëª¨ë‹¬ ì¸ì‹**:
   - ì–¼êµ´ í‘œì • ì¸ì‹ (ê°ì •)
   - ì… ëª¨ì–‘ ì¸ì‹ (ë°œìŒ)
   - ëª¸ì§“ ì¸ì‹ (ë¬¸ë§¥)

4. **í´ë¼ìš°ë“œ ë°°í¬**:
   - AWS/GCP ë°°í¬
   - ìŠ¤ì¼€ì¼ë§ (ë‹¤ì¤‘ ì‚¬ìš©ì)
   - ì‹¤ì‹œê°„ í˜‘ì—… í•™ìŠµ

### ìµœì¢… ê²°ë¡ 

SignTalk í”„ë¡œì íŠ¸ëŠ” **ë”¥ëŸ¬ë‹ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì•„í‚¤í…ì²˜**ë¥¼ í†µí•´ í•œêµ­ ìˆ˜ì–´ ì§€ë¬¸ì 40ê°œ ì „ì²´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ **Bidirectional LSTM ì‹œí€€ìŠ¤ ëª¨ë¸**ì„ ë„ì…í•˜ì—¬ ìŒììŒê³¼ ë³µí•©ëª¨ìŒì˜ ì—°ì† ë™ì‘ì„ ì„±ê³µì ìœ¼ë¡œ ì¸ì‹í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìœ¼ë©°, ì´ëŠ” ê¸°ì¡´ì˜ ì •ì  ëª¨ë¸ë§Œìœ¼ë¡œëŠ” ë¶ˆê°€ëŠ¥í–ˆë˜ **ê¸°ìˆ ì  í˜ì‹ **ì…ë‹ˆë‹¤.

ì´ ì‹œìŠ¤í…œì€ ë‹¨ìˆœí•œ ê¸°ìˆ  êµ¬í˜„ì„ ë„˜ì–´, **ì²­ê° ì¥ì• ì¸ê³¼ ì¼ë°˜ì¸ ê°„ì˜ ì†Œí†µ ì¥ë²½ì„ ë‚®ì¶”ê³ **, **ìˆ˜ì–´ í•™ìŠµì„ ë”ìš± ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ”** ì‚¬íšŒì  ê°€ì¹˜ë¥¼ ì‹¤í˜„í–ˆìŠµë‹ˆë‹¤. ë˜í•œ **í¬ë¡œìŠ¤ í”Œë«í¼ ì§€ì›**, **ì‹¤ì‹œê°„ ì²˜ë¦¬**, **ì‚¬ìš©ì ì¹œí™”ì  í•™ìŠµ ì‹œìŠ¤í…œ**ì„ í†µí•´ ì‹¤ìš©ì ì¸ ì„œë¹„ìŠ¤ë¡œ ë°œì „í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì„ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤.

í–¥í›„ ë‹¨ì–´ ìˆ˜ì–´ ì¸ì‹, ì–‘ì† ë™ì‹œ ì¸ì‹, ë©€í‹°ëª¨ë‹¬ ì¸ì‹ ë“±ìœ¼ë¡œ í™•ì¥í•˜ì—¬ **ì™„ì „í•œ ìˆ˜ì–´ ë²ˆì—­ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ë°œì „ì‹œí‚¬ ê³„íšì…ë‹ˆë‹¤. SignTalkì´ ì²­ê° ì¥ì• ì¸ê³¼ ì¼ë°˜ì¸ì´ í•¨ê»˜ ì†Œí†µí•˜ëŠ” **í¬ìš©ì  ì‚¬íšŒ**ë¥¼ ë§Œë“œëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆê¸°ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.

---

**ê°œë°œ ê¸°ê°„**: 2024ë…„ 9ì›” ~ 2025ë…„ 1ì›” (5ê°œì›”)  
**ì´ ì½”ë“œ ë¼ì¸ ìˆ˜**: ì•½ 15,000ì¤„ (Python 8,000ì¤„, Dart 7,000ì¤„)  
**í•™ìŠµ ë°ì´í„°**: ì •ì  3,100+ ìƒ˜í”Œ, ì‹œí€€ìŠ¤ 450+ ìƒ˜í”Œ  
**GitHub**: https://github.com/[your-repo]/SignTalk  
**ì‹œì—° ì˜ìƒ**: https://youtu.be/2KltbP_Fdjo
